# Data Basics

## Introduction

This chapter covers the fundamental concepts of working with data in R. You'll learn how to import, clean, and prepare data for analysis, which are essential skills for any data analysis project across all natural science disciplines.

## Understanding Data Structures

Before diving into data analysis, it's important to understand the basic data structures in R:

### Data Types

R has several basic data types:

- **Numeric**: Decimal values (e.g., measurements of temperature, pH, concentration, or distance)
- **Integer**: Whole numbers (e.g., counts of organisms, samples, or observations)
- **Character**: Text strings (e.g., species names, site descriptions, or treatment labels)
- **Logical**: TRUE/FALSE values (e.g., presence/absence data or condition met/not met)
- **Factor**: Categorical variables with levels (e.g., experimental treatments, taxonomic classifications, or soil types)
- **Date/Time**: Temporal data (e.g., sampling dates, observation times, or seasonal markers)

```{r}
# Examples of different data types
numeric_example <- 25.4  # Temperature in Celsius
character_example <- "Adelie"  # Penguin species
logical_example <- TRUE  # Presence/absence data
factor_example <- factor(c("Control", "Treatment", "Control"), 
                         levels = c("Control", "Treatment"))
date_example <- as.Date("2020-07-15")  # Sampling date

# Print examples
print(numeric_example)
print(character_example)
print(logical_example)
print(factor_example)
print(date_example)
```

::: {.callout-tip}
## PROFESSIONAL TIP: Data Management Best Practices

Proper data management is critical for reproducible research in natural sciences:

- **Document metadata**: Always maintain detailed records about data collection methods, units, and variable definitions
- **Use consistent naming conventions**: Create clear, consistent file and variable names (e.g., `site_01_temp_2023.csv` instead of `data1.csv`)
- **Preserve raw data**: Never modify your original data files; always work with copies for cleaning and analysis
- **Version control**: Use Git or similar tools to track changes to your data processing scripts
- **Implement quality control**: Create automated checks for impossible values, outliers, and inconsistencies
- **Plan for missing data**: Develop a consistent strategy for handling missing values before analysis begins
- **Create tidy data**: Structure data with one observation per row and one variable per column
- **Use open formats**: Store data in non-proprietary formats (CSV, TSV) for long-term accessibility
- **Back up regularly**: Maintain multiple copies of your data in different physical locations
- **Consider data repositories**: Share your data through repositories like Dryad, Zenodo, or discipline-specific databases
:::

### Data Structures in R

R has several data structures for organizing information:

```{r}
# Load real datasets
library(readr)
penguins <- read_csv("../data/environmental/climate_data.csv")
crops <- read_csv("../data/agriculture/crop_yields.csv")

# Vector example - penguin bill lengths
bill_lengths <- na.omit(penguins$bill_length_mm[1:10])
print(bill_lengths)

# Matrix example - create a matrix from penguin measurements
penguin_matrix <- as.matrix(penguins[1:5, 3:6])
print(penguin_matrix)

# Data frame example - first few rows of penguin data
penguin_data <- penguins[1:5, ]
print(penguin_data)

# List example - store different aspects of the dataset
penguin_summary <- list(
  species = unique(penguins$species),
  avg_bill_length = mean(penguins$bill_length_mm, na.rm = TRUE),
  sample_size = nrow(penguins),
  years = unique(penguins$year)
)
print(penguin_summary)
```

## Importing Data

### Reading Data Files

R provides several functions for importing data from different file formats:

```{r}
# CSV files - Palmer Penguins dataset
penguins_csv <- read.csv("../data/environmental/climate_data.csv")
head(penguins_csv, 3)

# Using the tidyverse approach for better handling
library(tidyverse)
penguins_tidy <- readr::read_csv("../data/environmental/climate_data.csv")
head(penguins_tidy, 3)

# Crop yields dataset
crops_csv <- read.csv("../data/agriculture/crop_yields.csv")
head(crops_csv, 3)
```

### Exploring Real-World Datasets

Let's explore some of the real-world datasets we have available:

```{r}
# Palmer Penguins dataset
penguins <- read_csv("../data/environmental/climate_data.csv")
glimpse(penguins)

# Basic summary statistics
summary(penguins$bill_length_mm)
summary(penguins$flipper_length_mm)

# Crop yields dataset
crops <- read_csv("../data/agriculture/crop_yields.csv")
glimpse(crops)
```

## Data Cleaning and Preparation

### Handling Missing Values

Missing values are common in scientific datasets and need to be addressed before analysis:

```{r}
# Check for missing values in the penguins dataset
sum(is.na(penguins))
colSums(is.na(penguins))

# Create a complete cases dataset
penguins_complete <- na.omit(penguins)
print(paste("Original dataset rows:", nrow(penguins)))
print(paste("Complete cases rows:", nrow(penguins_complete)))

# Replace missing values with the mean for numeric columns
penguins_imputed <- penguins
penguins_imputed$bill_length_mm[is.na(penguins_imputed$bill_length_mm)] <- 
  mean(penguins_imputed$bill_length_mm, na.rm = TRUE)
penguins_imputed$bill_depth_mm[is.na(penguins_imputed$bill_depth_mm)] <- 
  mean(penguins_imputed$bill_depth_mm, na.rm = TRUE)

# Check if missing values were replaced
sum(is.na(penguins_imputed$bill_length_mm))
```

### Data Transformation

Often, you'll need to transform variables to meet statistical assumptions or for better visualization:

```{r}
# Load the biodiversity dataset
biodiversity <- read_csv("../data/ecology/biodiversity.csv")
glimpse(biodiversity)

# Log transformation of a skewed variable (if available)
if("n" %in% colnames(biodiversity)) {
  biodiversity$log_n <- log(biodiversity$n + 1)  # Add 1 to handle zeros
  
  # Compare original and transformed
  summary(biodiversity$n)
  summary(biodiversity$log_n)
}

# Standardization (z-score) of penguin measurements
penguins_std <- penguins %>%
  mutate(
    bill_length_std = scale(bill_length_mm),
    flipper_length_std = scale(flipper_length_mm),
    body_mass_std = scale(body_mass_g)
  )

# View the first few rows of the transformed data
head(select(penguins_std, species, bill_length_mm, bill_length_std, 
             flipper_length_mm, flipper_length_std), 5)
```

### Creating New Variables

Creating new variables from existing ones is a common data preparation task:

```{r}
# Create new variables in the penguins dataset
penguins_derived <- penguins %>%
  filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm)) %>%
  mutate(
    bill_ratio = bill_length_mm / bill_depth_mm,
    size_category = case_when(
      body_mass_g < 3500 ~ "Small",
      body_mass_g < 4500 ~ "Medium",
      TRUE ~ "Large"
    )
  )

# View the new variables
head(select(penguins_derived, species, bill_length_mm, bill_depth_mm, 
             bill_ratio, body_mass_g, size_category), 5)
```

## Data Manipulation with dplyr

The dplyr package provides a powerful grammar for data manipulation:

```{r}
library(dplyr)

# Filter rows - only Adelie penguins
adelie_penguins <- penguins %>%
  filter(species == "Adelie")
head(adelie_penguins, 3)

# Select columns - focus on measurements
penguin_measurements <- penguins %>%
  select(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
head(penguin_measurements, 3)

# Create new variables
penguins_analyzed <- penguins %>%
  mutate(
    bill_ratio = bill_length_mm / bill_depth_mm,
    body_mass_kg = body_mass_g / 1000
  )
head(select(penguins_analyzed, species, bill_ratio, body_mass_kg), 3)

# Summarize data by species
penguin_summary <- penguins %>%
  group_by(species) %>%
  summarize(
    count = n(),
    avg_bill_length = mean(bill_length_mm, na.rm = TRUE),
    avg_bill_depth = mean(bill_depth_mm, na.rm = TRUE),
    avg_body_mass = mean(body_mass_g, na.rm = TRUE)
  ) %>%
  arrange(desc(avg_body_mass))
print(penguin_summary)

# Analyze crop yields data
crop_summary <- crops %>%
  filter(!is.na(`Wheat (tonnes per hectare)`)) %>%
  group_by(Entity) %>%
  summarize(
    years_recorded = n(),
    avg_wheat_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE),
    max_wheat_yield = max(`Wheat (tonnes per hectare)`, na.rm = TRUE)
  ) %>%
  arrange(desc(avg_wheat_yield)) %>%
  head(10)  # Top 10 countries by average wheat yield

print(crop_summary)
```

## Exploratory Data Analysis

Before diving into formal statistical tests, it's essential to explore your data:

```{r}
# Basic summary statistics
summary(penguins$bill_length_mm)
summary(penguins$flipper_length_mm)
summary(penguins$body_mass_g)

# Correlation between variables
cor_matrix <- cor(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g),
  use = "complete.obs"
)
print(cor_matrix)

# Basic visualization - histogram of bill lengths
library(ggplot2)
ggplot(penguins, aes(x = bill_length_mm)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Penguin Bill Lengths",
       x = "Bill Length (mm)",
       y = "Frequency") +
  theme_minimal()

# Boxplot of body mass by species
ggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +
  geom_boxplot() +
  labs(title = "Body Mass by Penguin Species",
       x = "Species",
       y = "Body Mass (g)") +
  theme_minimal() +
  theme(legend.position = "none")

# Scatterplot of bill length vs. flipper length
ggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm, color = species)) +
  geom_point(alpha = 0.7) +
  labs(title = "Bill Length vs. Flipper Length",
       x = "Flipper Length (mm)",
       y = "Bill Length (mm)") +
  theme_minimal()
```

## Summary

In this chapter, we've covered the basics of working with data in R:

- Understanding different data types and structures
- Importing data from various file formats
- Cleaning and preparing data for analysis
- Creating new variables
- Using dplyr for powerful data manipulation
- Conducting initial exploratory data analysis

These skills form the foundation for all the analyses we'll perform in the subsequent chapters. By mastering these basics, you'll be well-prepared to tackle more complex analytical challenges in various scientific fields.

## Exercises

1. Load the Palmer Penguins dataset (`../data/environmental/climate_data.csv`) and create a summary of the number of penguins by species and island.
2. Calculate the mean and standard deviation of bill length, bill depth, and body mass for each penguin species.
3. Create a new variable that represents the ratio of flipper length to body mass. Interpret what this ratio might represent biologically.
4. Create a visualization that shows the relationship between bill length and bill depth, colored by species.
5. Load the crop yields dataset (`../data/agriculture/crop_yields.csv`) and analyze trends in wheat yields over time for a country of your choice.
6. Compare the distributions of body mass between male and female penguins using appropriate visualizations.
