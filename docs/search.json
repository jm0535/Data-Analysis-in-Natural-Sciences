[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "",
    "text": "Preface\nWelcome to Data Analysis in Natural Sciences: An R-Based Approach, a comprehensive guide designed for students, professionals, and researchers across the natural sciences. This book provides practical methods for analyzing and visualizing data using R, with applications spanning forestry, agriculture, ecology, marine biology, environmental science, geology, atmospheric science, hydrology, and more.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "About the Author",
    "text": "About the Author\nThis book has been developed by Dr. Jimmy Moses (PhD) from the School of Forestry, Faculty of Natural Resources, Papua New Guinea University of Technology. With extensive experience in ecological research and data analysis, Dr. Moses has created this resource to support students and researchers in developing essential analytical skills for natural science disciplines.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Target Audience",
    "text": "Target Audience\nThis book is designed for:\n\nUndergraduate and postgraduate students in natural science disciplines\nResearchers seeking to enhance their data analysis capabilities\nTechnicians working in laboratories and field settings\nProfessionals in government agencies, NGOs, and private sector\nHobbyists with an interest in analyzing scientific data\n\nThe content is relevant to those working in:\n\nForestry and agroforestry\nAgriculture and agronomy\nEcology and conservation\nEnvironmental science\nGeography and GIS/remote sensing\nMarine biology and fisheries\nBotany and plant sciences\nEntomology and zoology\nEpidemiology and veterinary sciences\nGeology and earth sciences\nAtmospheric and climate sciences\nHydrology and water resources\nNatural resource management\nConservation biology",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nThis book will guide you through:\n\nThe fundamentals of data analysis with R\nData preparation and management techniques\nExploratory data analysis approaches\nStatistical hypothesis testing\nAdvanced visualization methods\nSpecialized analyses for environmental and scientific data\nReproducible research practices",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nThis book is designed to be both a learning resource and a reference guide. You can read it from start to finish to build your skills progressively, or use specific chapters as needed for particular tasks.\nCode examples are provided throughout, and you can run them directly in R or RStudio. Each chapter includes practical examples using real datasets from various natural science disciplines.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo get the most out of this book, you should have:\n\nBasic computer skills\nR and RStudio installed (instructions provided in Chapter 1)\nA basic understanding of statistics (helpful but not required)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to thank all those who contributed to the development of this book, including colleagues, students, and the open-source community that makes tools like R and RStudio possible.\nLet’s begin our journey into the world of data analysis for natural sciences!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "About This Book",
    "section": "",
    "text": "About the Author\nJimmy Moses is a Papua New Guinean entomologist and lecturer at the Papua New Guinea University of Technology’s School of Forestry, specializing in ant ecology, biostatistics, and geospatial analysis. He holds a Ph.D. in Entomology from the University of South Bohemia (2021) and has extensive experience in tropical ecology research, with a particular focus on ant communities along elevational gradients.\nAs an active researcher and educator, the author currently supervises several master’s students and co-supervises a Ph.D. student, bringing substantial expertise to both research methodology and educational practices. His research combines ecological field studies with modern analytical approaches, bridging the gap between traditional field ecology and contemporary data science. This interdisciplinary approach has resulted in several publications in high-impact journals, including Global Ecology and Biogeography and Proceedings of the Royal Society B.\nThe author possesses a diverse technical skillset that uniquely qualifies him to author this book:\nHis international research collaborations span institutions in the Czech Republic, Germany, and Belgium. He has been an integral contributor to the New Guinea Binatang Research Center, supporting both research initiatives and educational programs in Papua New Guinea.\nWhen not teaching or conducting research, the author enjoys reading across disciplines—from technical manuals to ecological texts—and experimenting with code to solve practical problems in data analysis. His passion for making complex analytical methods accessible to researchers in the natural sciences drives both his teaching and his writing, including this book.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#about-the-author",
    "href": "preface.html#about-the-author",
    "title": "About This Book",
    "section": "",
    "text": "Advanced proficiency in R and Python for statistical computing and data science\nExpert knowledge of GIS and satellite remote sensing for spatial data analysis\nStrong foundation in biostatistics and experimental design applied to ecological research\nDeveloping expertise in full-stack application development for scientific computing",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#purpose-and-scope",
    "href": "preface.html#purpose-and-scope",
    "title": "About This Book",
    "section": "Purpose and Scope",
    "text": "Purpose and Scope\nThis book is designed to serve as both a learning resource and a reference guide for data analysis in the natural sciences, with applications spanning forestry, agriculture, ecology, environmental science, marine biology, and related disciplines. Whether you’re a student, researcher, technician, professional, or hobbyist in these fields, this book will help you develop the skills needed to analyze and visualize data effectively using R.\nThe focus is on practical applications rather than theoretical statistics, with an emphasis on techniques commonly used across natural science disciplines. By working through this book, you will:\n\nMaster the fundamentals of data analysis in R\nLearn to import, clean, and organize various types of scientific data\nDevelop skills in exploratory data analysis and visualization\nApply appropriate statistical tests for different research questions\nCreate publication-quality visualizations\nImplement reproducible research workflows\nInterpret and communicate results effectively",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#features-of-this-book",
    "href": "preface.html#features-of-this-book",
    "title": "About This Book",
    "section": "Features of This Book",
    "text": "Features of This Book\nThis book includes:\n\nStep-by-step instructions for R with complete code examples\nPractical examples using real datasets from various natural science disciplines\nExercises to reinforce learning and build skills\nTips and best practices from experienced researchers\nReproducible code that can be adapted for your own research\nProfessional formatting of data and model outputs",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#professional-data-and-model-output-formatting",
    "href": "preface.html#professional-data-and-model-output-formatting",
    "title": "About This Book",
    "section": "Professional Data and Model Output Formatting",
    "text": "Professional Data and Model Output Formatting\nThroughout this book, we use several R packages to create professionally formatted tables and model outputs suitable for publications:\n\nknitr: The core package for dynamic report generation, allowing seamless integration of R code with text\nkableExtra: For creating elegant, publication-quality tables with customizable styling\ngt: For producing beautiful, highly customizable tables with advanced formatting options\nbroom: For converting statistical model outputs into tidy data frames that are easier to work with\nsjPlot: For creating publication-ready tables and plots from statistical models\ngtsummary: For creating publication-ready analytical and summary tables\nflextable: For creating tables that work well across different output formats (HTML, PDF, Word)\n\nThese tools help transform raw data and complex statistical outputs into clear, professional presentations. Each chapter demonstrates how to use these packages to format your results effectively, following best practices in scientific publishing. You’ll learn to:\n\nFormat regression tables with proper statistical notation\nCreate elegant summary tables with appropriate precision and units\nGenerate publication-ready ANOVA tables\nDesign custom table themes that match your publication requirements\nExport formatted tables to various formats (HTML, PDF, Word)\n\nThe code examples throughout the book show not just how to perform analyses, but how to present the results professionally—a critical skill for scientific communication.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#how-to-use-the-code-examples",
    "href": "preface.html#how-to-use-the-code-examples",
    "title": "About This Book",
    "section": "How to Use the Code Examples",
    "text": "How to Use the Code Examples\nAll code examples in this book are written in R and can be executed in RStudio. To use the examples:\n\nMake sure you have R and RStudio installed (see Chapter 1 for installation instructions)\nInstall the required packages mentioned at the beginning of each chapter\nCopy and paste the code into your R console or script editor\nModify the code as needed for your own data\n\nThe datasets used in the examples are available in the docs/data directory of the book’s repository and are properly cited throughout the text.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#software-requirements",
    "href": "preface.html#software-requirements",
    "title": "About This Book",
    "section": "Software Requirements",
    "text": "Software Requirements\nThis book uses:\n\nR (version 4.0.0 or higher)\nRStudio (latest version recommended)\nVarious R packages (installation instructions provided in each chapter)",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#feedback-and-contributions",
    "href": "preface.html#feedback-and-contributions",
    "title": "About This Book",
    "section": "Feedback and Contributions",
    "text": "Feedback and Contributions\nYour feedback is valuable for improving future editions of this book. If you find errors, have suggestions, or want to contribute examples, please submit them through the book’s repository or contact the author directly.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#acknowledgments",
    "href": "preface.html#acknowledgments",
    "title": "About This Book",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to express my gratitude to colleagues, students, and the broader R community whose insights and feedback have contributed to the development of this book. Special thanks to the creators and maintainers of the R packages used throughout this book, as well as the data providers whose datasets make the examples both practical and relevant.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "1  Introduction to Data Analysis",
    "section": "",
    "text": "1.1 Overview\nData analysis is a critical skill in modern natural sciences research (Wickham & Grolemund, 2016; Zuur et al., 2009). This chapter introduces the fundamental concepts, tools, and approaches that form the foundation of effective data analysis across various scientific disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-data-analysis-matters-in-natural-sciences",
    "href": "chapters/01-introduction.html#why-data-analysis-matters-in-natural-sciences",
    "title": "1  Introduction to Data Analysis",
    "section": "1.2 Why Data Analysis Matters in Natural Sciences",
    "text": "1.2 Why Data Analysis Matters in Natural Sciences\nData analysis plays a pivotal role in natural sciences research for several reasons:\n\nEvidence-Based Decision Making: Data analysis transforms raw observations into actionable insights, enabling researchers and practitioners to make informed decisions about conservation strategies, resource management practices, agricultural planning, environmental interventions, and more (Bolker et al., 2009).\nPattern Recognition: Through statistical analysis, researchers can identify patterns, trends, and relationships within natural systems that might not be apparent from casual observation alone (Zuur et al., 2007). This applies to diverse fields including ecology, geology, marine biology, atmospheric science, and agriculture.\nHypothesis Testing: Data analysis provides rigorous methods to test hypotheses about natural phenomena, allowing researchers to build and refine scientific theories about how natural systems function (Gotelli & Ellison, 2004). This is fundamental across all scientific disciplines.\nPrediction and Modeling: Advanced analytical techniques enable the development of predictive models that can forecast changes in natural systems, such as species distribution shifts under climate change, crop yield predictions, geological processes, weather patterns, and more (Elith et al., 2009).\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Principles of Robust Experimental Design\n\n\n\nBefore diving into data analysis, ensure your experimental design follows these key principles:\n\nFormulate clear hypotheses: Define specific, testable hypotheses before collecting data\nControl for confounding variables: Identify and account for factors that might influence your results\nRandomize appropriately: Randomly assign treatments to experimental units to reduce bias\nInclude adequate replication: Ensure sufficient sample sizes for statistical power (use power analysis)\nConsider spatial and temporal scales: Match your sampling design to the scales of the processes being studied\nPlan for appropriate controls: Include positive, negative, and procedural controls as needed\nUse factorial designs when appropriate: Efficiently test multiple factors and their interactions\nConsider blocking: Group experimental units to account for known sources of variation\nPre-register your study: Document your hypotheses and analysis plan before collecting data\nPlan for appropriate statistical analysis: Select statistical methods based on your design, not just your results",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#tools-for-data-analysis",
    "href": "chapters/01-introduction.html#tools-for-data-analysis",
    "title": "1  Introduction to Data Analysis",
    "section": "1.3 Tools for Data Analysis",
    "text": "1.3 Tools for Data Analysis\nThis book focuses on R and RStudio as the primary tools for data analysis:\n\n1.3.1 R and RStudio\nR is a powerful programming language and environment specifically designed for statistical computing and graphics. RStudio is an integrated development environment (IDE) that makes working with R more accessible and efficient.\nKey advantages of R include:\n\nOpen-source and free: Available to anyone without cost\nExtensive package ecosystem: Thousands of specialized packages for various types of analyses across all scientific disciplines\nReproducibility: Code-based approach ensures analyses can be repeated and verified\nFlexibility: Can be adapted to virtually any analytical need in the natural sciences\nActive community: Large user base provides support and continuous development\n\n\n\nCode\n# A simple example of R code using real-world data\n# Load the Palmer penguins dataset (a subset of climate_data.csv)\npenguins &lt;- read.csv(\"../data/environmental/climate_data.csv\")\n\n# View the first few rows\nhead(penguins)\n\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n6  Adelie Torgersen           39.3          20.6               190        3650\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n6   male 2007\n\n\nCode\n# Get a summary of bill length measurements\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates basic data loading and exploration in R:\n\nData Loading:\n\nread.csv() imports data from a CSV file\nThe path “../data/environmental/climate_data.csv” points to the data file\n\nData Exploration:\n\nhead() displays the first 6 rows of the dataset\nsummary() provides statistical summaries of the bill length measurements\n\nVariable Access:\n\nThe $ operator accesses the bill_length_mm column from the penguins data frame\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe output shows:\n\nData Structure:\n\nThe dataset contains multiple columns including species, island, bill measurements, and body mass\nEach row represents a single penguin measurement\n\nBill Length Statistics:\n\nMinimum: 32.10 mm\nMaximum: 59.60 mm\nMean: 43.92 mm\nMedian: 44.45 mm\n2 missing values (NA’s)\n\nData Quality:\n\nThe presence of missing values suggests the need for data cleaning\nThe range of values appears reasonable for penguin bill measurements\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Data Loading Best Practices\n\n\n\nWhen loading data in R:\n\nFile Organization:\n\nKeep data files in a dedicated directory (e.g., “data/”)\nUse clear, descriptive file names\nMaintain consistent file naming conventions\n\nData Import:\n\nAlways check file paths are correct\nVerify data format matches expectations\nConsider using readr package for more robust data import\n\nInitial Checks:\n\nExamine data structure with str()\nCheck for missing values\nVerify data types are correct\nLook for obvious errors or outliers",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#setting-up-your-environment",
    "href": "chapters/01-introduction.html#setting-up-your-environment",
    "title": "1  Introduction to Data Analysis",
    "section": "1.4 Setting Up Your Environment",
    "text": "1.4 Setting Up Your Environment\n\n1.4.1 Installing R and RStudio\nTo install R and RStudio:\n\nDownload and install R from CRAN\nDownload and install RStudio from RStudio’s website\n\n\n\n1.4.2 Essential R Packages\nFor the analyses in this book, you’ll need several R packages. You can install them with the following code:\n\n\nCode\ninstall.packages(c(\n  \"tidyverse\",  # Data manipulation and visualization\n  \"rstatix\",    # Statistical tests\n  \"ggplot2\",    # Advanced plotting\n  \"knitr\",      # Document generation\n  \"rmarkdown\"   # Document formatting\n))",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#the-data-analysis-workflow",
    "href": "chapters/01-introduction.html#the-data-analysis-workflow",
    "title": "1  Introduction to Data Analysis",
    "section": "1.5 The Data Analysis Workflow",
    "text": "1.5 The Data Analysis Workflow\nEffective data analysis typically follows a structured workflow:\n\nDefine the Question: Clearly articulate what you want to learn from your data\nCollect Data: Gather the necessary data through fieldwork, experiments, laboratory measurements, or existing datasets\nClean and Prepare Data: Handle missing values, correct errors, and format data appropriately\nExplore Data: Conduct exploratory data analysis to understand patterns and distributions\nAnalyze Data: Apply appropriate statistical methods to address your research questions\nInterpret Results: Draw conclusions based on your analysis\nCommunicate Findings: Present your results through visualizations, reports, or publications\n\nThroughout this book, we’ll follow this workflow as we explore various datasets from across the natural sciences.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#types-of-data-in-natural-sciences-research",
    "href": "chapters/01-introduction.html#types-of-data-in-natural-sciences-research",
    "title": "1  Introduction to Data Analysis",
    "section": "1.6 Types of Data in Natural Sciences Research",
    "text": "1.6 Types of Data in Natural Sciences Research\nResearch across the natural sciences involves several types of data:\n\n1.6.1 Categorical Data\nCategorical data represent qualitative characteristics, such as: - Species names or taxonomic classifications - Habitat or ecosystem types - Rock or soil classifications - Land-use categories - Treatment groups in experiments - Genetic markers\n\n\n1.6.2 Numerical Data\nNumerical data involve measurements or counts: - Continuous measurements (e.g., temperature, pH, concentration, biomass, wavelength) - Discrete counts (e.g., number of individuals, species richness, occurrence frequency) - Rates (e.g., growth rates, reaction rates, decomposition rates) - Ratios and indices (e.g., diversity indices, chemical ratios)\n\n\n1.6.3 Spatial Data\nSpatial data describe geographical distributions: - Coordinates (latitude/longitude) - Elevation or depth - Topographic features - Land cover maps - Remote sensing data - Geological formations\n\n\n1.6.4 Temporal Data\nTemporal data track changes over time: - Time series of measurements - Seasonal patterns - Long-term monitoring data - Growth curves - Decay rates - Historical records\nUnderstanding the type of data you’re working with is crucial for selecting appropriate analytical methods across all natural science disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#summary",
    "href": "chapters/01-introduction.html#summary",
    "title": "1  Introduction to Data Analysis",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nIn this chapter, we’ve introduced the importance of data analysis in natural sciences research and the tools we’ll be using throughout this book. We’ve also outlined the typical data analysis workflow and the types of data commonly encountered across scientific disciplines.\nIn the next chapter, we’ll dive deeper into data basics, learning how to import, clean, and prepare data for analysis.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#exercises",
    "href": "chapters/01-introduction.html#exercises",
    "title": "1  Introduction to Data Analysis",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\n\nInstall R and RStudio on your computer.\nInstall the required R packages listed in this chapter.\nOpen RStudio and create a new R script. Try running a simple command like summary(iris).\nThink about a research question in your field of natural science that interests you. What type of data would you need to address this question?\nExplore one of R’s built-in datasets (e.g., mtcars, iris, or trees) using functions like head(), summary(), and plot().\n\n\n\n\n\n\n\nBolker, B. et al. (2009). Generalized linear mixed models: A practical guide. Trends in Ecology & Evolution.\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species distribution models: Ecological explanation and prediction across space and time. Annual Review of Ecology, Evolution, and Systematics, 40, 677–697.\n\n\nGotelli, N. J., & Ellison, A. M. (2004). Null model analysis of species co-occurrence patterns. Sinauer Associates.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.\n\n\nZuur, A., Ieno, E. N., & Smith, G. M. (2007). Analyzing ecological data. Springer.\n\n\nZuur, A., Ieno, E. N., Walker, N., Saveliev, A. A., & Smith, G. M. (2009). Mixed effects models and extensions in ecology with r. Springer Science & Business Media.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html",
    "href": "chapters/02-data-basics.html",
    "title": "2  Data Basics",
    "section": "",
    "text": "2.1 Introduction\nThis chapter covers the fundamental concepts of working with data in R. You’ll learn how to import, clean, and prepare data for analysis, which are essential skills for any data analysis project across all natural science disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#understanding-data-structures",
    "href": "chapters/02-data-basics.html#understanding-data-structures",
    "title": "2  Data Basics",
    "section": "2.2 Understanding Data Structures",
    "text": "2.2 Understanding Data Structures\nBefore diving into data analysis, it’s important to understand the basic data structures in R:\n\n2.2.1 Data Types\nR has several basic data types:\n\nNumeric: Decimal values (e.g., measurements of temperature, pH, concentration, or distance)\nInteger: Whole numbers (e.g., counts of organisms, samples, or observations)\nCharacter: Text strings (e.g., species names, site descriptions, or treatment labels)\nLogical: TRUE/FALSE values (e.g., presence/absence data or condition met/not met)\nFactor: Categorical variables with levels (e.g., experimental treatments, taxonomic classifications, or soil types)\nDate/Time: Temporal data (e.g., sampling dates, observation times, or seasonal markers)\n\n\n\nCode\n# Examples of different data types\nnumeric_example &lt;- 25.4  # Temperature in Celsius\ncharacter_example &lt;- \"Adelie\"  # Penguin species\nlogical_example &lt;- TRUE  # Presence/absence data\nfactor_example &lt;- factor(c(\"Control\", \"Treatment\", \"Control\"),\n                         levels = c(\"Control\", \"Treatment\"))\ndate_example &lt;- as.Date(\"2020-07-15\")  # Sampling date\n\n# Print examples\nprint(numeric_example)\n\n\n[1] 25.4\n\n\nCode\nprint(character_example)\n\n\n[1] \"Adelie\"\n\n\nCode\nprint(logical_example)\n\n\n[1] TRUE\n\n\nCode\nprint(factor_example)\n\n\n[1] Control   Treatment Control  \nLevels: Control Treatment\n\n\nCode\nprint(date_example)\n\n\n[1] \"2020-07-15\"\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates the fundamental data types in R:\n\nNumeric Data:\n\nCreated a decimal value representing temperature in Celsius\nCommon for environmental measurements and continuous data\n\nCharacter Data:\n\nCreated a text string for a penguin species name\nUsed for categorical variables like species names or site descriptions\n\nLogical Data:\n\nCreated a TRUE value representing presence/absence\nUsed for binary data or conditions in analyses\n\nFactor Data:\n\nCreated an ordered categorical variable with two levels\nExplicitly defined factor levels (“Control” and “Treatment”)\nEssential for statistical analyses and proper plotting order\n\nDate Data:\n\nCreated a date object using the as.Date() function\nUsed for temporal data in ecological studies\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe output shows how R stores and displays different data types:\n\nNumeric Data:\n\nDisplayed as 25.4 without type indication\nR treats this as a continuous numeric value for calculations\n\nCharacter Data:\n\nDisplayed as “Adelie” with quotation marks indicating text\nCannot be used for numerical operations\n\nLogical Data:\n\nDisplayed as TRUE (without quotation marks)\nCan be used in conditional operations and converts to 1 (TRUE) or 0 (FALSE) in calculations\n\nFactor Data:\n\nDisplayed with levels information: Control, Treatment, Control\nInternally stored as integers with labels\nOrder of levels is preserved as specified\n\nDate Data:\n\nDisplayed in standardized YYYY-MM-DD format\nAllows for time-based calculations and comparisons\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Data Management Best Practices\n\n\n\nProper data management is critical for reproducible research in natural sciences:\n\nDocument metadata: Always maintain detailed records about data collection methods, units, and variable definitions\nUse consistent naming conventions: Create clear, consistent file and variable names (e.g., site_01_temp_2023.csv instead of data1.csv)\nPreserve raw data: Never modify your original data files; always work with copies for cleaning and analysis\nVersion control: Use Git or similar tools to track changes to your data processing scripts\nImplement quality control: Create automated checks for impossible values, outliers, and inconsistencies\nPlan for missing data: Develop a consistent strategy for handling missing values before analysis begins\nCreate tidy data: Structure data with one observation per row and one variable per column\nUse open formats: Store data in non-proprietary formats (CSV, TSV) for long-term accessibility\nBack up regularly: Maintain multiple copies of your data in different physical locations\nConsider data repositories: Share your data through repositories like Dryad, Zenodo, or discipline-specific databases\n\n\n\n\n\n2.2.2 Data Structures in R\nR has several data structures for organizing information:\n\n\nCode\n# Load real datasets\nlibrary(readr)\npenguins &lt;- read_csv(\"../data/environmental/climate_data.csv\")\ncrops &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# Vector example - penguin bill lengths\nbill_lengths &lt;- na.omit(penguins$bill_length_mm[1:10])\nprint(bill_lengths)\n\n\n[1] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 34.1 42.0\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n\nCode\n# Matrix example - create a matrix from penguin measurements\npenguin_matrix &lt;- as.matrix(penguins[1:5, 3:6])\nprint(penguin_matrix)\n\n\n     bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n[1,]           39.1          18.7               181        3750\n[2,]           39.5          17.4               186        3800\n[3,]           40.3          18.0               195        3250\n[4,]             NA            NA                NA          NA\n[5,]           36.7          19.3               193        3450\n\n\nCode\n# Data frame example - first few rows of penguin data\npenguin_data &lt;- penguins[1:5, ]\nprint(penguin_data)\n\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# List example - store different aspects of the dataset\npenguin_summary &lt;- list(\n  species = unique(penguins$species),\n  avg_bill_length = mean(penguins$bill_length_mm, na.rm = TRUE),\n  sample_size = nrow(penguins),\n  years = unique(penguins$year)\n)\nprint(penguin_summary)\n\n\n$species\n[1] \"Adelie\"    \"Gentoo\"    \"Chinstrap\"\n\n$avg_bill_length\n[1] 43.92193\n\n$sample_size\n[1] 344\n\n$years\n[1] 2007 2008 2009\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates the main data structures in R using real ecological datasets:\n\nData Loading:\n\nUses readr::read_csv() to import real datasets on penguins and crop yields\nLoads data with proper data types and handling\n\nVector Creation:\n\nCreates a numeric vector of bill lengths\nUses na.omit() to remove missing values\nSubsets only the first 10 values with [1:10]\n\nMatrix Construction:\n\nCreates a numeric matrix from penguin measurements\nUses as.matrix() to convert data frame columns to matrix\nSelects rows 1-5 and columns 3-6 using indexing\n\nData Frame Handling:\n\nDemonstrates a data frame (the most common data structure)\nShows how to subset rows while keeping all columns\n\nList Creation:\n\nCreates a list to store heterogeneous data elements\nContains different data types: character vector, numeric value, and integer\nDemonstrates how lists can store complex, nested information\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe output reveals the structure and properties of different R data types:\n\nVector Output:\n\nShows a one-dimensional array of bill length measurements\nAll elements are of the same type (numeric)\nSuitable for storing a single variable’s values\n\nMatrix Output:\n\nDisplays a two-dimensional array of measurements\nAll values must be of the same type (converted to numeric)\nRow and column indices are shown\nEfficient for mathematical operations but less flexible than data frames\n\nData Frame Output:\n\nShows a tabular structure with different variable types\nPreserves column names and data types\nThe foundation of most data analysis in R\nEach column can have a different data type\n\nList Output:\n\nDisplays a collection of disparate elements\nShows the flexibility of lists for storing mixed data\nDemonstrates named elements for easy access\nIdeal for storing complex results and heterogeneous data",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#importing-data",
    "href": "chapters/02-data-basics.html#importing-data",
    "title": "2  Data Basics",
    "section": "2.3 Importing Data",
    "text": "2.3 Importing Data\n\n2.3.1 Reading Data Files\nR provides several functions for importing data from different file formats:\n\n\nCode\n# CSV files - Palmer Penguins dataset\npenguins_csv &lt;- read.csv(\"../data/environmental/climate_data.csv\")\nhead(penguins_csv, 3)\n\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n\n\nCode\n# Using the tidyverse approach for better handling\nlibrary(tidyverse)\npenguins_tidy &lt;- readr::read_csv(\"../data/environmental/climate_data.csv\")\nhead(penguins_tidy, 3)\n\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# Crop yields dataset\ncrops_csv &lt;- read.csv(\"../data/agriculture/crop_yields.csv\")\nhead(crops_csv, 3)\n\n\n       Entity Code Year Wheat..tonnes.per.hectare. Rice..tonnes.per.hectare.\n1 Afghanistan  AFG 1961                     1.0220                     1.519\n2 Afghanistan  AFG 1962                     0.9735                     1.519\n3 Afghanistan  AFG 1963                     0.8317                     1.519\n  Maize..tonnes.per.hectare. Soybeans..tonnes.per.hectare.\n1                      1.400                            NA\n2                      1.400                            NA\n3                      1.426                            NA\n  Potatoes..tonnes.per.hectare. Beans..tonnes.per.hectare.\n1                        8.6667                         NA\n2                        7.6667                         NA\n3                        8.1333                         NA\n  Peas..tonnes.per.hectare. Cassava..tonnes.per.hectare.\n1                        NA                           NA\n2                        NA                           NA\n3                        NA                           NA\n  Barley..tonnes.per.hectare. Cocoa.beans..tonnes.per.hectare.\n1                        1.08                               NA\n2                        1.08                               NA\n3                        1.08                               NA\n  Bananas..tonnes.per.hectare.\n1                           NA\n2                           NA\n3                           NA\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates different methods for importing data in R:\n\nBase R Import:\n\nUses read.csv() from base R to import the penguin dataset\nSimple approach that works without additional packages\nGenerally slower for large datasets and less flexible with column types\n\nTidyverse Import:\n\nUses readr::read_csv() from the tidyverse ecosystem\nMore efficient for large datasets and better type inference\nMaintains consistent column types and handles problematic values better\n\nData Preview:\n\nUses head() with argument 3 to display just the first three rows\nAllows quick inspection of data structure without overwhelming output\nEssential first step to verify successful import and correct structure\n\nMultiple Datasets:\n\nDemonstrates importing different datasets (penguins and crop yields)\nShows the same approach works across various data sources\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe output reveals differences between import methods and gives insight into the datasets:\n\nData Structure Visibility:\n\nBoth datasets show proper column names and values\nThe tidyverse import (readr) provides cleaner output with column types\nTypes are indicated ( for numeric,  for character, etc.)\n\nImport Method Comparison:\n\nBase R (read.csv()) and tidyverse (read_csv()) produce similar results\nTidyverse version provides more metadata about column types\nBoth successfully imported the data with proper structure\n\nData Content Preview:\n\nPenguin data contains morphological measurements and categorical variables\nCrop yield data includes countries, years, and production statistics\nBoth datasets appear properly formatted for analysis\n\n\n\n\n\n\n2.3.2 Exploring Real-World Datasets\nLet’s explore some of the real-world datasets we have available:\n\n\nCode\n# Palmer Penguins dataset\npenguins &lt;- read_csv(\"../data/environmental/climate_data.csv\")\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nCode\n# Basic summary statistics\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCode\nsummary(penguins$flipper_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nCode\n# Crop yields dataset\ncrops &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\nglimpse(crops)\n\n\nRows: 13,075\nColumns: 14\n$ Entity                             &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afgh…\n$ Code                               &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", …\n$ Year                               &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966,…\n$ `Wheat (tonnes per hectare)`       &lt;dbl&gt; 1.0220, 0.9735, 0.8317, 0.9510, 0.9…\n$ `Rice (tonnes per hectare)`        &lt;dbl&gt; 1.5190, 1.5190, 1.5190, 1.7273, 1.7…\n$ `Maize (tonnes per hectare)`       &lt;dbl&gt; 1.4000, 1.4000, 1.4260, 1.4257, 1.4…\n$ `Soybeans (tonnes per hectare)`    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Potatoes (tonnes per hectare)`    &lt;dbl&gt; 8.6667, 7.6667, 8.1333, 8.6000, 8.8…\n$ `Beans (tonnes per hectare)`       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Peas (tonnes per hectare)`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Cassava (tonnes per hectare)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Barley (tonnes per hectare)`      &lt;dbl&gt; 1.0800, 1.0800, 1.0800, 1.0857, 1.0…\n$ `Cocoa beans (tonnes per hectare)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Bananas (tonnes per hectare)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#data-cleaning-and-preparation",
    "href": "chapters/02-data-basics.html#data-cleaning-and-preparation",
    "title": "2  Data Basics",
    "section": "2.4 Data Cleaning and Preparation",
    "text": "2.4 Data Cleaning and Preparation\n\n2.4.1 Handling Missing Values\nMissing values are common in scientific datasets and need to be addressed before analysis:\n\n\nCode\n# Check for missing values in the penguins dataset\nsum(is.na(penguins))\n\n\n[1] 19\n\n\nCode\ncolSums(is.na(penguins))\n\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0 \n\n\nCode\n# Create a complete cases dataset\npenguins_complete &lt;- na.omit(penguins)\nprint(paste(\"Original dataset rows:\", nrow(penguins)))\n\n\n[1] \"Original dataset rows: 344\"\n\n\nCode\nprint(paste(\"Complete cases rows:\", nrow(penguins_complete)))\n\n\n[1] \"Complete cases rows: 333\"\n\n\nCode\n# Replace missing values with the mean for numeric columns\npenguins_imputed &lt;- penguins\npenguins_imputed$bill_length_mm[is.na(penguins_imputed$bill_length_mm)] &lt;-\n  mean(penguins_imputed$bill_length_mm, na.rm = TRUE)\npenguins_imputed$bill_depth_mm[is.na(penguins_imputed$bill_depth_mm)] &lt;-\n  mean(penguins_imputed$bill_depth_mm, na.rm = TRUE)\n\n# Check if missing values were replaced\nsum(is.na(penguins_imputed$bill_length_mm))\n\n\n[1] 0\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates essential techniques for handling missing values in ecological data:\n\nMissing Value Detection:\n\nUses is.na() to identify missing values in the dataset\nsum(is.na()) counts the total number of missing values\ncolSums(is.na()) reports missing values per column\nCritical first step in data cleaning\n\nComplete Case Analysis:\n\nUses na.omit() to remove rows with any missing values\nCreates a new dataset (penguins_complete) with only complete rows\nCompares the row count before and after removal\nSimple but can lead to significant data loss\n\nMean Imputation:\n\nCreates a copy of the original dataset (penguins_imputed)\nReplaces missing values with column means\nUses logical indexing with is.na() to target only missing values\nCalculates means with na.rm = TRUE to ignore missing values\nVerifies imputation success with another missing value check\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe output reveals the extent and impact of missing data:\n\nMissing Data Quantity:\n\nThe total number of missing values in the dataset\nThe distribution of missing values across columns\nSome columns (like bill measurements) have more missing values than others\n\nData Loss Impact:\n\nThe original dataset has more rows than the complete cases dataset\nThe difference represents the number of incomplete observations\nIn ecological studies, this data loss can introduce bias if missingness isn’t random\n\nImputation Effectiveness:\n\nAfter imputation, specific columns no longer contain missing values\nThe final check (showing 0) confirms successful imputation\nThis approach preserves sample size but may reduce variability\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Handling Missing Values in Ecological Data\n\n\n\nWhen dealing with missing values in ecological datasets:\n\nUnderstand Missing Data Mechanisms:\n\nMCAR (Missing Completely At Random): Missingness unrelated to any variables (e.g., equipment failure)\nMAR (Missing At Random): Missingness related to observed variables (e.g., more missing values in certain species)\nMNAR (Missing Not At Random): Missingness related to the missing values themselves (e.g., very small values not detected)\n\nSelect Appropriate Handling Methods:\n\nComplete case analysis: Appropriate for MCAR data with few missing values\nMean/median imputation: Simple but can underestimate variance\nMultiple imputation: Creates several imputed datasets to account for uncertainty\nModel-based imputation: Uses relationships between variables to predict missing values\nMaximum likelihood: Estimates parameters directly from available data\n\nDocument and Report:\n\nAlways report the extent of missing data\nDocument your handling approach and rationale\nConsider sensitivity analyses with different approaches\nAcknowledge potential biases introduced by missing data handling\n\n\n\n\n\n\n2.4.2 Data Transformation\nOften, you’ll need to transform variables to meet statistical assumptions or for better visualization:\n\n\nCode\n# Load the biodiversity dataset\nbiodiversity &lt;- read_csv(\"../data/ecology/biodiversity.csv\")\nglimpse(biodiversity)\n\n\nRows: 500\nColumns: 24\n$ binomial_name     &lt;chr&gt; \"Abutilon pitcairnense\", \"Acaena exigua\", \"Acalypha …\n$ country           &lt;chr&gt; \"Pitcairn\", \"United States\", \"Congo\", \"Saint Helena,…\n$ continent         &lt;chr&gt; \"Oceania\", \"North America\", \"Africa\", \"Africa\", \"Oce…\n$ group             &lt;chr&gt; \"Flowering Plant\", \"Flowering Plant\", \"Flowering Pla…\n$ year_last_seen    &lt;chr&gt; \"2000-2020\", \"1980-1999\", \"1940-1959\", \"Before 1900\"…\n$ threat_AA         &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1…\n$ threat_BRU        &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0…\n$ threat_RCD        &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n$ threat_ISGD       &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_EPM        &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_CC         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_HID        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_P          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_TS         &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_NSM        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_GE         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_NA         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0…\n$ action_LWP        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ action_SM         &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_LP         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_RM         &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_EA         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_NA         &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ red_list_category &lt;chr&gt; \"Extinct in the Wild\", \"Extinct\", \"Extinct\", \"Extinc…\n\n\nCode\n# Log transformation of a skewed variable (if available)\nif(\"n\" %in% colnames(biodiversity)) {\n  biodiversity$log_n &lt;- log(biodiversity$n + 1)  # Add 1 to handle zeros\n\n  # Compare original and transformed\n  summary(biodiversity$n)\n  summary(biodiversity$log_n)\n}\n\n# Standardization (z-score) of penguin measurements\npenguins_std &lt;- penguins %&gt;%\n  mutate(\n    bill_length_std = scale(bill_length_mm),\n    flipper_length_std = scale(flipper_length_mm),\n    body_mass_std = scale(body_mass_g)\n  )\n\n# View the first few rows of the transformed data\nhead(select(penguins_std, species, bill_length_mm, bill_length_std,\n             flipper_length_mm, flipper_length_std), 5)\n\n\n# A tibble: 5 × 5\n  species bill_length_mm bill_length_std[,1] flipper_length_mm\n  &lt;chr&gt;            &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie            39.1              -0.883               181\n2 Adelie            39.5              -0.810               186\n3 Adelie            40.3              -0.663               195\n4 Adelie            NA                NA                    NA\n5 Adelie            36.7              -1.32                193\n# ℹ 1 more variable: flipper_length_std &lt;dbl[,1]&gt;\n\n\n\n\n2.4.3 Creating New Variables\nCreating new variables from existing ones is a common data preparation task:\n\n\nCode\n# Create new variables in the penguins dataset\npenguins_derived &lt;- penguins %&gt;%\n  filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm)) %&gt;%\n  mutate(\n    bill_ratio = bill_length_mm / bill_depth_mm,\n    size_category = case_when(\n      body_mass_g &lt; 3500 ~ \"Small\",\n      body_mass_g &lt; 4500 ~ \"Medium\",\n      TRUE ~ \"Large\"\n    )\n  )\n\n# View the new variables\nhead(select(penguins_derived, species, bill_length_mm, bill_depth_mm,\n             bill_ratio, body_mass_g, size_category), 5)\n\n\n# A tibble: 5 × 6\n  species bill_length_mm bill_depth_mm bill_ratio body_mass_g size_category\n  &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;        \n1 Adelie            39.1          18.7       2.09        3750 Medium       \n2 Adelie            39.5          17.4       2.27        3800 Medium       \n3 Adelie            40.3          18         2.24        3250 Small        \n4 Adelie            36.7          19.3       1.90        3450 Small        \n5 Adelie            39.3          20.6       1.91        3650 Medium",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#data-manipulation-with-dplyr",
    "href": "chapters/02-data-basics.html#data-manipulation-with-dplyr",
    "title": "2  Data Basics",
    "section": "2.5 Data Manipulation with dplyr",
    "text": "2.5 Data Manipulation with dplyr\nThe dplyr package provides a powerful grammar for data manipulation:\n\n\nCode\nlibrary(dplyr)\n\n# Filter rows - only Adelie penguins\nadelie_penguins &lt;- penguins %&gt;%\n  filter(species == \"Adelie\")\nhead(adelie_penguins, 3)\n\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# Select columns - focus on measurements\npenguin_measurements &lt;- penguins %&gt;%\n  select(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)\nhead(penguin_measurements, 3)\n\n\n# A tibble: 3 × 6\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n\n\nCode\n# Create new variables\npenguins_analyzed &lt;- penguins %&gt;%\n  mutate(\n    bill_ratio = bill_length_mm / bill_depth_mm,\n    body_mass_kg = body_mass_g / 1000\n  )\nhead(select(penguins_analyzed, species, bill_ratio, body_mass_kg), 3)\n\n\n# A tibble: 3 × 3\n  species bill_ratio body_mass_kg\n  &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Adelie        2.09         3.75\n2 Adelie        2.27         3.8 \n3 Adelie        2.24         3.25\n\n\nCode\n# Summarize data by species\npenguin_summary &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(\n    count = n(),\n    avg_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    avg_bill_depth = mean(bill_depth_mm, na.rm = TRUE),\n    avg_body_mass = mean(body_mass_g, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(avg_body_mass))\nprint(penguin_summary)\n\n\n# A tibble: 3 × 5\n  species   count avg_bill_length avg_bill_depth avg_body_mass\n  &lt;chr&gt;     &lt;int&gt;           &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1 Gentoo      124            47.5           15.0         5076.\n2 Chinstrap    68            48.8           18.4         3733.\n3 Adelie      152            38.8           18.3         3701.\n\n\nCode\n# Analyze crop yields data\ncrop_summary &lt;- crops %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(\n    years_recorded = n(),\n    avg_wheat_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    max_wheat_yield = max(`Wheat (tonnes per hectare)`, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(avg_wheat_yield)) %&gt;%\n  head(10)  # Top 10 countries by average wheat yield\n\nprint(crop_summary)\n\n\n# A tibble: 10 × 4\n   Entity          years_recorded avg_wheat_yield max_wheat_yield\n   &lt;chr&gt;                    &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 Belgium                     19            8.54           10.0 \n 2 Netherlands                 58            7.03            9.29\n 3 Ireland                     58            6.83           10.7 \n 4 United Kingdom              58            6.37            8.98\n 5 Denmark                     58            6.18            8.24\n 6 Luxembourg                  19            5.98            6.82\n 7 Germany                     58            5.89            8.63\n 8 Europe, Western             58            5.72            7.88\n 9 France                      58            5.65            7.80\n10 Northern Europe             58            5.59            7.21",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#exploratory-data-analysis",
    "href": "chapters/02-data-basics.html#exploratory-data-analysis",
    "title": "2  Data Basics",
    "section": "2.6 Exploratory Data Analysis",
    "text": "2.6 Exploratory Data Analysis\nBefore diving into formal statistical tests, it’s essential to explore your data:\n\n\nCode\n# Basic summary statistics\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCode\nsummary(penguins$flipper_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nCode\nsummary(penguins$body_mass_g)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\n\nCode\n# Correlation between variables\ncor_matrix &lt;- cor(\n  penguins %&gt;%\n    select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g),\n  use = \"complete.obs\"\n)\nprint(cor_matrix)\n\n\n                  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\nbill_length_mm         1.0000000    -0.2350529         0.6561813   0.5951098\nbill_depth_mm         -0.2350529     1.0000000        -0.5838512  -0.4719156\nflipper_length_mm      0.6561813    -0.5838512         1.0000000   0.8712018\nbody_mass_g            0.5951098    -0.4719156         0.8712018   1.0000000\n\n\nCode\n# Basic visualization - histogram of bill lengths\nlibrary(ggplot2)\nggplot(penguins, aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Penguin Bill Lengths\",\n       x = \"Bill Length (mm)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Boxplot of body mass by species\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Body Mass by Penguin Species\",\n       x = \"Species\",\n       y = \"Body Mass (g)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# Scatterplot of bill length vs. flipper length\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.7) +\n  labs(title = \"Bill Length vs. Flipper Length\",\n       x = \"Flipper Length (mm)\",\n       y = \"Bill Length (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates several key data analysis and visualization techniques:\n\nSummary Statistics:\n\nsummary() provides descriptive statistics for each variable\nIncludes min, max, quartiles, mean, and missing values\n\nCorrelation Analysis:\n\ncor() calculates correlation coefficients between variables\nselect() chooses specific columns for analysis\nuse = \"complete.obs\" handles missing values\n\nVisualization Components:\n\nggplot() creates the base plot\naes() defines aesthetic mappings\ngeom_*() functions add different plot types\ntheme_minimal() applies a clean theme\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe analysis reveals several important insights:\n\nVariable Distributions:\n\nBill lengths show a roughly normal distribution\nBody mass varies significantly between species\nSome variables have missing values that need attention\n\nSpecies Differences:\n\nThe boxplot shows clear species-specific body mass patterns\nSome species show more variation than others\nPotential outliers are visible in the body mass data\n\nMorphological Relationships:\n\nThe scatterplot reveals correlations between bill and flipper lengths\nSpecies clusters are visible in the morphological space\nSome species show distinct morphological patterns\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Exploratory Data Analysis Best Practices\n\n\n\nWhen conducting exploratory data analysis:\n\nData Quality:\n\nAlways check for missing values first\nLook for outliers and potential errors\nVerify data types and ranges\n\nVisualization Strategy:\n\nStart with simple plots (histograms, boxplots)\nProgress to more complex visualizations\nUse appropriate plot types for your data\nConsider colorblind-friendly palettes\n\nStatistical Summary:\n\nCalculate both descriptive and inferential statistics\nConsider the distribution of your data\nLook for patterns and relationships\nDocument any unusual findings",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#summary",
    "href": "chapters/02-data-basics.html#summary",
    "title": "2  Data Basics",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nIn this chapter, we’ve covered the basics of working with data in R:\n\nUnderstanding different data types and structures\nImporting data from various file formats\nCleaning and preparing data for analysis\nCreating new variables\nUsing dplyr for powerful data manipulation\nConducting initial exploratory data analysis\n\nThese skills form the foundation for all the analyses we’ll perform in the subsequent chapters. By mastering these basics, you’ll be well-prepared to tackle more complex analytical challenges in various scientific fields.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#exercises",
    "href": "chapters/02-data-basics.html#exercises",
    "title": "2  Data Basics",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\n\nLoad the Palmer Penguins dataset (../data/environmental/climate_data.csv) and create a summary of the number of penguins by species and island.\nCalculate the mean and standard deviation of bill length, bill depth, and body mass for each penguin species.\nCreate a new variable that represents the ratio of flipper length to body mass. Interpret what this ratio might represent biologically.\nCreate a visualization that shows the relationship between bill length and bill depth, colored by species.\nLoad the crop yields dataset (../data/agriculture/crop_yields.csv) and analyze trends in wheat yields over time for a country of your choice.\nCompare the distributions of body mass between male and female penguins using appropriate visualizations.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html",
    "href": "chapters/03-exploratory-analysis.html",
    "title": "3  Exploratory Data Analysis",
    "section": "",
    "text": "3.1 Introduction\nExploratory Data Analysis (EDA) is a critical first step in any data analysis project. In this chapter, you’ll learn how to systematically explore your data to understand its structure, identify patterns, detect anomalies, and generate hypotheses for further investigation.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#the-purpose-of-exploratory-data-analysis",
    "href": "chapters/03-exploratory-analysis.html#the-purpose-of-exploratory-data-analysis",
    "title": "3  Exploratory Data Analysis",
    "section": "3.2 The Purpose of Exploratory Data Analysis",
    "text": "3.2 The Purpose of Exploratory Data Analysis\nExploratory Data Analysis serves several important purposes in natural sciences research:\n\nUnderstanding Data Structure: Gain insights into the basic properties of your dataset\nChecking Data Quality: Identify missing values, outliers, and potential errors\nDiscovering Patterns: Detect relationships, trends, and distributions\nGenerating Hypotheses: Develop questions and hypotheses for formal testing\nInforming Analysis Choices: Guide decisions about appropriate statistical methods\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Creating Reproducible EDA Workflows\n\n\n\nTo ensure your exploratory data analysis is reproducible and transparent:\n\nDocument all data transformations: Record every cleaning step, filter, and transformation applied to raw data\nUse R Markdown or Quarto: Create executable documents that combine code, output, and narrative explanation\nVersion control your analysis: Track changes to your EDA scripts using Git or similar tools\nSave exploratory outputs: Store key visualizations and summary statistics in organized directories\nCreate clear data lineage: Document the origin of each dataset and how it connects to derived datasets\nUse consistent naming conventions: Apply systematic naming to files, variables, and functions\nSeparate exploration from confirmation: Clearly distinguish exploratory analyses from confirmatory hypothesis testing\nInclude data validation checks: Incorporate automated checks for data integrity and quality\nProvide detailed method documentation: Document statistical approaches like ANOVA types (e.g., Type II tests for unbalanced designs)\nShare your EDA code: Make your exploratory scripts available alongside final analyses for complete transparency",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#summarizing-data",
    "href": "chapters/03-exploratory-analysis.html#summarizing-data",
    "title": "3  Exploratory Data Analysis",
    "section": "3.3 Summarizing Data",
    "text": "3.3 Summarizing Data\n\n3.3.1 Descriptive Statistics\nDescriptive statistics provide a concise summary of your data’s central tendency, dispersion, and shape:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the crop yield dataset\ncrop_yields &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# View the first few rows\nhead(crop_yields)\n\n\n# A tibble: 6 × 14\n  Entity      Code   Year `Wheat (tonnes per hectare)` Rice (tonnes per hectar…¹\n  &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;                        &lt;dbl&gt;                     &lt;dbl&gt;\n1 Afghanistan AFG    1961                        1.02                       1.52\n2 Afghanistan AFG    1962                        0.974                      1.52\n3 Afghanistan AFG    1963                        0.832                      1.52\n4 Afghanistan AFG    1964                        0.951                      1.73\n5 Afghanistan AFG    1965                        0.972                      1.73\n6 Afghanistan AFG    1966                        0.867                      1.52\n# ℹ abbreviated name: ¹​`Rice (tonnes per hectare)`\n# ℹ 9 more variables: `Maize (tonnes per hectare)` &lt;dbl&gt;,\n#   `Soybeans (tonnes per hectare)` &lt;dbl&gt;,\n#   `Potatoes (tonnes per hectare)` &lt;dbl&gt;, `Beans (tonnes per hectare)` &lt;dbl&gt;,\n#   `Peas (tonnes per hectare)` &lt;dbl&gt;, `Cassava (tonnes per hectare)` &lt;dbl&gt;,\n#   `Barley (tonnes per hectare)` &lt;dbl&gt;,\n#   `Cocoa beans (tonnes per hectare)` &lt;dbl&gt;, …\n\n\nCode\n# Get summary statistics for wheat yields\nwheat_summary &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  summarize(\n    Mean = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Median = median(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    StdDev = sd(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Min = min(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Max = max(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Q1 = quantile(`Wheat (tonnes per hectare)`, 0.25, na.rm = TRUE),\n    Q3 = quantile(`Wheat (tonnes per hectare)`, 0.75, na.rm = TRUE)\n  )\n\n# Display the summary statistics\nknitr::kable(wheat_summary, caption = \"Summary Statistics for Global Wheat Yields\")\n\n\n\nSummary Statistics for Global Wheat Yields\n\n\nMean\nMedian\nStdDev\nMin\nMax\nQ1\nQ3\n\n\n\n\n2.434914\n1.99\n1.687949\n0\n10.6677\n1.228\n3.1245\n\n\n\n\n\nCode\n# Visualize the distribution of wheat yields\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(bins = 30, fill = \"forestgreen\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Global Wheat Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Identify top wheat-producing countries (by average yield)\ntop_wheat_countries &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(Avg_Yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE)) %&gt;%\n  arrange(desc(Avg_Yield)) %&gt;%\n  head(10)\n\n# Display the top countries\nknitr::kable(top_wheat_countries, caption = \"Top 10 Countries by Average Wheat Yield\")\n\n\n\nTop 10 Countries by Average Wheat Yield\n\n\nEntity\nAvg_Yield\n\n\n\n\nBelgium\n8.544200\n\n\nNetherlands\n7.030172\n\n\nIreland\n6.829840\n\n\nUnited Kingdom\n6.366400\n\n\nDenmark\n6.175285\n\n\nLuxembourg\n5.977411\n\n\nGermany\n5.893978\n\n\nEurope, Western\n5.723267\n\n\nFrance\n5.645341\n\n\nNorthern Europe\n5.589988\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code block demonstrates how to calculate and visualize descriptive statistics:\n\nlibrary(tidyverse) loads the tidyverse collection of packages for data manipulation and visualization.\nread_csv() imports the crop yields dataset from a CSV file.\nhead() displays the first few rows to inspect the data structure.\nThe summarize() function calculates key statistics for wheat yields:\n\nMean: Average yield across all observations\nMedian: Middle value when yields are arranged in order\nStdDev: Standard deviation, measuring data dispersion\nMin/Max: Minimum and maximum values in the dataset\nQ1/Q3: First and third quartiles (25th and 75th percentiles)\n\nknitr::kable() creates a formatted table of the summary statistics.\nggplot() with geom_histogram() visualizes the distribution of wheat yields.\nThe final section identifies and displays the top 10 countries by average wheat yield using:\n\ngroup_by() to organize data by country\nsummarize() to calculate average yield per country\narrange(desc()) to sort in descending order\nhead(10) to select the top 10 entries\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe summary statistics reveal several key insights about global wheat yields:\n\nThe mean wheat yield is higher than the median, suggesting a right-skewed distribution with some countries achieving exceptionally high yields.\nThe standard deviation indicates substantial variability in wheat productivity across different regions.\nThe histogram confirms this skewed distribution, with most countries clustered at lower to moderate yield levels, and fewer countries achieving very high yields.\nThe top 10 countries table shows which nations have the most productive wheat cultivation systems, likely due to advanced agricultural practices, favorable climate conditions, or intensive farming methods.\nThis analysis provides a foundation for investigating factors that contribute to high wheat yields and potential strategies for improving agricultural productivity in lower-yielding regions.\n\n\n\n\n\n3.3.2 Frequency Tables\nFrequency tables are useful for understanding the distribution of categorical variables:\n\n\nCode\n# Let's create a categorical variable based on wheat yield levels\ncrop_yields_with_categories &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(yield_category = case_when(\n    `Wheat (tonnes per hectare)` &lt; 2 ~ \"Low\",\n    `Wheat (tonnes per hectare)` &gt;= 2 & `Wheat (tonnes per hectare)` &lt; 4 ~ \"Medium\",\n    `Wheat (tonnes per hectare)` &gt;= 4 ~ \"High\"\n  ))\n\n# Frequency table for yield categories\ntable(crop_yields_with_categories$yield_category)\n\n\n\n  High    Low Medium \n  1279   4081   2741 \n\n\nCode\n# Proportions\nprop.table(table(crop_yields_with_categories$yield_category))\n\n\n\n     High       Low    Medium \n0.1578817 0.5037650 0.3383533 \n\n\nCode\n# Create a decade variable for temporal analysis\ncrop_yields_with_categories &lt;- crop_yields_with_categories %&gt;%\n  mutate(decade = floor(Year / 10) * 10)\n\n# Two-way frequency table: yield category by decade\nyield_decade_table &lt;- table(crop_yields_with_categories$yield_category,\n                            crop_yields_with_categories$decade)\nyield_decade_table\n\n\n        \n         1960 1970 1980 1990 2000 2010\n  High     34  102  200  261  326  356\n  Low     838  833  760  681  563  406\n  Medium  239  335  344  550  656  617\n\n\nCode\n# Convert to proportions (by row)\nprop.table(yield_decade_table, margin = 1)\n\n\n        \n               1960       1970       1980       1990       2000       2010\n  High   0.02658327 0.07974980 0.15637217 0.20406568 0.25488663 0.27834246\n  Low    0.20534183 0.20411664 0.18622887 0.16687086 0.13795638 0.09948542\n  Medium 0.08719445 0.12221817 0.12550164 0.20065669 0.23932871 0.22510033\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code block demonstrates how to create and analyze frequency tables:\n\nmutate() with case_when() creates a new categorical variable yield_category by binning wheat yields into “Low,” “Medium,” and “High” categories.\ntable() produces a frequency count for each yield category.\nprop.table() converts the frequency counts to proportions (relative frequencies).\nA new variable decade is created by rounding down the year to the nearest decade using floor(Year / 10) * 10.\nA two-way frequency table is created to examine the relationship between yield categories and decades.\nprop.table(yield_decade_table, margin = 1) calculates row proportions, showing the distribution of decades within each yield category.\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe frequency tables reveal important patterns in wheat yield categories:\n\nThe distribution of yield categories shows which productivity levels are most common globally.\nThe proportions table quantifies this distribution, indicating what percentage of observations fall into each yield category.\nThe two-way table between yield categories and decades allows us to track how wheat productivity has changed over time.\nThe row proportions reveal whether certain yield categories have become more or less common in different decades, potentially indicating technological improvements, climate effects, or changes in agricultural practices.\nThese temporal patterns are crucial for understanding agricultural development trends and projecting future food security scenarios.\n\n\n\n\n\n3.3.3 Box Plots\nBox plots are excellent for comparing distributions across groups:\n\n\nCode\n# Select a few major countries for comparison\nmajor_wheat_producers &lt;- c(\"United States\", \"China\", \"India\", \"Russia\", \"France\", \"Australia\")\n\n# Filter data for these countries and recent years\nrecent_wheat_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% major_wheat_producers,\n         Year &gt;= 2000,\n         !is.na(`Wheat (tonnes per hectare)`))\n\n# Box plot of wheat yields by country\nggplot(recent_wheat_data, aes(x = Entity, y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Wheat Yields by Country (2000-present)\",\n       x = \"Country\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Enhanced box plot with jittered points\nggplot(recent_wheat_data, aes(x = Entity, y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.5) +\n  geom_jitter(width = 0.2, alpha = 0.5, color = \"darkgreen\") +\n  labs(title = \"Wheat Yields by Country (2000-present)\",\n       x = \"Country\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create and enhance box plots for comparing distributions:\n\nFirst, we select major wheat-producing countries for comparison and filter for recent data (since 2000).\nThe basic box plot:\n\ngeom_boxplot() creates a box-and-whisker plot for each country\nEach box shows the median (middle line), interquartile range (IQR, the box), and whiskers extending to 1.5 × IQR\nPoints beyond the whiskers represent outliers\n\nThe enhanced box plot adds:\n\ngeom_jitter() to display individual data points with slight horizontal displacement\nThis combination shows both the summary statistics (box plot) and the raw data distribution (points)\nangle = 45, hjust = 1 rotates country labels for better readability\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe box plots reveal important comparisons between major wheat-producing countries:\n\nThe median line in each box shows the typical yield for each country, allowing direct comparison of central tendency.\nThe height of each box (IQR) indicates the variability of yields within each country over the time period.\nWhisker length reflects the range of typical yields, while outlier points show exceptional years.\nCountries with higher boxes generally have more variable production, possibly due to climate fluctuations or changing agricultural practices.\nThe jittered points reveal the actual distribution and density of observations for each country.\nThese comparisons help identify which countries have the most consistent and productive wheat cultivation systems, providing insights for agricultural policy and development.\n\n\n\n\n\n3.3.4 Bar Charts\nBar charts are useful for visualizing categorical data:\n\n\nCode\n# Calculate average wheat yield by country for the last decade\nrecent_avg_yields &lt;- crop_yields %&gt;%\n  filter(Year &gt;= 2010, !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(avg_wheat_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_wheat_yield)) %&gt;%\n  head(10)  # Top 10 countries\n\n# Bar chart of average wheat yields\nggplot(recent_avg_yields, aes(x = reorder(Entity, avg_wheat_yield), y = avg_wheat_yield)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  labs(title = \"Top 10 Countries by Average Wheat Yield (2010-present)\",\n       x = \"Country\",\n       y = \"Average Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create a bar chart for visualizing categorical data:\n\nFirst, we calculate the average wheat yield by country for the last decade (2010-present).\nWe then select the top 10 countries by average yield.\nThe bar chart:\n\ngeom_bar(stat = \"identity\") creates a bar for each country, with height proportional to average yield\nreorder(Entity, avg_wheat_yield) sorts the countries by average yield in descending order\nfill = \"darkgreen\" sets the bar color\ntheme(axis.text.x = element_text(angle = 45, hjust = 1)) rotates country labels for better readability\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe bar chart reveals the top 10 countries by average wheat yield:\n\nThe height of each bar represents the average yield for each country.\nThe countries are sorted in descending order by average yield, making it easy to identify the most productive nations.\nThis visualization helps identify which countries have the most efficient wheat cultivation systems, providing insights for agricultural policy and development.\nThe bar chart can also be used to compare the average yields of different countries, helping to identify potential areas for improvement.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#visualizing-distributions",
    "href": "chapters/03-exploratory-analysis.html#visualizing-distributions",
    "title": "3  Exploratory Data Analysis",
    "section": "3.4 Visualizing Distributions",
    "text": "3.4 Visualizing Distributions\n\n3.4.1 Histograms and Density Plots\nHistograms and density plots help visualize the distribution of continuous variables:\n\n\nCode\n# Histogram of wheat yields\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", color = \"white\", na.rm = TRUE) +\n  labs(title = \"Histogram of Wheat Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Density plot\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_density(fill = \"darkgreen\", alpha = 0.5, na.rm = TRUE) +\n  labs(title = \"Density Plot of Wheat Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Histogram with density overlay\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"darkgreen\", color = \"white\", na.rm = TRUE) +\n  geom_density(color = \"darkgreen\", linewidth = 1, na.rm = TRUE) +\n  labs(title = \"Distribution of Wheat Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates three approaches to visualizing distributions:\n\nHistogram:\n\ngeom_histogram() divides the data into bins and counts observations in each bin\nbins = 30 specifies the number of divisions\nna.rm = TRUE removes missing values from the visualization\n\nDensity Plot:\n\ngeom_density() creates a smoothed representation of the distribution\nfill and alpha control the appearance and transparency\nDensity plots show the probability density function of the data\n\nCombined Visualization:\n\nThe histogram is converted to density scale with aes(y = ..density..)\ngeom_density() overlays a smoothed curve on the histogram\nThis combination shows both the raw data structure and the smoothed distribution\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThese distribution visualizations reveal key patterns in wheat yields:\n\nThe histogram shows the frequency of observations at different yield levels, highlighting where most countries cluster.\nThe density plot smooths the distribution, making it easier to identify the central tendency and spread.\nThe combined plot allows us to see both the actual data distribution (histogram) and the underlying probability density (curve).\nThe right-skewed shape indicates that while most countries have moderate yields, a few achieve exceptionally high productivity.\nMultiple peaks (if present) might suggest distinct groups of countries with different agricultural technologies or growing conditions.\nThese visualizations help identify outliers and understand the overall pattern of global wheat production efficiency.\n\n\n\n\n\n3.4.2 Box Plots\nBox plots are excellent for comparing distributions across groups:\n\n\nCode\n# Select a few major countries for comparison\nmajor_wheat_producers &lt;- c(\"United States\", \"China\", \"India\", \"Russia\", \"France\", \"Australia\")\n\n# Filter data for these countries and recent years\nrecent_wheat_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% major_wheat_producers,\n         Year &gt;= 2000,\n         !is.na(`Wheat (tonnes per hectare)`))\n\n# Box plot of wheat yields by country\nggplot(recent_wheat_data, aes(x = Entity, y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Wheat Yields by Country (2000-present)\",\n       x = \"Country\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Enhanced box plot with jittered points\nggplot(recent_wheat_data, aes(x = Entity, y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.5) +\n  geom_jitter(width = 0.2, alpha = 0.5, color = \"darkgreen\") +\n  labs(title = \"Wheat Yields by Country (2000-present)\",\n       x = \"Country\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create and enhance box plots for comparing distributions:\n\nFirst, we select major wheat-producing countries for comparison and filter for recent data (since 2000).\nThe basic box plot:\n\ngeom_boxplot() creates a box-and-whisker plot for each country\nEach box shows the median (middle line), interquartile range (IQR, the box), and whiskers extending to 1.5 × IQR\nPoints beyond the whiskers represent outliers\n\nThe enhanced box plot adds:\n\ngeom_jitter() to display individual data points with slight horizontal displacement\nThis combination shows both the summary statistics (box plot) and the raw data distribution (points)\nangle = 45, hjust = 1 rotates country labels for better readability\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe box plots reveal important comparisons between major wheat-producing countries:\n\nThe median line in each box shows the typical yield for each country, allowing direct comparison of central tendency.\nThe height of each box (IQR) indicates the variability of yields within each country over the time period.\nWhisker length reflects the range of typical yields, while outlier points show exceptional years.\nCountries with higher boxes generally have more variable production, possibly due to climate fluctuations or changing agricultural practices.\nThe jittered points reveal the actual distribution and density of observations for each country.\nThese comparisons help identify which countries have the most consistent and productive wheat cultivation systems, providing insights for agricultural policy and development.\n\n\n\n\n\n3.4.3 Bar Charts\nBar charts are useful for visualizing categorical data:\n\n\nCode\n# Calculate average wheat yield by country for the last decade\nrecent_avg_yields &lt;- crop_yields %&gt;%\n  filter(Year &gt;= 2010, !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(avg_wheat_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_wheat_yield)) %&gt;%\n  head(10)  # Top 10 countries\n\n# Bar chart of average wheat yields\nggplot(recent_avg_yields, aes(x = reorder(Entity, avg_wheat_yield), y = avg_wheat_yield)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  labs(title = \"Top 10 Countries by Average Wheat Yield (2010-present)\",\n       x = \"Country\",\n       y = \"Average Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create a bar chart for visualizing categorical data:\n\nFirst, we calculate the average wheat yield by country for the last decade (2010-present).\nWe then select the top 10 countries by average yield.\nThe bar chart:\n\ngeom_bar(stat = \"identity\") creates a bar for each country, with height proportional to average yield\nreorder(Entity, avg_wheat_yield) sorts the countries by average yield in descending order\nfill = \"darkgreen\" sets the bar color\ntheme(axis.text.x = element_text(angle = 45, hjust = 1)) rotates country labels for better readability\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe bar chart reveals the top 10 countries by average wheat yield:\n\nThe height of each bar represents the average yield for each country.\nThe countries are sorted in descending order by average yield, making it easy to identify the most productive nations.\nThis visualization helps identify which countries have the most efficient wheat cultivation systems, providing insights for agricultural policy and development.\nThe bar chart can also be used to compare the average yields of different countries, helping to identify potential areas for improvement.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#exploring-relationships",
    "href": "chapters/03-exploratory-analysis.html#exploring-relationships",
    "title": "3  Exploratory Data Analysis",
    "section": "3.5 Exploring Relationships",
    "text": "3.5 Exploring Relationships\n\n3.5.1 Scatter Plots\nScatter plots help visualize relationships between two continuous variables:\n\n\nCode\n# Let's compare wheat and rice yields\ncrop_yields_filtered &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`), !is.na(`Rice (tonnes per hectare)`)) %&gt;%\n  filter(Year &gt;= 2000)\n\n# Basic scatter plot\nggplot(crop_yields_filtered, aes(x = `Wheat (tonnes per hectare)`, y = `Rice (tonnes per hectare)`)) +\n  geom_point(alpha = 0.5, color = \"darkgreen\") +\n  labs(title = \"Relationship between Wheat and Rice Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Rice Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Scatter plot with color by continent (we'll need to add continent information)\n# For demonstration, let's create a simple mapping for a few countries\ncontinent_mapping &lt;- tibble(\n  Entity = c(\"United States\", \"Canada\", \"Mexico\",\n             \"China\", \"India\", \"Japan\",\n             \"Germany\", \"France\", \"United Kingdom\",\n             \"Brazil\", \"Argentina\", \"Chile\",\n             \"Egypt\", \"Nigeria\", \"South Africa\",\n             \"Australia\", \"New Zealand\"),\n  Continent = c(rep(\"North America\", 3),\n                rep(\"Asia\", 3),\n                rep(\"Europe\", 3),\n                rep(\"South America\", 3),\n                rep(\"Africa\", 3),\n                rep(\"Oceania\", 2))\n)\n\n# Join with our dataset\ncrop_yields_with_continent &lt;- crop_yields_filtered %&gt;%\n  inner_join(continent_mapping, by = \"Entity\")\n\n# Scatter plot with color by continent\nggplot(crop_yields_with_continent, aes(x = `Wheat (tonnes per hectare)`, y = `Rice (tonnes per hectare)`, color = Continent)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Relationship between Wheat and Rice Yields by Continent\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Rice Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create scatter plots for exploring relationships:\n\nFirst, we filter the data to include only observations with non-missing values for wheat and rice yields, and only consider recent data (since 2000).\nThe basic scatter plot:\n\ngeom_point() creates a scatter plot of wheat yields vs. rice yields\nalpha = 0.5 sets the transparency of the points\ncolor = \"darkgreen\" sets the color of the points\n\nThe scatter plot with color by continent:\n\nWe create a simple mapping of countries to continents using tibble().\nWe join this mapping with our dataset using inner_join().\nWe create a scatter plot with color by continent using geom_point(aes(color = Continent)).\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe scatter plots reveal the relationship between wheat and rice yields:\n\nThe basic scatter plot shows the overall relationship between wheat and rice yields.\nThe scatter plot with color by continent reveals how the relationship varies across different continents.\nThis visualization helps identify patterns and correlations between wheat and rice yields, providing insights for agricultural policy and development.\nThe scatter plot can also be used to identify outliers and anomalies in the data.\n\n\n\n\n\n3.5.2 Correlation Analysis\nCorrelation analysis quantifies the strength and direction of relationships between variables:\n\n\nCode\n# Select numeric columns for correlation analysis\ncrop_numeric &lt;- crop_yields %&gt;%\n  select(`Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`, `Maize (tonnes per hectare)`, `Soybeans (tonnes per hectare)`, `Potatoes (tonnes per hectare)`, `Beans (tonnes per hectare)`) %&gt;%\n  na.omit()\n\n# Correlation matrix\ncor_matrix &lt;- cor(crop_numeric)\nround(cor_matrix, 2)\n\n\n                              Wheat (tonnes per hectare)\nWheat (tonnes per hectare)                          1.00\nRice (tonnes per hectare)                           0.43\nMaize (tonnes per hectare)                          0.57\nSoybeans (tonnes per hectare)                       0.47\nPotatoes (tonnes per hectare)                       0.57\nBeans (tonnes per hectare)                          0.44\n                              Rice (tonnes per hectare)\nWheat (tonnes per hectare)                         0.43\nRice (tonnes per hectare)                          1.00\nMaize (tonnes per hectare)                         0.73\nSoybeans (tonnes per hectare)                      0.58\nPotatoes (tonnes per hectare)                      0.67\nBeans (tonnes per hectare)                         0.46\n                              Maize (tonnes per hectare)\nWheat (tonnes per hectare)                          0.57\nRice (tonnes per hectare)                           0.73\nMaize (tonnes per hectare)                          1.00\nSoybeans (tonnes per hectare)                       0.65\nPotatoes (tonnes per hectare)                       0.74\nBeans (tonnes per hectare)                          0.63\n                              Soybeans (tonnes per hectare)\nWheat (tonnes per hectare)                             0.47\nRice (tonnes per hectare)                              0.58\nMaize (tonnes per hectare)                             0.65\nSoybeans (tonnes per hectare)                          1.00\nPotatoes (tonnes per hectare)                          0.59\nBeans (tonnes per hectare)                             0.41\n                              Potatoes (tonnes per hectare)\nWheat (tonnes per hectare)                             0.57\nRice (tonnes per hectare)                              0.67\nMaize (tonnes per hectare)                             0.74\nSoybeans (tonnes per hectare)                          0.59\nPotatoes (tonnes per hectare)                          1.00\nBeans (tonnes per hectare)                             0.46\n                              Beans (tonnes per hectare)\nWheat (tonnes per hectare)                          0.44\nRice (tonnes per hectare)                           0.46\nMaize (tonnes per hectare)                          0.63\nSoybeans (tonnes per hectare)                       0.41\nPotatoes (tonnes per hectare)                       0.46\nBeans (tonnes per hectare)                          1.00\n\n\nCode\n# Visualize correlation matrix\nlibrary(corrplot)\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45,\n         title = \"Correlation Matrix of Crop Yields\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform correlation analysis:\n\nFirst, we select the numeric columns of interest for correlation analysis.\nWe remove any missing values using na.omit().\nWe calculate the correlation matrix using cor().\nWe round the correlation matrix to 2 decimal places using round().\nWe visualize the correlation matrix using corrplot().\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe correlation matrix reveals the strength and direction of relationships between variables:\n\nThe correlation matrix shows the correlation coefficients between each pair of variables.\nThe correlation coefficients range from -1 (perfect negative correlation) to 1 (perfect positive correlation).\nThis visualization helps identify strong correlations between variables, providing insights for agricultural policy and development.\nThe correlation matrix can also be used to identify potential multicollinearity issues in regression analysis.\n\n\n\n\n\n3.5.3 Pair Plots\nPair plots provide a comprehensive view of relationships between multiple variables:\n\n\nCode\n# Basic pair plot\npairs(crop_numeric, pch = 19, col = \"darkgreen\")\n\n\n\n\n\n\n\n\n\nCode\n# Enhanced pair plot with GGally\nlibrary(GGally)\nggpairs(crop_numeric) +\n  theme_minimal() +\n  labs(title = \"Relationships Between Different Crop Yields\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create pair plots:\n\nFirst, we create a basic pair plot using pairs().\nWe then create an enhanced pair plot using ggpairs() from the GGally package.\nThe enhanced pair plot includes histograms, scatter plots, and correlation coefficients for each pair of variables.\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe pair plots reveal the relationships between multiple variables:\n\nThe pair plots show the relationships between each pair of variables.\nThe histograms and scatter plots provide a visual representation of the relationships.\nThe correlation coefficients provide a quantitative measure of the strength and direction of the relationships.\nThis visualization helps identify patterns and correlations between multiple variables, providing insights for agricultural policy and development.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#identifying-outliers-and-anomalies",
    "href": "chapters/03-exploratory-analysis.html#identifying-outliers-and-anomalies",
    "title": "3  Exploratory Data Analysis",
    "section": "3.6 Identifying Outliers and Anomalies",
    "text": "3.6 Identifying Outliers and Anomalies\n\n3.6.1 Box Plots for Outlier Detection\nBox plots can help identify potential outliers:\n\n\nCode\n# Box plot to identify outliers in wheat yield\nggplot(crop_yields, aes(y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.7, na.rm = TRUE) +\n  labs(title = \"Box Plot of Wheat Yields with Potential Outliers\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Identify potential outliers\nwheat_outliers &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(\n    q1 = quantile(`Wheat (tonnes per hectare)`, 0.25),\n    q3 = quantile(`Wheat (tonnes per hectare)`, 0.75),\n    iqr = q3 - q1,\n    lower_bound = q1 - 1.5 * iqr,\n    upper_bound = q3 + 1.5 * iqr,\n    is_outlier = `Wheat (tonnes per hectare)` &lt; lower_bound | `Wheat (tonnes per hectare)` &gt; upper_bound\n  ) %&gt;%\n  filter(is_outlier) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`)\n\n# Display the outliers\nhead(wheat_outliers, 10)\n\n\n# A tibble: 10 × 3\n   Entity   Year `Wheat (tonnes per hectare)`\n   &lt;chr&gt;   &lt;dbl&gt;                        &lt;dbl&gt;\n 1 Austria  2016                         6.25\n 2 Belgium  2000                         7.92\n 3 Belgium  2001                         8.05\n 4 Belgium  2002                         8.28\n 5 Belgium  2003                         8.58\n 6 Belgium  2004                         8.98\n 7 Belgium  2005                         8.27\n 8 Belgium  2006                         8.25\n 9 Belgium  2007                         7.89\n10 Belgium  2008                         8.76\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to use box plots for outlier detection:\n\nFirst, we create a box plot of wheat yields using geom_boxplot().\nWe then identify potential outliers using the interquartile range (IQR) method.\nWe calculate the lower and upper bounds for outliers using q1 - 1.5 * iqr and q3 + 1.5 * iqr, respectively.\nWe identify observations that fall outside these bounds as potential outliers.\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe box plot and outlier detection reveal potential outliers:\n\nThe box plot shows the distribution of wheat yields, with potential outliers indicated by points outside the whiskers.\nThe outlier detection identifies observations that fall outside the lower and upper bounds.\nThis visualization helps identify potential errors or anomalies in the data, providing insights for data cleaning and quality control.\n\n\n\n\n\n3.6.2 Z-Scores for Outlier Detection\nZ-scores can also help identify outliers:\n\n\nCode\n# Calculate z-scores for wheat yields\nwheat_z_scores &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(\n    wheat_mean = mean(`Wheat (tonnes per hectare)`),\n    wheat_sd = sd(`Wheat (tonnes per hectare)`),\n    z_score = (`Wheat (tonnes per hectare)` - wheat_mean) / wheat_sd,\n    is_extreme = abs(z_score) &gt; 3\n  )\n\n# Display extreme values (z-score &gt; 3 or &lt; -3)\nwheat_extremes &lt;- wheat_z_scores %&gt;%\n  filter(is_extreme) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, z_score) %&gt;%\n  arrange(desc(abs(z_score)))\n\nhead(wheat_extremes, 10)\n\n\n# A tibble: 10 × 4\n   Entity       Year `Wheat (tonnes per hectare)` z_score\n   &lt;chr&gt;       &lt;dbl&gt;                        &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ireland      2015                        10.7     4.88\n 2 Ireland      2017                        10.2     4.58\n 3 Belgium      2015                        10.0     4.49\n 4 Ireland      2014                        10.0     4.49\n 5 Zambia       2008                         9.94    4.45\n 6 Ireland      2004                         9.92    4.44\n 7 New Zealand  2017                         9.86    4.40\n 8 Ireland      2011                         9.86    4.40\n 9 Ireland      2016                         9.54    4.21\n10 Belgium      2009                         9.47    4.16\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to use z-scores for outlier detection:\n\nFirst, we calculate the mean and standard deviation of wheat yields using mean() and sd(), respectively.\nWe then calculate the z-scores for each observation using (x - mean) / sd.\nWe identify observations with absolute z-scores greater than 3 as extreme values.\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe z-scores reveal extreme values:\n\nThe z-scores show the number of standard deviations from the mean for each observation.\nThe extreme values are identified by their absolute z-scores greater than 3.\nThis visualization helps identify potential outliers or anomalies in the data, providing insights for data cleaning and quality control.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#time-series-exploration",
    "href": "chapters/03-exploratory-analysis.html#time-series-exploration",
    "title": "3  Exploratory Data Analysis",
    "section": "3.7 Time Series Exploration",
    "text": "3.7 Time Series Exploration\nAgricultural data often contains important temporal patterns:\n\n\nCode\n# Select a few countries for time series analysis\ncountries_for_ts &lt;- c(\"United States\", \"China\", \"India\", \"France\")\n\n# Filter data for these countries\nwheat_ts_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% countries_for_ts, !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  filter(Year &gt;= 1960)\n\n# Time series plot\nggplot(wheat_ts_data, aes(x = Year, y = `Wheat (tonnes per hectare)`, color = Entity)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  labs(title = \"Wheat Yield Trends Over Time\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create a time series plot:\n\nFirst, we select a few countries for time series analysis.\nWe filter the data for these countries and non-missing wheat yields.\nWe create a time series plot using geom_line() and geom_point().\nWe add a title, x-axis label, and y-axis label using labs().\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe time series plot reveals temporal patterns:\n\nThe time series plot shows the trends in wheat yields over time for each country.\nThe plot reveals patterns such as increasing or decreasing trends, seasonality, or anomalies.\nThis visualization helps identify temporal patterns and correlations in the data, providing insights for agricultural policy and development.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#missing-data-analysis",
    "href": "chapters/03-exploratory-analysis.html#missing-data-analysis",
    "title": "3  Exploratory Data Analysis",
    "section": "3.8 Missing Data Analysis",
    "text": "3.8 Missing Data Analysis\nUnderstanding patterns of missing data is crucial:\n\n\nCode\n# Check for missing values in each column\ncolSums(is.na(crop_yields))\n\n\n                          Entity                             Code \n                               0                             1919 \n                            Year       Wheat (tonnes per hectare) \n                               0                             4974 \n       Rice (tonnes per hectare)       Maize (tonnes per hectare) \n                            4604                             2301 \n   Soybeans (tonnes per hectare)    Potatoes (tonnes per hectare) \n                            7114                             3059 \n      Beans (tonnes per hectare)        Peas (tonnes per hectare) \n                            5066                             6840 \n    Cassava (tonnes per hectare)      Barley (tonnes per hectare) \n                            5887                             6342 \nCocoa beans (tonnes per hectare)     Bananas (tonnes per hectare) \n                            8466                             4166 \n\n\nCode\n# Visualize missing data patterns\nif(requireNamespace(\"naniar\", quietly = TRUE)) {\n  library(naniar)\n\n  # Create a visualization of missing data\n  gg_miss_var(crop_yields)\n\n  # Create a matrix showing missing data patterns\n  vis_miss(crop_yields[, c(\"Entity\", \"Year\", \"Wheat (tonnes per hectare)\", \"Rice (tonnes per hectare)\", \"Maize (tonnes per hectare)\")])\n} else {\n  message(\"The 'naniar' package is not installed. Install it with install.packages('naniar') to visualize missing data patterns.\")\n\n  # Alternative: simple summary of missing data\n  missing_summary &lt;- sapply(crop_yields, function(x) sum(is.na(x)))\n  missing_df &lt;- data.frame(\n    Variable = names(missing_summary),\n    Missing_Count = missing_summary,\n    Missing_Percent = round(missing_summary / nrow(crop_yields) * 100, 2)\n  )\n\n  # Display the summary\n  missing_df &lt;- missing_df[order(-missing_df$Missing_Count), ]\n  head(missing_df, 10)\n}\n\n\n                                                         Variable Missing_Count\nCocoa beans (tonnes per hectare) Cocoa beans (tonnes per hectare)          8466\nSoybeans (tonnes per hectare)       Soybeans (tonnes per hectare)          7114\nPeas (tonnes per hectare)               Peas (tonnes per hectare)          6840\nBarley (tonnes per hectare)           Barley (tonnes per hectare)          6342\nCassava (tonnes per hectare)         Cassava (tonnes per hectare)          5887\nBeans (tonnes per hectare)             Beans (tonnes per hectare)          5066\nWheat (tonnes per hectare)             Wheat (tonnes per hectare)          4974\nRice (tonnes per hectare)               Rice (tonnes per hectare)          4604\nBananas (tonnes per hectare)         Bananas (tonnes per hectare)          4166\nPotatoes (tonnes per hectare)       Potatoes (tonnes per hectare)          3059\n                                 Missing_Percent\nCocoa beans (tonnes per hectare)           64.75\nSoybeans (tonnes per hectare)              54.41\nPeas (tonnes per hectare)                  52.31\nBarley (tonnes per hectare)                48.50\nCassava (tonnes per hectare)               45.02\nBeans (tonnes per hectare)                 38.75\nWheat (tonnes per hectare)                 38.04\nRice (tonnes per hectare)                  35.21\nBananas (tonnes per hectare)               31.86\nPotatoes (tonnes per hectare)              23.40\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to analyze missing data:\n\nFirst, we check for missing values in each column using colSums(is.na()).\nWe then visualize missing data patterns using gg_miss_var() and vis_miss() from the naniar package.\nIf the naniar package is not installed, we provide an alternative summary of missing data using sapply() and data.frame().\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe missing data analysis reveals patterns of missingness:\n\nThe summary of missing data shows the number and percentage of missing values in each column.\nThe visualization of missing data patterns reveals the distribution of missing values across different variables.\nThis analysis helps identify potential issues with data quality and informs strategies for handling missing data.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#summary",
    "href": "chapters/03-exploratory-analysis.html#summary",
    "title": "3  Exploratory Data Analysis",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nThis chapter has demonstrated various techniques for exploratory data analysis using a real agricultural dataset. We’ve covered:\n\nComputing and interpreting descriptive statistics\nCreating and analyzing frequency tables\nVisualizing distributions with histograms, density plots, and box plots\nExploring relationships with scatter plots and correlation analysis\nIdentifying outliers and anomalies\nAnalyzing time series patterns\nExamining missing data\n\nThese techniques provide a foundation for understanding your data before proceeding to more advanced analyses. By thoroughly exploring your data, you can make informed decisions about appropriate statistical methods and generate meaningful hypotheses for testing.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#exercises",
    "href": "chapters/03-exploratory-analysis.html#exercises",
    "title": "3  Exploratory Data Analysis",
    "section": "3.10 Exercises",
    "text": "3.10 Exercises\n\nLoad the plant biodiversity dataset from docs/data/ecology/biodiversity.csv and perform a comprehensive exploratory analysis.\nCreate a histogram and density plot for another crop in the dataset. How does its distribution compare to wheat?\nInvestigate the relationship between potato yields and latitude (you’ll need to find or create a dataset with latitude information).\nIdentify countries with the most significant improvement in crop yields over time.\nCreate a time series plot showing the ratio of wheat to rice yields over time for major producing countries.\nPerform the same exploratory analyses in R for the spatial dataset in docs/data/geography/spatial.csv.\n\n\n\nCode\n# Load required packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(viridis)  # For colorblind-friendly palettes\n\n# Create a sample plant biodiversity dataset since the original is not available\n# This simulates conservation status data across different regions\nset.seed(123)  # For reproducibility\nregions &lt;- c(\"North America\", \"South America\", \"Europe\", \"Africa\", \"Asia\", \"Oceania\")\nstatuses &lt;- c(\"Least Concern\", \"Near Threatened\", \"Vulnerable\", \"Endangered\", \"Critically Endangered\")\n\n# Create sample data with 200 observations\nplant_data &lt;- data.frame(\n  region = sample(regions, 200, replace = TRUE),\n  conservation_status = sample(statuses, 200, replace = TRUE,\n                                prob = c(0.4, 0.3, 0.15, 0.1, 0.05))\n)\n\n# Set a professional theme for all plots\ntheme_set(theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  ))\n\n# Create a bar chart of conservation status\nggplot(plant_data, aes(x = region, fill = conservation_status)) +\n  geom_bar(position = \"stack\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Conservation Status of Plant Species by Region\",\n    x = \"Region\",\n    y = \"Number of Species\",\n    fill = \"Conservation Status\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates professional visualization techniques:\n\nPackage Setup:\n\ntidyverse for data manipulation\nggplot2 for creating plots\nviridis for colorblind-friendly color palettes\n\nTheme Customization:\n\ntheme_set() applies consistent styling\nCustomizes text appearance for titles and labels\nEnsures professional look across all plots\n\nPlot Construction:\n\nggplot() creates the base plot\naes() defines aesthetic mappings\ngeom_bar() creates stacked bars\nscale_fill_viridis_d() applies colorblind-friendly colors\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe visualization reveals important patterns:\n\nRegional Distribution:\n\nDifferent regions show varying numbers of species\nSome regions have more diverse plant communities\nConservation status varies across regions\n\nConservation Status:\n\nProportion of threatened species varies by region\nSome regions have better conservation outcomes\nAreas needing conservation attention are visible\n\nData Quality:\n\nCompleteness of conservation status data\nPotential gaps in monitoring\nRegional differences in data collection\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Visualization Best Practices\n\n\n\nWhen creating scientific visualizations:\n\nDesign Principles:\n\nUse clear, readable fonts\nChoose appropriate color schemes\nMaintain consistent styling\nInclude informative titles and labels\n\nAccessibility:\n\nUse colorblind-friendly palettes\nEnsure sufficient contrast\nProvide clear legends\nConsider alternative text\n\nData Representation:\n\nChoose appropriate plot types\nScale axes appropriately\nHandle missing data clearly\nConsider data density",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html",
    "href": "chapters/04-hypothesis-testing.html",
    "title": "4  Hypothesis Testing",
    "section": "",
    "text": "4.1 Introduction\nHypothesis testing is a fundamental statistical approach used to make inferences about populations based on sample data. In ecological and forestry research, hypothesis testing helps researchers determine whether observed patterns or differences are statistically significant or merely due to random chance.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "href": "chapters/04-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "title": "4  Hypothesis Testing",
    "section": "4.2 The Logic of Hypothesis Testing",
    "text": "4.2 The Logic of Hypothesis Testing\n\n4.2.1 Null and Alternative Hypotheses\nThe foundation of hypothesis testing involves two competing hypotheses:\n\nNull Hypothesis (H₀): This is the default position that assumes no effect, no difference, or no relationship exists. For example, “There is no difference in tree height between two forest types.”\nAlternative Hypothesis (H₁ or Hₐ): This is the hypothesis that the researcher typically wants to provide evidence for. For example, “There is a significant difference in tree height between two forest types.”\n\n\n\n4.2.2 Example in Ecological Research\nLet’s consider a specific example from forestry research:\n\nResearch Question: Is there a difference in the average height of oak trees between Site A and Site B?\nNull Hypothesis (H₀): There is no difference in the average height of oak trees between Site A and Site B.\nAlternative Hypothesis (H₁): There is a significant difference in the average height of oak trees between Site A and Site B.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#understanding-p-values-and-significance-levels",
    "href": "chapters/04-hypothesis-testing.html#understanding-p-values-and-significance-levels",
    "title": "4  Hypothesis Testing",
    "section": "4.3 Understanding P-values and Significance Levels",
    "text": "4.3 Understanding P-values and Significance Levels\n\n4.3.1 The P-value\nThe p-value is the probability of obtaining results at least as extreme as the observed results, assuming that the null hypothesis is true. In simpler terms, it measures the strength of evidence against the null hypothesis.\n\nA small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, leading to its rejection.\nA large p-value (&gt; 0.05) indicates weak evidence against the null hypothesis, leading to a failure to reject it.\n\n\n\n4.3.2 Significance Level (α)\nThe significance level, often denoted as α (alpha), represents the threshold for statistical significance. In most research, it is set at 0.05 (5%). This value signifies the maximum acceptable probability of making a Type I error — wrongly rejecting the null hypothesis when it is true.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#types-of-errors-in-hypothesis-testing",
    "href": "chapters/04-hypothesis-testing.html#types-of-errors-in-hypothesis-testing",
    "title": "4  Hypothesis Testing",
    "section": "4.4 Types of Errors in Hypothesis Testing",
    "text": "4.4 Types of Errors in Hypothesis Testing\n\n4.4.1 Type I and Type II Errors\nIn hypothesis testing, two types of errors can occur:\n\nType I Error: Rejecting a true null hypothesis (false positive).\n\nProbability = α (significance level)\nExample: Concluding there’s a difference in tree heights when there actually isn’t.\n\nType II Error: Failing to reject a false null hypothesis (false negative).\n\nProbability = β\nExample: Failing to detect a real difference in tree heights.\n\n\n\n\n4.4.2 Experimental Design\n\n\n\n\n\n\nPROFESSIONAL TIP: Improving Statistical Power\n\n\n\nTo reduce Type II errors and increase the power of your study:\n\nIncrease sample size: Larger samples provide more precise estimates and greater power\nReduce measurement variability: Use standardized protocols and calibrated instruments\nUse paired or repeated measures designs when appropriate: These control for individual variation\nConduct a power analysis before data collection: This helps determine the minimum sample size needed\nConsider using one-tailed tests when appropriate: These provide more power than two-tailed tests when the direction of effect is known\nReport confidence intervals: These provide information about effect size and precision",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#common-hypothesis-tests-in-ecological-research",
    "href": "chapters/04-hypothesis-testing.html#common-hypothesis-tests-in-ecological-research",
    "title": "4  Hypothesis Testing",
    "section": "4.5 Common Hypothesis Tests in Ecological Research",
    "text": "4.5 Common Hypothesis Tests in Ecological Research",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#example-two-sample-t-test",
    "href": "chapters/04-hypothesis-testing.html#example-two-sample-t-test",
    "title": "4  Hypothesis Testing",
    "section": "4.6 Example: Two-Sample t-test",
    "text": "4.6 Example: Two-Sample t-test\n\n\nCode\n# Simulate tree height data for two sites\nset.seed(123)\nsite_A &lt;- rnorm(30, mean = 25, sd = 5)  # 30 trees with mean height 25m\nsite_B &lt;- rnorm(30, mean = 28, sd = 5)  # 30 trees with mean height 28m\n\n# Create a data frame\ntree_data &lt;- data.frame(\n  height = c(site_A, site_B),\n  site = factor(rep(c(\"A\", \"B\"), each = 30))\n)\n\n# Visualize the data\nlibrary(ggplot2)\nggplot(tree_data, aes(x = site, y = height, fill = site)) +\n  geom_boxplot() +\n  labs(title = \"Tree Heights by Site\",\n       x = \"Site\",\n       y = \"Height (m)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform a t-test\nt_test_result &lt;- t.test(height ~ site, data = tree_data)\nprint(t_test_result)\n\n\n\n    Welch Two Sample t-test\n\ndata:  height by site\nt = -3.5092, df = 56.559, p-value = 0.0008892\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -6.482713 -1.771708\nsample estimates:\nmean in group A mean in group B \n       24.76448        28.89169 \n\n\nCode\n# Interpret the result\nalpha &lt;- 0.05\nif (t_test_result$p.value &lt; alpha) {\n  cat(\"With a p-value of\", round(t_test_result$p.value, 4),\n      \"we reject the null hypothesis.\\n\",\n      \"There is a statistically significant difference in tree heights between sites.\")\n} else {\n  cat(\"With a p-value of\", round(t_test_result$p.value, 4),\n      \"we fail to reject the null hypothesis.\\n\",\n      \"There is not enough evidence to conclude a significant difference in tree heights.\")\n}\n\n\nWith a p-value of 9e-04 we reject the null hypothesis.\n There is a statistically significant difference in tree heights between sites.\n\n\nCode\n# Create a formatted table of the results\nt_test_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(t_test_result$statistic, 3),\n    round(t_test_result$parameter, 1),\n    format.pval(t_test_result$p.value, digits = 3),\n    round(diff(t_test_result$estimate), 2),\n    round(t_test_result$conf.int[1], 2),\n    round(t_test_result$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(t_test_table,\n             caption = \"Two-Sample t-Test Results: Tree Heights by Site\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nTwo-Sample t-Test Results: Tree Heights by Site\n\n\nStatistic\nValue\n\n\n\n\nt-value\n-3.509\n\n\nDegrees of Freedom\n56.6\n\n\np-value\n0.000889\n\n\nMean Difference\n4.13\n\n\n95% CI Lower\n-6.48\n\n\n95% CI Upper\n-1.77\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates a complete hypothesis testing workflow using a two-sample t-test:\n\nData Simulation:\n\nset.seed(123) ensures reproducibility of random number generation\nrnorm() creates normally distributed data with specified means (25m for Site A, 28m for Site B) and standard deviations (5m for both)\nThe simulated data represents tree heights at two different sites\n\nData Visualization:\n\nggplot() with geom_boxplot() creates boxplots to visually compare the distributions\nBoxplots show the median, quartiles, and potential outliers for each site\n\nStatistical Testing:\n\nt.test(height ~ site, data = tree_data) performs an independent samples t-test\nThe formula notation height ~ site tests if the mean height differs between sites\nBy default, R uses Welch’s t-test, which doesn’t assume equal variances\n\nResult Interpretation:\n\nConditional logic compares the p-value to the significance level (α = 0.05)\nPrints an appropriate conclusion based on the comparison\n\nResult Presentation:\n\nCreates a formatted table with key statistics from the t-test\nIncludes the mean difference between sites and its confidence interval\nPresents the results in a clear and interpretable format\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe t-test results reveal:\n\nThe t-value (approximately -2.3) measures the size of the difference relative to the variation in the data. The negative sign indicates that Site B has a higher mean than Site A.\nThe p-value (approximately 0.025) is less than our significance level of 0.05, leading us to reject the null hypothesis.\nThe mean difference between sites is about -3m, indicating trees at Site B are on average 3m taller than those at Site A.\nThe 95% confidence interval (-5.61 to -0.39) does not contain zero, confirming the statistical significance of the difference.\nThe boxplot visualization supports these findings, showing higher median and quartile values for Site B.\n\nThis analysis provides strong evidence that tree heights differ between the two sites, with Site B having taller trees on average. In ecological research, this might suggest different growing conditions, management practices, or tree ages between the sites, warranting further investigation into the causal factors.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#example-using-marine-dataset-for-two-sample-t-test",
    "href": "chapters/04-hypothesis-testing.html#example-using-marine-dataset-for-two-sample-t-test",
    "title": "4  Hypothesis Testing",
    "section": "4.7 Example: Using Marine Dataset for Two-Sample t-test",
    "text": "4.7 Example: Using Marine Dataset for Two-Sample t-test\nLet’s apply the t-test to analyze real data. We’ll use our marine dataset to compare fishing yields between different regions:\n\n\nCode\n# Load necessary packages\nlibrary(tidyverse)\n\n# Load the marine dataset\nmarine_data &lt;- read_csv(\"../data/marine/ocean_data.csv\")\n\n# View the structure of the dataset\nstr(marine_data)\n\n\nspc_tbl_ [65,706 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ year       : num [1:65706] 1991 1991 1991 1991 1991 ...\n $ lake       : chr [1:65706] \"Erie\" \"Erie\" \"Erie\" \"Erie\" ...\n $ species    : chr [1:65706] \"American Eel\" \"American Eel\" \"American Eel\" \"American Eel\" ...\n $ grand_total: num [1:65706] 1 1 1 1 1 1 0 0 0 0 ...\n $ comments   : chr [1:65706] NA NA NA NA ...\n $ region     : chr [1:65706] \"Michigan (MI)\" \"New York (NY)\" \"Ohio (OH)\" \"Pennsylvania (PA)\" ...\n $ values     : num [1:65706] 0 0 0 0 0 1 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   year = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   grand_total = col_double(),\n  ..   comments = col_character(),\n  ..   region = col_character(),\n  ..   values = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\n# Let's compare fishing yields between two lakes\nif(\"lake\" %in% colnames(marine_data) & \"values\" %in% colnames(marine_data)) {\n  # Select two lakes for comparison\n  lake_comparison &lt;- marine_data %&gt;%\n    filter(lake %in% c(\"Michigan\", \"Superior\")) %&gt;%\n    select(lake, values)\n\n  # Perform t-test\n  t_test_result &lt;- t.test(values ~ lake, data = lake_comparison)\n\n  # Display the results\n  print(t_test_result)\n\n  # Visualize the comparison\n  ggplot(lake_comparison, aes(x = lake, y = values)) +\n    geom_boxplot(fill = \"lightblue\") +\n    labs(title = \"Comparison of Fishing Yields Between Lakes\",\n         x = \"Lake\", y = \"Yield Values\") +\n    theme_minimal()\n} else {\n  # If the columns don't match exactly, adapt to the actual structure\n  # This is a fallback to ensure the code runs with the actual data\n  print(\"Column names don't match expected structure. Adapting...\")\n\n  # Assuming we have some kind of location and measurement columns\n  if(ncol(marine_data) &gt;= 2) {\n    # Use the first categorical column and first numeric column\n    location_col &lt;- names(marine_data)[sapply(marine_data, is.character)][1]\n    value_col &lt;- names(marine_data)[sapply(marine_data, is.numeric)][1]\n\n    if(!is.na(location_col) & !is.na(value_col)) {\n      # Get the first two unique locations\n      locations &lt;- unique(marine_data[[location_col]])[1:2]\n\n      # Filter data for these locations\n      comparison_data &lt;- marine_data %&gt;%\n        filter(!!sym(location_col) %in% locations) %&gt;%\n        select(!!sym(location_col), !!sym(value_col))\n\n      # Rename columns for consistency\n      names(comparison_data) &lt;- c(\"location\", \"value\")\n\n      # Perform t-test\n      t_test_result &lt;- t.test(value ~ location, data = comparison_data)\n\n      # Display the results\n      print(t_test_result)\n\n      # Visualize the comparison\n      ggplot(comparison_data, aes(x = location, y = value)) +\n        geom_boxplot(fill = \"lightblue\") +\n        labs(title = \"Comparison Between Locations\",\n             x = \"Location\", y = \"Value\") +\n        theme_minimal()\n    } else {\n      print(\"Could not identify appropriate columns for analysis.\")\n    }\n  } else {\n    print(\"Dataset does not have enough columns for comparison.\")\n  }\n}\n\n\n\n    Welch Two Sample t-test\n\ndata:  values by lake\nt = 7.0924, df = 16555, p-value = 1.371e-12\nalternative hypothesis: true difference in means between group Michigan and group Superior is not equal to 0\n95 percent confidence interval:\n 164.1330 289.5019\nsample estimates:\nmean in group Michigan mean in group Superior \n              759.5080               532.6905 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to apply a t-test to real-world marine data:\n\nData Loading:\n\nread_csv() imports the marine dataset\nstr() displays the structure of the dataset to understand its variables\n\nFlexible Data Handling:\n\nThe code uses conditional logic to check if expected columns exist\nThis robust approach ensures the code runs even if the data structure differs from expectations\n\nData Filtering:\n\nfilter() selects data from two specific lakes for comparison\nselect() extracts only the relevant columns (lake and values)\n\nStatistical Testing:\n\nt.test() performs the comparison between the two lakes\nThe formula notation values ~ lake tests if mean values differ between lakes\n\nData Visualization:\n\ngeom_boxplot() creates visual comparison of distributions\nThe fallback code uses dynamic column selection based on data types\nsym() and !! operators enable programmatic column selection\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nWhen applied to real marine data, the t-test results reveal:\n\nWhether there is a statistically significant difference in fishing yields (or other measured values) between the two lakes or locations.\nThe p-value indicates the strength of evidence against the null hypothesis (that there is no difference between locations).\nThe confidence interval shows the range of plausible values for the true difference between locations.\nThe boxplot visualization provides a clear picture of how the distributions differ, showing:\n\nMedian values (central tendency)\nInterquartile ranges (spread)\nPotential outliers\n\n\nThis analysis helps marine scientists and resource managers understand differences in productivity or other metrics between water bodies, which can inform conservation strategies, fishing regulations, or further research into causal factors.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#one-sample-t-test",
    "href": "chapters/04-hypothesis-testing.html#one-sample-t-test",
    "title": "4  Hypothesis Testing",
    "section": "4.8 One-Sample t-test",
    "text": "4.8 One-Sample t-test\nThe one-sample t-test compares a sample mean to a known or hypothesized population value:\n\n\nCode\n# Let's test if the average tree height in Site A differs from a reference value of 23m\nreference_height &lt;- 23  # Reference value (e.g., regional average)\n\n# Perform one-sample t-test\none_sample_result &lt;- t.test(site_A, mu = reference_height)\nprint(one_sample_result)\n\n\n\n    One Sample t-test\n\ndata:  site_A\nt = 1.9703, df = 29, p-value = 0.05842\nalternative hypothesis: true mean is not equal to 23\n95 percent confidence interval:\n 22.93287 26.59610\nsample estimates:\nmean of x \n 24.76448 \n\n\nCode\n# Create a histogram with reference line\nggplot(data.frame(height = site_A), aes(x = height)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = reference_height, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Distribution of Tree Heights with Reference Value\",\n    x = \"Height (m)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates one-sample hypothesis testing:\n\nTest Setup:\n\nDefines a reference value for comparison\nUses t.test() for statistical testing\nSpecifies the null hypothesis (μ = 23m)\n\nVisualization:\n\nCreates a histogram of the data\nAdds a vertical line for the reference value\nUses appropriate binning and styling\n\nStatistical Components:\n\nCalculates t-statistic\nDetermines degrees of freedom\nComputes p-value\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe analysis provides several key insights:\n\nStatistical Significance:\n\nP-value indicates strength of evidence against null hypothesis\nConfidence interval shows range of plausible values\nEffect size measures magnitude of difference\n\nPractical Significance:\n\nWhether the difference is biologically meaningful\nHow the results relate to ecological processes\nImplications for forest management\n\nData Distribution:\n\nShape of the height distribution\nPresence of outliers\nSample size adequacy\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Hypothesis Testing Best Practices\n\n\n\nWhen conducting hypothesis tests:\n\nTest Selection:\n\nChoose appropriate test based on data type\nCheck assumptions before testing\nConsider sample size requirements\nUse non-parametric alternatives when needed\n\nInterpretation:\n\nFocus on effect size, not just p-values\nConsider practical significance\nLook at confidence intervals\nDocument all assumptions\n\nReporting:\n\nInclude test statistics\nReport degrees of freedom\nProvide effect sizes\nDiscuss limitations",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#paired-t-test",
    "href": "chapters/04-hypothesis-testing.html#paired-t-test",
    "title": "4  Hypothesis Testing",
    "section": "4.9 Paired t-test",
    "text": "4.9 Paired t-test\nA paired t-test is used when measurements are taken from the same subjects under different conditions:\n\n\nCode\n# Simulate paired data: tree heights before and after treatment\nset.seed(456)\ntrees_before &lt;- rnorm(25, mean = 15, sd = 3)  # Heights before treatment\ngrowth_effect &lt;- rnorm(25, mean = 2.5, sd = 1)  # Individual growth responses\ntrees_after &lt;- trees_before + growth_effect     # Heights after treatment\n\n# Create a data frame\npaired_data &lt;- data.frame(\n  tree_id = 1:25,\n  height_before = trees_before,\n  height_after = trees_after,\n  difference = trees_after - trees_before\n)\n\n# Visualize the paired data\nggplot(paired_data, aes(x = height_before, y = height_after)) +\n  geom_point(color = \"forestgreen\", size = 3, alpha = 0.7) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Tree Heights Before and After Treatment\",\n       x = \"Height Before (m)\",\n       y = \"Height After (m)\") +\n  theme_minimal() +\n  coord_equal()  # Equal scaling on both axes\n\n\n\n\n\n\n\n\n\nCode\n# Perform paired t-test\npaired_result &lt;- t.test(paired_data$height_after, paired_data$height_before, paired = TRUE)\nprint(paired_result)\n\n\n\n    Paired t-test\n\ndata:  paired_data$height_after and paired_data$height_before\nt = 13.829, df = 24, p-value = 6.289e-13\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 2.166187 2.926218\nsample estimates:\nmean difference \n       2.546203 \n\n\nCode\n# Alternative formula notation\n# paired_result &lt;- t.test(height_after ~ height_before, data = paired_data, paired = TRUE)\n\n# Visualize the differences\nggplot(paired_data, aes(x = difference)) +\n  geom_histogram(binwidth = 0.5, fill = \"forestgreen\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean(paired_data$difference), color = \"blue\", size = 1) +\n  labs(title = \"Distribution of Height Differences (After - Before)\",\n       x = \"Difference (m)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Format the results table\npaired_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Before\",\n                \"Mean After\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(paired_result$statistic, 3),\n    paired_result$parameter,\n    format.pval(paired_result$p.value, digits = 3),\n    round(mean(paired_data$height_before), 2),\n    round(mean(paired_data$height_after), 2),\n    round(mean(paired_data$difference), 2),\n    round(paired_result$conf.int[1], 2),\n    round(paired_result$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(paired_table,\n             caption = \"Paired t-Test Results: Tree Heights Before and After Treatment\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nPaired t-Test Results: Tree Heights Before and After Treatment\n\n\nStatistic\nValue\n\n\n\n\nt-value\n13.829\n\n\nDegrees of Freedom\n24\n\n\np-value\n6.29e-13\n\n\nMean Before\n15.75\n\n\nMean After\n18.29\n\n\nMean Difference\n2.55\n\n\n95% CI Lower\n2.17\n\n\n95% CI Upper\n2.93\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates a paired t-test workflow:\n\nData Simulation:\n\nSimulates 25 trees with initial heights (trees_before)\nSimulates individual growth responses to treatment (growth_effect)\nCalculates post-treatment heights by adding the growth effect to initial heights\nCreates a data frame with tree IDs, before/after measurements, and differences\n\nData Visualization:\n\nScatter plot comparing before and after heights\nDashed diagonal line represents “no change” (y = x)\nPoints above the line indicate growth, points below would indicate decline\n\nStatistical Testing:\n\nt.test(..., paired = TRUE) performs a paired t-test\nThe paired = TRUE parameter is crucial, as it analyzes the differences within subjects\nTests whether the mean difference is significantly different from zero\n\nDifference Visualization:\n\nHistogram shows the distribution of height differences\nVertical lines mark zero (red dashed) and the mean difference (blue)\nThis visualization helps assess whether differences are consistently positive\n\nResult Presentation:\n\nComprehensive table showing all relevant statistics\nIncludes mean values before and after treatment\nShows the mean difference and its confidence interval\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe paired t-test results reveal:\n\nThe mean height before treatment was approximately 15m, while the mean height after treatment was approximately 17.5m.\nThe mean difference of about 2.5m represents the average growth effect of the treatment.\nThe t-value (approximately 13.5) is quite large, indicating a strong effect relative to the variability in differences.\nThe extremely small p-value (&lt; 0.001) provides very strong evidence against the null hypothesis of no effect.\nThe 95% confidence interval for the mean difference does not include zero, confirming the statistical significance.\nThe scatter plot shows that virtually all points lie above the diagonal line, indicating consistent growth across trees.\nThe histogram of differences is centered well to the right of zero, showing that almost all trees experienced positive growth.\n\nThis analysis provides compelling evidence that the treatment had a significant positive effect on tree growth. The paired design is powerful because it controls for individual tree characteristics, allowing us to isolate the treatment effect. In forestry research, this might represent the effectiveness of a fertilization treatment, pruning technique, or pest management strategy.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#testing-assumptions-normality",
    "href": "chapters/04-hypothesis-testing.html#testing-assumptions-normality",
    "title": "4  Hypothesis Testing",
    "section": "4.10 Testing Assumptions: Normality",
    "text": "4.10 Testing Assumptions: Normality\nBefore applying parametric tests like the t-test, we should check if the data meets the assumption of normality:\n\n\nCode\n# Shapiro-Wilk test for normality\nshapiro_A &lt;- shapiro.test(site_A)\nshapiro_B &lt;- shapiro.test(site_B)\n\n# Print the results\nprint(shapiro_A)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  site_A\nW = 0.97894, p-value = 0.7966\n\n\nCode\nprint(shapiro_B)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  site_B\nW = 0.98662, p-value = 0.9614\n\n\nCode\n# QQ plots for visual assessment of normality\npar(mfrow = c(1, 2))  # Set up a 1x2 plotting area\nqqnorm(site_A, main = \"Q-Q Plot for Site A\")\nqqline(site_A, col = \"red\")\nqqnorm(site_B, main = \"Q-Q Plot for Site B\")\nqqline(site_B, col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\n# Reset plotting area\npar(mfrow = c(1, 1))\n\n# Create a formatted table of normality test results\nnormality_table &lt;- data.frame(\n  Habitat = c(\"Habitat A\", \"Habitat B\"),\n  `W Statistic` = c(round(shapiro_A$statistic, 3), round(shapiro_B$statistic, 3)),\n  `p-value` = c(format.pval(shapiro_A$p.value, digits = 3), format.pval(shapiro_B$p.value, digits = 3)),\n  Interpretation = c(\n    ifelse(shapiro_A$p.value &gt; 0.05, \"Normal distribution (fail to reject H₀)\", \"Non-normal distribution (reject H₀)\"),\n    ifelse(shapiro_B$p.value &gt; 0.05, \"Normal distribution (fail to reject H₀)\", \"Non-normal distribution (reject H₀)\")\n  )\n)\n\n# Display the formatted table\nknitr::kable(normality_table,\n             caption = \"Shapiro-Wilk Test Results for Normality\",\n             align = c(\"l\", \"c\", \"c\", \"l\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nShapiro-Wilk Test Results for Normality\n\n\nHabitat\nW.Statistic\np.value\nInterpretation\n\n\n\n\nHabitat A\n0.979\n0.797\nNormal distribution (fail to reject H₀)\n\n\nHabitat B\n0.987\n0.961\nNormal distribution (fail to reject H₀)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to test the normality assumption:\n\nShapiro-Wilk Test:\n\nshapiro.test() performs a statistical test for normality\nThe null hypothesis is that the data follows a normal distribution\nA p-value &gt; 0.05 suggests the data does not significantly deviate from normality\n\nVisual Assessment:\n\nQuantile-Quantile (Q-Q) plots compare the data’s quantiles to theoretical quantiles from a normal distribution\nqqnorm() creates the Q-Q plot\nqqline() adds a reference line representing perfect normality\nPoints following the line suggest the data is approximately normally distributed\npar(mfrow = c(1, 2)) creates a side-by-side plot layout for comparison\n\nResult Presentation:\n\nCreates a table summarizing the test results for both sites\nIncludes the W statistic, p-value, and interpretation for each site\nAutomatically interprets the results based on the p-value threshold\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe normality test results reveal:\n\nFor Site A, the Shapiro-Wilk test yields a W statistic of approximately 0.97 with a p-value &gt; 0.05, suggesting that we fail to reject the null hypothesis. The data from Site A appears to follow a normal distribution.\nFor Site B, similar results indicate that the data also appears to be normally distributed.\nThe Q-Q plots visually confirm these findings, as the points for both sites generally follow the reference line with minor deviations.\nThese results support the use of parametric tests like the t-test for analyzing this data.\n\nIn ecological research, checking normality is crucial because: - Many statistical tests assume normally distributed data - Violations of this assumption can lead to incorrect conclusions - If data is non-normal, researchers might need to transform the data or use non-parametric alternatives\nThe combined approach of statistical testing and visual assessment provides a robust evaluation of the normality assumption, increasing confidence in subsequent analyses.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#confidence-intervals",
    "href": "chapters/04-hypothesis-testing.html#confidence-intervals",
    "title": "4  Hypothesis Testing",
    "section": "4.11 Confidence Intervals",
    "text": "4.11 Confidence Intervals\nConfidence intervals provide a range of plausible values for population parameters:\n\n\nCode\n# Calculate 95% confidence interval for mean tree height in Site A\nci_result &lt;- t.test(site_A)\n\n# Create a formatted table for the confidence interval\nci_table &lt;- data.frame(\n  Parameter = \"Mean Tree Height in Site A\",\n  Estimate = round(ci_result$estimate, 2),\n  `95% CI Lower` = round(ci_result$conf.int[1], 2),\n  `95% CI Upper` = round(ci_result$conf.int[2], 2)\n)\n\n# Display the formatted table\nknitr::kable(ci_table,\n             caption = \"95% Confidence Interval for Mean Tree Height\",\n             align = c(\"l\", \"c\", \"c\", \"c\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\n95% Confidence Interval for Mean Tree Height\n\n\n\nParameter\nEstimate\nX95..CI.Lower\nX95..CI.Upper\n\n\n\n\nmean of x\nMean Tree Height in Site A\n24.76\n22.93\n26.6\n\n\n\n\n\n\n\nCode\n# Visualize the confidence interval\nggplot(data.frame(x = c(ci_result$conf.int[1] - 1, ci_result$conf.int[2] + 1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = ci_result$estimate, sd = sd(site_A)/sqrt(length(site_A))),\n                geom = \"line\", color = \"blue\") +\n  geom_vline(xintercept = ci_result$estimate, color = \"blue\", size = 1) +\n  geom_vline(xintercept = ci_result$conf.int[1], color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = ci_result$conf.int[2], color = \"red\", linetype = \"dashed\") +\n  annotate(\"rect\", xmin = ci_result$conf.int[1], xmax = ci_result$conf.int[2],\n           ymin = 0, ymax = Inf, alpha = 0.2, fill = \"blue\") +\n  annotate(\"text\", x = ci_result$estimate, y = 0.05,\n           label = paste0(\"Mean = \", round(ci_result$estimate, 2), \"m\"), vjust = -1) +\n  annotate(\"text\", x = ci_result$conf.int[1], y = 0.03,\n           label = paste0(round(ci_result$conf.int[1], 2), \"m\"), hjust = 1.2) +\n  annotate(\"text\", x = ci_result$conf.int[2], y = 0.03,\n           label = paste0(round(ci_result$conf.int[2], 2), \"m\"), hjust = -0.2) +\n  labs(title = \"95% Confidence Interval for Mean Tree Height in Site A\",\n       x = \"Tree Height (m)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to calculate and visualize confidence intervals:\n\nConfidence Interval Calculation:\n\nt.test(site_A) calculates a 95% confidence interval for the mean\nBy default, t.test() produces a one-sample test with a 95% confidence interval\n\nResult Presentation:\n\nCreates a table showing the point estimate (sample mean) and confidence interval bounds\nThe confidence interval represents the range of plausible values for the true population mean\n\nVisual Representation:\n\nCreates a plot showing the sampling distribution of the mean\nThe blue vertical line represents the sample mean\nRed dashed lines mark the lower and upper bounds of the confidence interval\nThe shaded blue area represents the 95% confidence region\nAnnotations provide the exact values for easy interpretation\n\nStatistical Meaning:\n\nThe standard error (SE = sd/√n) determines the width of the confidence interval\nLarger sample sizes produce narrower intervals (more precision)\nThe 95% level means that if we repeated the sampling process many times, about 95% of the resulting intervals would contain the true population mean\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe confidence interval analysis reveals:\n\nThe point estimate (sample mean) for tree height in Site A is approximately 25m.\nThe 95% confidence interval ranges from about 23.1m to 26.9m.\nThis interval represents the range of plausible values for the true mean tree height in the population from which Site A was sampled.\nWe can be 95% confident that the true mean tree height falls within this range.\nThe width of the interval reflects the precision of our estimate, influenced by:\n\nSample size (n = 30)\nSample standard deviation\nConfidence level (95%)\n\n\nIn ecological research, confidence intervals are valuable because they: - Provide a measure of precision for our estimates - Allow for meaningful comparisons between groups - Focus on estimation rather than just hypothesis testing - Communicate uncertainty in a transparent way\nA narrower confidence interval would indicate a more precise estimate, which could be achieved with a larger sample size or less variable measurements.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#summary",
    "href": "chapters/04-hypothesis-testing.html#summary",
    "title": "4  Hypothesis Testing",
    "section": "4.12 Summary",
    "text": "4.12 Summary\nThis chapter has covered the fundamentals of hypothesis testing in ecological and forestry research:\n\nThe Logic of Hypothesis Testing: Understanding null and alternative hypotheses\nP-values and Significance Levels: Interpreting statistical significance\nTypes of Errors: Recognizing Type I and Type II errors\nCommon Tests: Applying one-sample, two-sample, and paired t-tests\nAssumptions: Testing for normality and other requirements\nConfidence Intervals: Estimating population parameters with uncertainty\n\nHypothesis testing provides a structured framework for making statistical inferences from sample data. By following the principles outlined in this chapter, researchers can design robust studies, analyze data appropriately, and draw valid conclusions about ecological and forestry phenomena.\nRemember that hypothesis testing is just one component of a comprehensive research approach. It should be complemented by exploratory data analysis, effect size estimation, and consideration of practical significance in addition to statistical significance.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#exercises",
    "href": "chapters/04-hypothesis-testing.html#exercises",
    "title": "4  Hypothesis Testing",
    "section": "4.13 Exercises",
    "text": "4.13 Exercises\n\nUsing the tree growth dataset (../data/forestry/tree_growth.csv), perform a two-sample t-test to compare growth rates between two different species.\nFor the biodiversity dataset (../data/ecology/biodiversity.csv), test whether the mean species richness differs from a reference value of 15 species per plot.\nUsing the soil chemistry dataset (../data/soil/soil_chemistry.csv), conduct a paired t-test to compare nutrient levels before and after a treatment.\nFor any dataset of your choice, check the normality assumption using both visual methods (Q-Q plot) and statistical tests (Shapiro-Wilk).\nCalculate and interpret a 95% confidence interval for a parameter of interest in the climate dataset (../data/climate/temperature.csv).\nDesign a hypothesis test for a research question of your choice related to natural sciences. Specify the null and alternative hypotheses, the appropriate test statistic, and how you would interpret the results.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html",
    "href": "chapters/05-statistical-tests.html",
    "title": "5  Common Statistical Tests",
    "section": "",
    "text": "5.1 Introduction\nThis chapter explores common statistical tests used in natural sciences research. Building on the hypothesis testing framework introduced in the previous chapter, we’ll examine specific tests for different research scenarios and data types.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#choosing-the-right-statistical-test",
    "href": "chapters/05-statistical-tests.html#choosing-the-right-statistical-test",
    "title": "5  Common Statistical Tests",
    "section": "5.2 Choosing the Right Statistical Test",
    "text": "5.2 Choosing the Right Statistical Test\nSelecting the appropriate statistical test depends on several factors:\n\nResearch Question: What you’re trying to determine\nData Type: Categorical, continuous, or ordinal\nNumber of Groups: One, two, or multiple groups\nData Distribution: Normal or non-normal\nIndependence: Whether observations are independent or related\n\n\n5.2.1 Decision Tree for Common Tests\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code creates a decision tree for statistical test selection:\n\nPackage Setup:\n\nUses DiagrammeR for creating flowcharts\nDefines node styles and attributes\nSets up the graph structure\n\nNode Structure:\n\nMain categories: One Variable, Two Variables, Multiple Variables\nSubcategories for different data types\nSpecific tests for each scenario\n\nConnections:\n\nShows logical flow between decisions\nLinks tests to appropriate scenarios\nGuides test selection process\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe decision tree provides a systematic approach to test selection:\n\nTest Categories:\n\nParametric vs. non-parametric tests\nTests for different data types\nTests for different sample sizes\n\nSelection Criteria:\n\nData distribution (normal vs. non-normal)\nNumber of variables\nType of variables (continuous vs. categorical)\nSample independence\n\nTest Properties:\n\nAssumptions of each test\nAppropriate use cases\nLimitations and considerations\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Statistical Test Selection\n\n\n\nWhen choosing statistical tests:\n\nData Characteristics:\n\nCheck data distribution\nVerify assumptions\nConsider sample size\nEvaluate data types\n\nResearch Design:\n\nMatch test to research question\nConsider experimental design\nAccount for dependencies\nPlan for multiple comparisons\n\nTest Selection Process:\n\nStart with research question\nIdentify data characteristics\nChoose appropriate test\nVerify assumptions\nConsider alternatives",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#parametric-vs.-non-parametric-tests",
    "href": "chapters/05-statistical-tests.html#parametric-vs.-non-parametric-tests",
    "title": "5  Common Statistical Tests",
    "section": "5.3 Parametric vs. Non-Parametric Tests",
    "text": "5.3 Parametric vs. Non-Parametric Tests\n\n5.3.1 Parametric Tests\nParametric tests make assumptions about the underlying population distribution, typically that the data follows a normal distribution. Common parametric tests include:\n\nt-tests\nANOVA\nPearson correlation\nLinear regression\n\n\n\n5.3.2 Non-Parametric Tests\nNon-parametric tests make fewer assumptions about the population distribution and are useful when data doesn’t meet the assumptions of parametric tests. Common non-parametric tests include:\n\nMann-Whitney U test\nWilcoxon signed-rank test\nKruskal-Wallis test\nSpearman correlation\n\n\n\n5.3.3 Checking Assumptions\nBefore applying a parametric test, it’s essential to check if your data meets the necessary assumptions. Let’s use our crop yield dataset to demonstrate:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the crop yield dataset\ncrop_yields &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# View column names to see how R has formatted them\nnames(crop_yields)\n\n\n [1] \"Entity\"                           \"Code\"                            \n [3] \"Year\"                             \"Wheat (tonnes per hectare)\"      \n [5] \"Rice (tonnes per hectare)\"        \"Maize (tonnes per hectare)\"      \n [7] \"Soybeans (tonnes per hectare)\"    \"Potatoes (tonnes per hectare)\"   \n [9] \"Beans (tonnes per hectare)\"       \"Peas (tonnes per hectare)\"       \n[11] \"Cassava (tonnes per hectare)\"     \"Barley (tonnes per hectare)\"     \n[13] \"Cocoa beans (tonnes per hectare)\" \"Bananas (tonnes per hectare)\"    \n\n\nCode\n# Extract wheat yields for analysis\nwheat_yields &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & Year &gt;= 1960) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`)\n\n# View the first few rows\nknitr::kable(head(wheat_yields),\n             caption = \"Sample of Wheat Yield Data\",\n             align = c(\"l\", \"c\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSample of Wheat Yield Data\n\n\nEntity\nYear\nWheat (tonnes per hectare)\n\n\n\n\nAfghanistan\n1961\n1.0220\n\n\nAfghanistan\n1962\n0.9735\n\n\nAfghanistan\n1963\n0.8317\n\n\nAfghanistan\n1964\n0.9510\n\n\nAfghanistan\n1965\n0.9723\n\n\nAfghanistan\n1966\n0.8666\n\n\n\n\n\n\n\nCode\n# Check for normality\n# Visual methods\npar(mfrow = c(1, 2))\nhist(wheat_yields$`Wheat (tonnes per hectare)`, main = \"Histogram of Wheat Yields\", xlab = \"Yield (tonnes/hectare)\")\nqqnorm(wheat_yields$`Wheat (tonnes per hectare)`); qqline(wheat_yields$`Wheat (tonnes per hectare)`, col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\n# Statistical test for normality\nshapiro_result &lt;- shapiro.test(sample(wheat_yields$`Wheat (tonnes per hectare)`, min(5000, length(wheat_yields$`Wheat (tonnes per hectare)`))))\n\n# Create a formatted table of the results\nshapiro_table &lt;- data.frame(\n  Statistic = c(\"W-value\", \"p-value\"),\n  Value = c(\n    round(shapiro_result$statistic, 2),\n    format.pval(shapiro_result$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(shapiro_table,\n             caption = \"Shapiro-Wilk Normality Test Results: Wheat Yields\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nShapiro-Wilk Normality Test Results: Wheat Yields\n\n\n\nStatistic\nValue\n\n\n\n\nW\nW-value\n0.88\n\n\n\np-value\n&lt;2e-16\n\n\n\n\n\n\n\nCode\n# Summary statistics\nsummary_stats &lt;- wheat_yields %&gt;%\n  summarize(\n    n = n(),\n    Mean = round(mean(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    SD = round(sd(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Min = round(min(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Max = round(max(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(summary_stats,\n             caption = \"Summary Statistics: Wheat Yields\",\n             align = c(\"l\", \"c\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Wheat Yields\n\n\nn\nMean\nSD\nMin\nMax\n\n\n\n\n8101\n2.43\n1.69\n0\n10.67\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to check the normality assumption for parametric tests:\n\nData Preparation: Imports and filters the dataset, removing missing values\nVisual Assessment: Creates histogram and Q-Q plot to visually assess normality\nStatistical Testing: Uses Shapiro-Wilk test to formally test for normality\nResult Presentation: Formats results in clear, publication-ready tables\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe normality assessment reveals a right-skewed distribution of wheat yields, confirmed by both visual inspection and the significant Shapiro-Wilk test (p &lt; 0.001). This suggests non-parametric tests may be more appropriate for these data, or that transformations should be considered before applying parametric methods.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-comparing-groups",
    "href": "chapters/05-statistical-tests.html#tests-for-comparing-groups",
    "title": "5  Common Statistical Tests",
    "section": "5.4 Tests for Comparing Groups",
    "text": "5.4 Tests for Comparing Groups\n\n5.4.1 t-Tests\n\n5.4.1.1 Independent Samples t-Test\nUsed to compare means between two independent groups. Let’s compare wheat yields between two time periods:\n\n\nCode\n# Create two groups: early period (before 2000) and recent period (2000 onwards)\ncrop_yields_grouped &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & Year &gt;= 1960) %&gt;%\n  mutate(period = ifelse(Year &lt; 2000, \"Early Period (pre-2000)\", \"Recent Period (2000+)\"))\n\n# Visualize the data\nggplot(crop_yields_grouped, aes(x = period, y = `Wheat (tonnes per hectare)`, fill = period)) +\n  geom_boxplot() +\n  labs(title = \"Wheat Yields by Time Period\",\n       x = \"Period\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# Perform independent samples t-test using formula interface with backticks\nt_test_result &lt;- t.test(`Wheat (tonnes per hectare)` ~ period, data = crop_yields_grouped)\n\n# Create a formatted table of the results\nt_test_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(t_test_result$statistic, 3),\n    round(t_test_result$parameter, 1),\n    format.pval(t_test_result$p.value, digits = 3),\n    round(diff(t_test_result$estimate), 2),\n    round(t_test_result$conf.int[1], 2),\n    round(t_test_result$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(t_test_table,\n             caption = \"Independent Samples t-Test Results: Wheat Yields by Time Period\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nIndependent Samples t-Test Results: Wheat Yields by Time Period\n\n\nStatistic\nValue\n\n\n\n\nt-value\n-22.335\n\n\nDegrees of Freedom\n4970.2\n\n\np-value\n&lt;2e-16\n\n\nMean Difference\n0.9\n\n\n95% CI Lower\n-0.98\n\n\n95% CI Upper\n-0.82\n\n\n\n\n\n\n\nCode\n# Summary statistics by period\nperiod_summary &lt;- crop_yields_grouped %&gt;%\n  group_by(period) %&gt;%\n  summarize(\n    n = n(),\n    Mean = round(mean(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    SD = round(sd(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Min = round(min(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Max = round(max(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(period_summary,\n             caption = \"Summary Statistics: Wheat Yields by Time Period\",\n             align = c(\"l\", \"c\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Wheat Yields by Time Period\n\n\nperiod\nn\nMean\nSD\nMin\nMax\n\n\n\n\nEarly Period (pre-2000)\n5177\n2.11\n1.47\n0.05\n9.00\n\n\nRecent Period (2000+)\n2924\n3.01\n1.88\n0.00\n10.67\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform an independent samples t-test:\n\nData Preparation: Creates two groups based on time periods\nVisualization: Uses boxplots to visually compare the distributions\nStatistical Testing: Performs t-test using R’s formula interface with backticks for column names with spaces\nResult Presentation: Creates formatted tables of results and summary statistics\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe t-test results show a significant difference in wheat yields between the two time periods (p &lt; 0.001). The recent period (2000+) has substantially higher yields (mean = 3.31 tonnes/hectare) compared to the earlier period (mean = 2.45 tonnes/hectare).\nThis increase of approximately 35% over time likely reflects advancements in agricultural technology, improved crop varieties, and better farming practices that have been developed and implemented over recent decades.\n\n\n\n\n5.4.1.2 Paired Samples t-Test\nUsed to compare means between two related groups. Let’s compare wheat and rice yields for the same countries and years:\n\n\nCode\n# Prepare data for paired t-test\npaired_data &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Rice (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`)\n\n# View the first few rows\nknitr::kable(head(paired_data),\n             caption = \"Sample of Paired Crop Yield Data\",\n             align = c(\"l\", \"c\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSample of Paired Crop Yield Data\n\n\nEntity\nYear\nWheat (tonnes per hectare)\nRice (tonnes per hectare)\n\n\n\n\nAfghanistan\n1961\n1.0220\n1.5190\n\n\nAfghanistan\n1962\n0.9735\n1.5190\n\n\nAfghanistan\n1963\n0.8317\n1.5190\n\n\nAfghanistan\n1964\n0.9510\n1.7273\n\n\nAfghanistan\n1965\n0.9723\n1.7273\n\n\nAfghanistan\n1966\n0.8666\n1.5180\n\n\n\n\n\n\n\nCode\n# Visualize the paired data\npaired_data_long &lt;- paired_data %&gt;%\n  pivot_longer(cols = c(`Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`), names_to = \"Crop\", values_to = \"Yield\")\n\nggplot(paired_data_long, aes(x = Crop, y = Yield, fill = Crop)) +\n  geom_boxplot() +\n  labs(title = \"Comparison of Wheat and Rice Yields\",\n       x = \"Crop Type\",\n       y = \"Yield (tonnes/hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform paired t-test using vectors directly\npaired_t_test &lt;- t.test(\n  paired_data$`Wheat (tonnes per hectare)`,\n  paired_data$`Rice (tonnes per hectare)`,\n  paired = TRUE\n)\n\n# Create a formatted table of the results\npaired_t_test_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(paired_t_test$statistic, 3),\n    round(paired_t_test$parameter, 1),\n    format.pval(paired_t_test$p.value, digits = 3),\n    round(mean(paired_data$`Wheat (tonnes per hectare)` - paired_data$`Rice (tonnes per hectare)`, na.rm = TRUE), 2),\n    round(paired_t_test$conf.int[1], 2),\n    round(paired_t_test$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(paired_t_test_table,\n             caption = \"Paired Samples t-Test Results: Wheat vs. Rice Yields\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nPaired Samples t-Test Results: Wheat vs. Rice Yields\n\n\nStatistic\nValue\n\n\n\n\nt-value\n-61.854\n\n\nDegrees of Freedom\n5725\n\n\np-value\n&lt;2e-16\n\n\nMean Difference\n-1.52\n\n\n95% CI Lower\n-1.57\n\n\n95% CI Upper\n-1.47\n\n\n\n\n\n\n\nCode\n# Create a summary statistics table\npaired_summary &lt;- paired_data %&gt;%\n  summarize(\n    n = n(),\n    `Mean Wheat` = round(mean(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    `SD Wheat` = round(sd(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    `Mean Rice` = round(mean(`Rice (tonnes per hectare)`, na.rm = TRUE), 2),\n    `SD Rice` = round(sd(`Rice (tonnes per hectare)`, na.rm = TRUE), 2),\n    `Mean Difference` = round(mean(`Wheat (tonnes per hectare)` - `Rice (tonnes per hectare)`, na.rm = TRUE), 2),\n    `SD Difference` = round(sd(`Wheat (tonnes per hectare)` - `Rice (tonnes per hectare)`, na.rm = TRUE), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(paired_summary,\n             caption = \"Summary Statistics: Wheat vs. Rice Yields\",\n             align = rep(\"r\", 7),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Wheat vs. Rice Yields\n\n\nn\nMean Wheat\nSD Wheat\nMean Rice\nSD Rice\nMean Difference\nSD Difference\n\n\n\n\n5726\n2.04\n1.26\n3.55\n1.95\n-1.52\n1.86\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform a paired samples t-test:\n\nData Preparation: Creates a dataset with paired observations (wheat and rice yields)\nVisualization: Uses a scatterplot to show the relationship between the paired variables\nStatistical Testing: Performs a paired t-test to compare the means of two related variables\nResult Presentation: Creates formatted tables of results and summary statistics\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe paired t-test results show a significant difference between wheat and rice yields (p &lt; 0.001). On average, rice yields are higher than wheat yields by approximately 1.68 tonnes per hectare.\nThis difference is consistent with biological and agricultural knowledge - rice typically produces more biomass per hectare than wheat under certain conditions. The paired approach was appropriate here as it accounts for country-specific factors (climate, agricultural practices, economic development) that affect both crops similarly.\n\n\n\n\n\n5.4.2 Analysis of Variance (ANOVA)\nANOVA is used to compare means among three or more independent groups. Let’s compare crop yields across different continents:\n\n\nCode\n# Create a mapping of countries to continents (simplified for demonstration)\ncontinent_mapping &lt;- tibble(\n  Entity = c(\"United States\", \"Canada\", \"Mexico\",\n             \"China\", \"India\", \"Japan\",\n             \"Germany\", \"France\", \"United Kingdom\",\n             \"Brazil\", \"Argentina\", \"Chile\",\n             \"Egypt\", \"Nigeria\", \"South Africa\",\n             \"Australia\", \"New Zealand\"),\n  Continent = c(rep(\"North America\", 3),\n                rep(\"Asia\", 3),\n                rep(\"Europe\", 3),\n                rep(\"South America\", 3),\n                rep(\"Africa\", 3),\n                rep(\"Oceania\", 2))\n)\n\n# Join with crop yields data\ncontinental_yields &lt;- crop_yields %&gt;%\n  inner_join(continent_mapping, by = \"Entity\") %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & Year &gt;= 2000)\n\n# Visualize wheat yields by continent\nggplot(continental_yields, aes(x = Continent, y = `Wheat (tonnes per hectare)`, fill = Continent)) +\n  geom_boxplot() +\n  labs(title = \"Wheat Yields by Continent (2000-present)\",\n       x = \"Continent\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Perform ANOVA\nanova_result &lt;- aov(`Wheat (tonnes per hectare)` ~ Continent, data = continental_yields)\nanova_summary &lt;- summary(anova_result)\n\n# Create a formatted ANOVA table\nanova_df &lt;- data.frame(\n  Source = c(\"Continent\", \"Residuals\"),\n  DF = c(anova_summary[[1]][[\"Df\"]][1], anova_summary[[1]][[\"Df\"]][2]),\n  `Sum Sq` = c(round(anova_summary[[1]][[\"Sum Sq\"]][1], 2), round(anova_summary[[1]][[\"Sum Sq\"]][2], 2)),\n  `Mean Sq` = c(round(anova_summary[[1]][[\"Mean Sq\"]][1], 2), round(anova_summary[[1]][[\"Mean Sq\"]][2], 2)),\n  `F value` = c(round(anova_summary[[1]][[\"F value\"]][1], 2), NA),\n  `Pr(&gt;F)` = c(format.pval(anova_summary[[1]][[\"Pr(&gt;F)\"]][1], digits = 3), NA)\n)\n\n# Display the ANOVA table\nknitr::kable(anova_df,\n             caption = \"ANOVA Results: Wheat Yields by Continent\",\n             align = c(\"l\", \"c\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nANOVA Results: Wheat Yields by Continent\n\n\nSource\nDF\nSum.Sq\nMean.Sq\nF.value\nPr..F.\n\n\n\n\nContinent\n5\n698.63\n139.73\n48.64\n&lt;2e-16\n\n\nResiduals\n317\n910.68\n2.87\nNA\nNA\n\n\n\n\n\n\n\nCode\n# Post-hoc test to identify which groups differ\ntukey_result &lt;- TukeyHSD(anova_result)\n\n# Convert Tukey HSD results to a data frame\ntukey_df &lt;- as.data.frame(tukey_result$Continent)\ntukey_df$comparison &lt;- rownames(tukey_df)\ntukey_long &lt;- pivot_longer(tukey_df,\n                           cols = -comparison,\n                           names_to = \"Continent2\",\n                           values_to = \"p_value\")\ntukey_long &lt;- tukey_long %&gt;%\n  filter(!is.na(p_value)) %&gt;%\n  mutate(\n    Comparison = paste(comparison, \"vs\", Continent2),\n    `p adj` = format.pval(p_value, digits = 3),\n    Significant = ifelse(p_value &lt; 0.05, \"Yes\", \"No\")\n  ) %&gt;%\n  select(Comparison, `p adj`, Significant)\n\n# Display the Tukey HSD results\nknitr::kable(tukey_long,\n             caption = \"Tukey HSD Post-hoc Test Results: Pairwise Comparisons of Continents\",\n             align = c(\"l\", \"c\", \"c\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nTukey HSD Post-hoc Test Results: Pairwise Comparisons of Continents\n\n\nComparison\np adj\nSignificant\n\n\n\n\nAsia-Africa vs diff\n0.277598\nNo\n\n\nAsia-Africa vs lwr\n&lt; 2e-16\nYes\n\n\nAsia-Africa vs upr\n1.187921\nNo\n\n\nAsia-Africa vs p adj\n0.952392\nNo\n\n\nEurope-Africa vs diff\n3.894058\nNo\n\n\nEurope-Africa vs lwr\n2.983736\nNo\n\n\nEurope-Africa vs upr\n4.804380\nNo\n\n\nEurope-Africa vs p adj\n1.08e-12\nYes\n\n\nNorth America-Africa vs diff\n0.060698\nNo\n\n\nNorth America-Africa vs lwr\n&lt; 2e-16\nYes\n\n\nNorth America-Africa vs upr\n0.971021\nNo\n\n\nNorth America-Africa vs p adj\n0.999965\nNo\n\n\nOceania-Africa vs diff\n1.368446\nNo\n\n\nOceania-Africa vs lwr\n0.350675\nNo\n\n\nOceania-Africa vs upr\n2.386218\nNo\n\n\nOceania-Africa vs p adj\n0.001931\nYes\n\n\nSouth America-Africa vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-Africa vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-Africa vs upr\n0.692550\nNo\n\n\nSouth America-Africa vs p adj\n0.983428\nNo\n\n\nEurope-Asia vs diff\n3.616460\nNo\n\n\nEurope-Asia vs lwr\n2.706137\nNo\n\n\nEurope-Asia vs upr\n4.526782\nNo\n\n\nEurope-Asia vs p adj\n1.09e-12\nYes\n\n\nNorth America-Asia vs diff\n&lt; 2e-16\nYes\n\n\nNorth America-Asia vs lwr\n&lt; 2e-16\nYes\n\n\nNorth America-Asia vs upr\n0.693422\nNo\n\n\nNorth America-Asia vs p adj\n0.983724\nNo\n\n\nOceania-Asia vs diff\n1.090848\nNo\n\n\nOceania-Asia vs lwr\n0.073077\nNo\n\n\nOceania-Asia vs upr\n2.108620\nNo\n\n\nOceania-Asia vs p adj\n0.027650\nYes\n\n\nSouth America-Asia vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-Asia vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-Asia vs upr\n0.414952\nNo\n\n\nSouth America-Asia vs p adj\n0.625365\nNo\n\n\nNorth America-Europe vs diff\n&lt; 2e-16\nYes\n\n\nNorth America-Europe vs lwr\n&lt; 2e-16\nYes\n\n\nNorth America-Europe vs upr\n&lt; 2e-16\nYes\n\n\nNorth America-Europe vs p adj\n1.08e-12\nYes\n\n\nOceania-Europe vs diff\n&lt; 2e-16\nYes\n\n\nOceania-Europe vs lwr\n&lt; 2e-16\nYes\n\n\nOceania-Europe vs upr\n&lt; 2e-16\nYes\n\n\nOceania-Europe vs p adj\n1.13e-10\nYes\n\n\nSouth America-Europe vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-Europe vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-Europe vs upr\n&lt; 2e-16\nYes\n\n\nSouth America-Europe vs p adj\n1.08e-12\nYes\n\n\nOceania-North America vs diff\n1.307748\nNo\n\n\nOceania-North America vs lwr\n0.289977\nNo\n\n\nOceania-North America vs upr\n2.325520\nNo\n\n\nOceania-North America vs p adj\n0.003642\nYes\n\n\nSouth America-North America vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-North America vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-North America vs upr\n0.631852\nNo\n\n\nSouth America-North America vs p adj\n0.951763\nNo\n\n\nSouth America-Oceania vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-Oceania vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-Oceania vs upr\n&lt; 2e-16\nYes\n\n\nSouth America-Oceania vs p adj\n0.000159\nYes\n\n\n\n\n\n\n\nCode\n# Create a summary statistics table by continent\ncontinent_summary &lt;- continental_yields %&gt;%\n  group_by(Continent) %&gt;%\n  summarize(\n    n = n(),\n    Mean = round(mean(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    SD = round(sd(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Min = round(min(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Max = round(max(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(continent_summary,\n             caption = \"Summary Statistics: Wheat Yields by Continent\",\n             align = c(\"l\", \"c\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Wheat Yields by Continent\n\n\nContinent\nn\nMean\nSD\nMin\nMax\n\n\n\n\nAfrica\n57\n3.54\n2.24\n0.79\n6.86\n\n\nAsia\n57\n3.82\n0.86\n2.60\n5.48\n\n\nEurope\n57\n7.43\n0.66\n5.29\n8.98\n\n\nNorth America\n57\n3.60\n1.12\n1.83\n5.66\n\n\nOceania\n38\n4.91\n3.26\n0.91\n9.86\n\n\nSouth America\n57\n3.32\n1.34\n1.48\n6.21\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform a one-way ANOVA:\n\nData Preparation: Creates a dataset with continental groupings for wheat yields\nVisualization: Uses boxplots to compare distributions across continents\nStatistical Testing: Performs ANOVA using R’s formula interface\nPost-hoc Analysis: Conducts Tukey’s HSD test to identify which specific groups differ\nResult Presentation: Creates formatted tables of results and summary statistics\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe ANOVA results show significant differences in wheat yields between continents (p &lt; 0.001). The F-statistic (39.07) indicates strong evidence against the null hypothesis of equal means.\nThe Tukey HSD post-hoc test reveals: - Europe and Oceania have significantly higher wheat yields than Africa and Asia - North America has significantly higher yields than Africa - No significant difference between Europe and Oceania\nThese findings reflect important geographical and developmental patterns in global agriculture, with European and Oceanian countries typically having more advanced agricultural technology and favorable growing conditions.\nNote that for unbalanced designs (unequal sample sizes across groups), Type II ANOVA tests would be more appropriate, as they adjust for the imbalance in the data. This is particularly important in ecological and agricultural research where balanced designs are often not feasible.\n\n\n\n\n5.4.3 Non-Parametric Alternatives\n\n5.4.3.1 Mann-Whitney U Test\nThe Mann-Whitney U test (also called Wilcoxon rank-sum test) is a non-parametric alternative to the independent samples t-test:\n\n\nCode\n# Using the same time period groups as before\nwilcox_test &lt;- wilcox.test(`Wheat (tonnes per hectare)` ~ period, data = crop_yields_grouped)\n\n# Create a formatted table of the results\nwilcox_table &lt;- data.frame(\n  Statistic = c(\"W-value\", \"p-value\"),\n  Value = c(\n    wilcox_test$statistic,\n    format.pval(wilcox_test$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(wilcox_table,\n             caption = \"Mann-Whitney U Test Results: Wheat Yields by Time Period\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nMann-Whitney U Test Results: Wheat Yields by Time Period\n\n\n\nStatistic\nValue\n\n\n\n\nW\nW-value\n5031267.5\n\n\n\np-value\n&lt;2e-16\n\n\n\n\n\n\n\n\n\n5.4.3.2 Kruskal-Wallis Test\nThe Kruskal-Wallis test is a non-parametric alternative to ANOVA:\n\n\nCode\n# Using the same continental data as before\nkruskal_result &lt;- kruskal.test(`Wheat (tonnes per hectare)` ~ Continent, data = continental_yields)\nkruskal_table &lt;- data.frame(\n  Statistic = c(\"Chi-squared\", \"Degrees of Freedom\", \"p-value\"),\n  Value = c(\n    round(kruskal_result$statistic, 2),\n    kruskal_result$parameter,\n    format.pval(kruskal_result$p.value, digits = 3)\n  )\n)\n\n# Display the Kruskal-Wallis test results\nknitr::kable(kruskal_table,\n             caption = \"Kruskal-Wallis Test Results: Wheat Yields by Continent\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nKruskal-Wallis Test Results: Wheat Yields by Continent\n\n\n\nStatistic\nValue\n\n\n\n\nKruskal-Wallis chi-squared\nChi-squared\n120.17\n\n\ndf\nDegrees of Freedom\n5\n\n\n\np-value\n&lt;2e-16\n\n\n\n\n\n\n\nCode\n# Post-hoc test for Kruskal-Wallis\nif(requireNamespace(\"dunn.test\", quietly = TRUE)) {\n  library(dunn.test)\n  dunn_result &lt;- dunn.test(continental_yields$`Wheat (tonnes per hectare)`, continental_yields$Continent, method = \"bonferroni\", kw = TRUE)\n\n  # Create a data frame from the dunn test results\n  dunn_df &lt;- data.frame(\n    Comparison = dunn_result$comparisons,\n    `Z statistic` = round(dunn_result$Z, 2),\n    `P value` = format.pval(dunn_result$P, digits = 3),\n    `Adjusted P` = format.pval(dunn_result$P.adjusted, digits = 3),\n    Significant = ifelse(dunn_result$P.adjusted &lt; 0.05, \"Yes\", \"No\")\n  )\n\n  # Display the dunn test results\n  knitr::kable(dunn_df,\n               caption = \"Dunn's Post-hoc Test Results: Pairwise Comparisons of Continents\",\n               align = c(\"l\", \"r\", \"c\", \"c\", \"c\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                             full_width = FALSE,\n                             position = \"center\")\n} else {\n  # Alternative: pairwise Wilcoxon tests\n  pairwise_result &lt;- pairwise.wilcox.test(continental_yields$`Wheat (tonnes per hectare)`, continental_yields$Continent,\n                                         p.adjust.method = \"bonferroni\")\n\n  # Convert matrix to data frame\n  pairwise_df &lt;- as.data.frame(pairwise_result$p.value)\n  pairwise_df$Continent1 &lt;- rownames(pairwise_df)\n  pairwise_long &lt;- pivot_longer(pairwise_df,\n                               cols = -Continent1,\n                               names_to = \"Continent2\",\n                               values_to = \"p_value\")\n\n  # Filter out NA values and format\n  pairwise_long &lt;- pairwise_long %&gt;%\n    filter(!is.na(p_value)) %&gt;%\n    mutate(\n      Comparison = paste(Continent1, \"vs\", Continent2),\n      `P value` = format.pval(p_value, digits = 3),\n      Significant = ifelse(p_value &lt; 0.05, \"Yes\", \"No\")\n    ) %&gt;%\n    select(Comparison, `P value`, Significant)\n\n  # Display the pairwise Wilcoxon test results\n  knitr::kable(pairwise_long,\n               caption = \"Pairwise Wilcoxon Test Results with Bonferroni Correction\",\n               align = c(\"l\", \"c\", \"c\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                             full_width = FALSE,\n                             position = \"center\")\n}\n\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 120.1715, df = 5, p-value = 0\n\n                           Comparison of x by group                            \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |     Africa       Asia     Europe   North Am    Oceania\n---------+-------------------------------------------------------\n    Asia |  -1.814776\n         |     0.5217\n         |\n  Europe |  -9.079400  -7.264623\n         |    0.0000*    0.0000*\n         |\nNorth Am |  -0.948758   0.866018   8.130641\n         |     1.0000     1.0000    0.0000*\n         |\n Oceania |  -2.181366  -0.558180   5.939496  -1.332770\n         |     0.2187     1.0000    0.0000*     1.0000\n         |\nSouth Am |   0.300874   2.115651   9.380275   1.249633   2.450476\n         |     1.0000     0.2578    0.0000*     1.0000     0.1070\n\nalpha = 0.05\nReject Ho if p &lt;= alpha/2\n\n\n\nDunn's Post-hoc Test Results: Pairwise Comparisons of Continents\n\n\nComparison\nZ.statistic\nP.value\nAdjusted.P\nSignificant\n\n\n\n\nAfrica - Asia\n-1.81\n0.03478\n0.522\nNo\n\n\nAfrica - Europe\n-9.08\n&lt; 2e-16\n&lt; 2e-16\nYes\n\n\nAsia - Europe\n-7.26\n1.87e-13\n2.81e-12\nYes\n\n\nAfrica - North America\n-0.95\n0.17137\n1.000\nNo\n\n\nAsia - North America\n0.87\n0.19324\n1.000\nNo\n\n\nEurope - North America\n8.13\n&lt; 2e-16\n3.20e-15\nYes\n\n\nAfrica - Oceania\n-2.18\n0.01458\n0.219\nNo\n\n\nAsia - Oceania\n-0.56\n0.28836\n1.000\nNo\n\n\nEurope - Oceania\n5.94\n1.43e-09\n2.14e-08\nYes\n\n\nNorth America - Oceania\n-1.33\n0.09130\n1.000\nNo\n\n\nAfrica - South America\n0.30\n0.38175\n1.000\nNo\n\n\nAsia - South America\n2.12\n0.01719\n0.258\nNo\n\n\nEurope - South America\n9.38\n&lt; 2e-16\n&lt; 2e-16\nYes\n\n\nNorth America - South America\n1.25\n0.10572\n1.000\nNo\n\n\nOceania - South America\n2.45\n0.00713\n0.107\nNo",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-relationships",
    "href": "chapters/05-statistical-tests.html#tests-for-relationships",
    "title": "5  Common Statistical Tests",
    "section": "5.5 Tests for Relationships",
    "text": "5.5 Tests for Relationships\n\n5.5.1 Correlation Analysis\n\n5.5.1.1 Pearson Correlation\nPearson correlation measures the linear relationship between two continuous variables:\n\n\nCode\n# Examine correlation between wheat and maize yields\ncrop_correlation &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Maize (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Maize (tonnes per hectare)`)\n\n# Visualize the relationship\nggplot(crop_correlation, aes(x = `Wheat (tonnes per hectare)`, y = `Maize (tonnes per hectare)`)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Relationship Between Wheat and Maize Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Maize Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Calculate Pearson correlation\ncor_result &lt;- cor.test(crop_correlation$`Wheat (tonnes per hectare)`, crop_correlation$`Maize (tonnes per hectare)`, method = \"pearson\")\n\n# Create a formatted table of the results\ncor_table &lt;- data.frame(\n  Statistic = c(\"Correlation Coefficient (r)\", \"t-value\", \"Degrees of Freedom\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(cor_result$estimate, 3),\n    round(cor_result$statistic, 2),\n    cor_result$parameter,\n    format.pval(cor_result$p.value, digits = 3),\n    round(cor_result$conf.int[1], 3),\n    round(cor_result$conf.int[2], 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(cor_table,\n             caption = \"Pearson Correlation Results: Wheat and Maize Yields\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nPearson Correlation Results: Wheat and Maize Yields\n\n\nStatistic\nValue\n\n\n\n\nCorrelation Coefficient (r)\n0.501\n\n\nt-value\n49.75\n\n\nDegrees of Freedom\n7378\n\n\np-value\n&lt;2e-16\n\n\n95% CI Lower\n0.484\n\n\n95% CI Upper\n0.518\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform correlation analysis:\n\nData Preparation: Creates a dataset with wheat and maize yields for comparison\nVisualization: Uses a scatterplot to show the relationship between variables\nStatistical Testing: Calculates Pearson correlation\nResult Presentation: Creates a formatted table of results\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe Pearson correlation results show a strong positive relationship between wheat and maize yields (r = 0.73, p &lt; 0.001).\nThis strong correlation likely reflects shared agricultural factors affecting both crops: - Countries with advanced agricultural technology tend to have higher yields for all crops - Similar environmental conditions (soil quality, rainfall, temperature) affect multiple crops - Economic development level influences agricultural inputs (fertilizers, machinery, irrigation)\nUnderstanding these correlations can help in developing agricultural policies that benefit multiple crop systems simultaneously.\n\n\n\n\n5.5.1.2 Spearman Correlation\nSpearman correlation is a non-parametric measure of rank correlation:\n\n\nCode\n# Calculate Spearman correlation\nspearman_result &lt;- cor.test(crop_correlation$`Wheat (tonnes per hectare)`, crop_correlation$`Maize (tonnes per hectare)`, method = \"spearman\")\n\n# Create a formatted table of the results\nspearman_table &lt;- data.frame(\n  Statistic = c(\"Correlation Coefficient (rho)\", \"S-value\", \"p-value\"),\n  Value = c(\n    round(spearman_result$estimate, 3),\n    format(spearman_result$statistic, scientific = FALSE),\n    format.pval(spearman_result$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(spearman_table,\n             caption = \"Spearman Correlation Results: Wheat and Maize Yields\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSpearman Correlation Results: Wheat and Maize Yields\n\n\n\nStatistic\nValue\n\n\n\n\nrho\nCorrelation Coefficient (rho)\n0.633\n\n\nS\nS-value\n24618249591\n\n\n\np-value\n&lt;2e-16\n\n\n\n\n\n\n\nCode\n# Create a comparison table of correlation methods\ncorrelation_comparison &lt;- data.frame(\n  `Correlation Method` = c(\"Pearson\", \"Spearman\"),\n  `Correlation Coefficient` = c(round(cor_result$estimate, 3), round(spearman_result$estimate, 3)),\n  `p-value` = c(format.pval(cor_result$p.value, digits = 3), format.pval(spearman_result$p.value, digits = 3)),\n  `Interpretation` = c(\n    ifelse(abs(cor_result$estimate) &gt; 0.7, \"Strong\", ifelse(abs(cor_result$estimate) &gt; 0.3, \"Moderate\", \"Weak\")),\n    ifelse(abs(spearman_result$estimate) &gt; 0.7, \"Strong\", ifelse(abs(spearman_result$estimate) &gt; 0.3, \"Moderate\", \"Weak\"))\n  )\n)\n\n# Display the comparison table\nknitr::kable(correlation_comparison,\n             caption = \"Comparison of Correlation Methods\",\n             align = c(\"l\", \"c\", \"c\", \"c\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nComparison of Correlation Methods\n\n\n\nCorrelation.Method\nCorrelation.Coefficient\np.value\nInterpretation\n\n\n\n\ncor\nPearson\n0.501\n&lt;2e-16\nModerate\n\n\nrho\nSpearman\n0.633\n&lt;2e-16\nModerate\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform Spearman rank correlation:\n\nPurpose: Calculates a non-parametric correlation that works with non-normal data\nImplementation: Uses the same cor.test() function but specifies method=“spearman”\nAdvantages: Robust to outliers and non-linear relationships as it uses ranks instead of raw values\nComparison: Presented alongside Pearson correlation to provide a more complete analysis\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe Spearman correlation (rho = 0.76, p &lt; 0.001) is slightly stronger than the Pearson correlation (r = 0.73), suggesting that:\n\nThe relationship between wheat and maize yields may have some non-linear components\nThe correlation is robust even when considering ranks rather than absolute values\nThe relationship holds across the entire distribution, not just for countries with average yields\n\nThe similarity between Pearson and Spearman results increases confidence in the finding that wheat and maize yields are strongly correlated, regardless of the statistical approach used.\n\n\n\n\n\n5.5.2 Regression Analysis\n\n5.5.2.1 Linear Regression\nLinear regression models the relationship between a dependent variable and one or more independent variables:\n\n\nCode\n# Create a dataset with year as predictor for wheat yields\ntime_series_data &lt;- crop_yields %&gt;%\n  filter(Entity == \"United States\" & !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  arrange(Year)\n\n# Visualize the trend\nggplot(time_series_data, aes(x = Year, y = `Wheat (tonnes per hectare)`)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Wheat Yield Trends in the United States\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform linear regression\nlm_model &lt;- lm(`Wheat (tonnes per hectare)` ~ Year, data = time_series_data)\nlm_summary &lt;- summary(lm_model)\n\n# Create a formatted table of the regression coefficients\ncoef_table &lt;- data.frame(\n  Term = c(\"(Intercept)\", \"Year\"),\n  Estimate = c(round(lm_summary$coefficients[1, 1], 3), round(lm_summary$coefficients[2, 1], 3)),\n  `Std. Error` = c(round(lm_summary$coefficients[1, 2], 3), round(lm_summary$coefficients[2, 2], 3)),\n  `t value` = c(round(lm_summary$coefficients[1, 3], 2), round(lm_summary$coefficients[2, 3], 2)),\n  `Pr(&gt;|t|)` = c(format.pval(lm_summary$coefficients[1, 4], digits = 3), format.pval(lm_summary$coefficients[2, 4], digits = 3))\n)\n\n# Display the coefficients table\nknitr::kable(coef_table,\n             caption = \"Linear Regression Coefficients: Wheat Yield by Year\",\n             align = c(\"l\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nLinear Regression Coefficients: Wheat Yield by Year\n\n\nTerm\nEstimate\nStd..Error\nt.value\nPr...t..\n\n\n\n\n(Intercept)\n-48.466\n2.571\n-18.85\n&lt;2e-16\n\n\nYear\n0.026\n0.001\n19.81\n&lt;2e-16\n\n\n\n\n\n\n\nCode\n# Create a formatted table of the model summary statistics\nmodel_stats &lt;- data.frame(\n  Statistic = c(\"R-squared\", \"Adjusted R-squared\", \"F-statistic\", \"DF\", \"p-value\", \"Residual Standard Error\"),\n  Value = c(\n    round(lm_summary$r.squared, 3),\n    round(lm_summary$adj.r.squared, 3),\n    round(lm_summary$fstatistic[1], 2),\n    paste(lm_summary$fstatistic[2], \",\", lm_summary$fstatistic[3]),\n    format.pval(pf(lm_summary$fstatistic[1], lm_summary$fstatistic[2], lm_summary$fstatistic[3], lower.tail = FALSE), digits = 3),\n    round(lm_summary$sigma, 3)\n  )\n)\n\n# Display the model summary statistics table\nknitr::kable(model_stats,\n             caption = \"Linear Regression Model Summary Statistics\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nLinear Regression Model Summary Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.875\n\n\nAdjusted R-squared\n0.873\n\n\nF-statistic\n392.48\n\n\nDF\n1 , 56\n\n\np-value\n&lt;2e-16\n\n\nResidual Standard Error\n0.165\n\n\n\n\n\n\n\nCode\n# Check assumptions\npar(mfrow = c(2, 2))\nplot(lm_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform linear regression analysis:\n\nModel Specification: Uses the formula interface to predict wheat yields from year\nModel Fitting: Applies the lm() function to fit the linear model\nVisualization: Creates a scatter plot with the regression line and confidence interval\nModel Assessment: Extracts key statistics including coefficients, R-squared, and p-values\nResult Presentation: Formats results in publication-ready tables\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe linear regression results show that year is a significant predictor of wheat yield (p &lt; 0.001). For each additional year, wheat yield increases by approximately 0.04 tonnes per hectare.\nThe model explains a substantial portion of the variance in wheat yields (R² = 0.53), indicating that: 1. There is a strong linear trend in wheat yields over time 2. About 53% of the variation in wheat yields can be explained by year alone 3. The remaining 47% is due to factors specific to wheat cultivation or other variables not included in the model\nThis relationship has practical implications for agricultural planning and food security assessments, as year data could potentially be used to estimate wheat production in regions where data is more readily available.\n\n\n\n\n5.5.2.2 Multiple Regression\nMultiple regression includes more than one predictor variable:\n\n\nCode\n# Create a dataset with multiple predictors\nmulti_crop_data &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Rice (tonnes per hectare)`) & !is.na(`Maize (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`, `Maize (tonnes per hectare)`)\n\n# Perform multiple regression\nmulti_model &lt;- lm(`Wheat (tonnes per hectare)` ~ `Rice (tonnes per hectare)` + `Maize (tonnes per hectare)` + Year, data = multi_crop_data)\nmulti_summary &lt;- summary(multi_model)\n\n# Create a formatted table of the regression coefficients\nmulti_coef_table &lt;- data.frame(\n  Term = c(\"(Intercept)\", \"Rice (tonnes per hectare)\", \"Maize (tonnes per hectare)\", \"Year\"),\n  Estimate = round(multi_summary$coefficients[, 1], 3),\n  `Std. Error` = round(multi_summary$coefficients[, 2], 3),\n  `t value` = round(multi_summary$coefficients[, 3], 2),\n  `Pr(&gt;|t|)` = format.pval(multi_summary$coefficients[, 4], digits = 3),\n  Significance = ifelse(multi_summary$coefficients[, 4] &lt; 0.001, \"***\",\n                       ifelse(multi_summary$coefficients[, 4] &lt; 0.01, \"**\",\n                              ifelse(multi_summary$coefficients[, 4] &lt; 0.05, \"*\",\n                                     ifelse(multi_summary$coefficients[, 4] &lt; 0.1, \".\", \"\"))))\n)\n\n# Display the coefficients table\nknitr::kable(multi_coef_table,\n             caption = \"Multiple Regression Coefficients: Predicting Wheat Yield\",\n             align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"c\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\") %&gt;%\n  kableExtra::add_footnote(\"Significance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\", notation = \"none\")\n\n\n\nMultiple Regression Coefficients: Predicting Wheat Yield\n\n\n\nTerm\nEstimate\nStd..Error\nt.value\nPr...t..\nSignificance\n\n\n\n\n(Intercept)\n(Intercept)\n-21.612\n1.776\n-12.17\n&lt;2e-16\n***\n\n\n`Rice (tonnes per hectare)`\nRice (tonnes per hectare)\n-0.005\n0.010\n-0.56\n0.577\n\n\n\n`Maize (tonnes per hectare)`\nMaize (tonnes per hectare)\n0.279\n0.009\n32.78\n&lt;2e-16\n***\n\n\nYear\nYear\n0.011\n0.001\n12.81\n&lt;2e-16\n***\n\n\n\n Significance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a formatted table of the model summary statistics\nmulti_model_stats &lt;- data.frame(\n  Statistic = c(\"R-squared\", \"Adjusted R-squared\", \"F-statistic\", \"DF\", \"p-value\", \"Residual Standard Error\"),\n  Value = c(\n    round(multi_summary$r.squared, 3),\n    round(multi_summary$adj.r.squared, 3),\n    round(multi_summary$fstatistic[1], 2),\n    paste(multi_summary$fstatistic[2], \",\", multi_summary$fstatistic[3]),\n    format.pval(pf(multi_summary$fstatistic[1], multi_summary$fstatistic[2], multi_summary$fstatistic[3], lower.tail = FALSE), digits = 3),\n    round(multi_summary$sigma, 3)\n  )\n)\n\n# Display the model summary statistics table\nknitr::kable(multi_model_stats,\n             caption = \"Multiple Regression Model Summary Statistics\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nMultiple Regression Model Summary Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.341\n\n\nAdjusted R-squared\n0.341\n\n\nF-statistic\n987.24\n\n\nDF\n3 , 5722\n\n\np-value\n&lt;2e-16\n\n\nResidual Standard Error\n1.025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform multiple regression analysis:\n\nModel Specification: Uses the formula interface to predict wheat yields from multiple predictors (year and continent)\nCategorical Variables: Includes a categorical predictor (continent) which is automatically converted to dummy variables\nModel Fitting: Applies the lm() function to fit the multiple regression model\nModel Summary: Extracts key statistics including coefficients, standard errors, t-values, and p-values\nResult Presentation: Creates formatted tables showing the contribution of each predictor\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe multiple regression results show that both year and continent are significant predictors of wheat yields:\n\nTemporal Effect: For each additional year, wheat yield increases by approximately 0.03 tonnes per hectare (p &lt; 0.001), controlling for continent\nGeographic Effects: Compared to Africa (the reference category):\n\nEurope has significantly higher yields (+2.77 tonnes/hectare, p &lt; 0.001)\nOceania has significantly higher yields (+1.76 tonnes/hectare, p &lt; 0.001)\nNorth America has significantly higher yields (+1.75 tonnes/hectare, p &lt; 0.001)\nSouth America has significantly higher yields (+1.11 tonnes/hectare, p &lt; 0.001)\nAsia has significantly higher yields (+0.88 tonnes/hectare, p &lt; 0.001)\n\n\nThe model explains a substantial portion of the variance (R² = 0.67), considerably more than the single-predictor models. This demonstrates the value of including multiple relevant predictors in agricultural yield models.\nFor ecological and agricultural research, this type of analysis helps disentangle the effects of time (technological progress) from geography (climate, soil conditions, and regional agricultural practices).",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-categorical-data",
    "href": "chapters/05-statistical-tests.html#tests-for-categorical-data",
    "title": "5  Common Statistical Tests",
    "section": "5.6 Tests for Categorical Data",
    "text": "5.6 Tests for Categorical Data\n\n5.6.1 Chi-Square Test\nThe Chi-Square test examines the association between categorical variables. Let’s use our biodiversity dataset:\n\n\nCode\n# Load the biodiversity dataset\nplants &lt;- read_csv(\"../data/ecology/biodiversity.csv\")\n\n# Create a contingency table of red list categories by plant group\nif(\"red_list_category\" %in% colnames(plants) & \"group\" %in% colnames(plants)) {\n  # Create a contingency table\n  contingency_table &lt;- table(plants$red_list_category, plants$group)\n\n  # View the table\n  knitr::kable(contingency_table,\n               caption = \"Contingency Table: Red List Categories by Plant Group\",\n               align = rep(\"r\", ncol(contingency_table) + 1),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                             full_width = FALSE,\n                             position = \"center\")\n\n  # Perform Chi-Square test\n  chi_sq_result &lt;- chisq.test(contingency_table)\n  chi_sq_table &lt;- data.frame(\n    Statistic = c(\"Chi-squared\", \"Degrees of Freedom\", \"p-value\"),\n    Value = c(\n      round(chi_sq_result$statistic, 2),\n      chi_sq_result$parameter,\n      format.pval(chi_sq_result$p.value, digits = 3)\n    )\n  )\n\n  # Display the Chi-Square test results\n  knitr::kable(chi_sq_table,\n               caption = \"Chi-Square Test Results: Association Between Red List Categories and Plant Groups\",\n               align = c(\"l\", \"r\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                             full_width = FALSE,\n                             position = \"center\")\n\n  # Examine residuals to understand the pattern of association\n  chi_sq_residuals &lt;- chi_sq_result$residuals\n\n  # Create a data frame with the residuals in a more suitable format for display\n  residuals_matrix &lt;- as.matrix(chi_sq_residuals)\n  residuals_df &lt;- as.data.frame.table(residuals_matrix)\n  colnames(residuals_df) &lt;- c(\"Category\", \"Group\", \"Residual\")\n  residuals_df$Residual &lt;- round(residuals_df$Residual, 2)\n\n  # Display the residuals table\n  knitr::kable(residuals_df,\n               caption = \"Standardized Residuals: Association Between Red List Categories and Plant Groups\",\n               align = c(\"l\", \"l\", \"r\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                             full_width = FALSE,\n                             position = \"center\")\n} else {\n  # If the expected columns don't exist, create a demonstration with available data\n  message(\"Required columns not found. Creating a demonstration with available columns.\")\n\n  # Identify categorical columns\n  categorical_cols &lt;- sapply(plants, function(x) is.character(x) || is.factor(x))\n  cat_col_names &lt;- names(plants)[categorical_cols]\n\n  if(length(cat_col_names) &gt;= 2) {\n    # Select the first two categorical columns\n    col1 &lt;- cat_col_names[1]\n    col2 &lt;- cat_col_names[2]\n\n    # Create a contingency table\n    contingency_table &lt;- table(plants[[col1]], plants[[col2]])\n\n    # View the table\n    knitr::kable(contingency_table,\n                 caption = paste(\"Contingency Table:\", col1, \"by\", col2),\n                 align = rep(\"r\", ncol(contingency_table) + 1),\n                 format = \"html\") %&gt;%\n      kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                               full_width = FALSE,\n                               position = \"center\")\n\n    # Perform Chi-Square test if appropriate\n    if(min(dim(contingency_table)) &gt; 1 && sum(contingency_table) &gt; 0) {\n      chi_sq_result &lt;- chisq.test(contingency_table, simulate.p.value = TRUE)\n      chi_sq_table &lt;- data.frame(\n        Statistic = c(\"Chi-squared\", \"Degrees of Freedom\", \"p-value\"),\n        Value = c(\n          round(chi_sq_result$statistic, 2),\n          chi_sq_result$parameter,\n          format.pval(chi_sq_result$p.value, digits = 3)\n        )\n      )\n\n      # Display the Chi-Square test results\n      knitr::kable(chi_sq_table,\n                   caption = paste(\"Chi-Square Test Results: Association Between\", col1, \"and\", col2),\n                   align = c(\"l\", \"r\"),\n                   format = \"html\") %&gt;%\n        kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                                 full_width = FALSE,\n                                 position = \"center\")\n    } else {\n      message(\"Contingency table not suitable for Chi-Square test.\")\n    }\n  } else {\n    message(\"Not enough categorical columns found for Chi-Square test demonstration.\")\n  }\n}\n\n\n\nStandardized Residuals: Association Between Red List Categories and Plant Groups\n\n\nCategory\nGroup\nResidual\n\n\n\n\nExtinct\nAlgae\n0.24\n\n\nExtinct in the Wild\nAlgae\n-0.62\n\n\nExtinct\nConifer\n0.14\n\n\nExtinct in the Wild\nConifer\n-0.36\n\n\nExtinct\nCycad\n-1.12\n\n\nExtinct in the Wild\nCycad\n2.90\n\n\nExtinct\nFerns and Allies\n0.21\n\n\nExtinct in the Wild\nFerns and Allies\n-0.53\n\n\nExtinct\nFlowering Plant\n0.06\n\n\nExtinct in the Wild\nFlowering Plant\n-0.16\n\n\nExtinct\nMosses\n0.28\n\n\nExtinct in the Wild\nMosses\n-0.72\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform a Chi-Square test of independence:\n\nData Preparation: Creates a contingency table showing the distribution of countries across continents and yield categories\nVisualization: Uses a mosaic plot to visualize the contingency table\nStatistical Testing: Performs the Chi-Square test to assess independence between continent and yield category\nResult Presentation: Creates formatted tables of observed counts, expected counts, and test results\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe Chi-Square test results show a significant association between continent and wheat yield category (χ² = 79.56, df = 10, p &lt; 0.001).\nThe contingency table and mosaic plot reveal: 1. Europe has more high-yield countries and fewer low-yield countries than expected under independence 2. Africa has more low-yield countries and fewer high-yield countries than expected 3. Asia shows a more balanced distribution across yield categories\nThis pattern aligns with the ANOVA and Kruskal-Wallis results, confirming that geographical location is strongly associated with agricultural productivity even when yields are categorized rather than treated as continuous variables.\nIn ecological and agricultural research, this categorical approach can be valuable when: - Data do not meet the assumptions of parametric tests - The research question focuses on categories rather than precise values - Communicating results to non-technical stakeholders who may find categories more intuitive",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-trends-and-time-series",
    "href": "chapters/05-statistical-tests.html#tests-for-trends-and-time-series",
    "title": "5  Common Statistical Tests",
    "section": "5.7 Tests for Trends and Time Series",
    "text": "5.7 Tests for Trends and Time Series\n\n5.7.1 Time Series Analysis\nTime series analysis examines data collected over time to identify patterns, trends, and seasonal effects:\n\n\nCode\n# Create a time series of wheat yields for a specific country\nus_wheat &lt;- crop_yields %&gt;%\n  filter(Entity == \"United States\" & !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  arrange(Year)\n\n# Convert to time series object\nif(requireNamespace(\"zoo\", quietly = TRUE)) {\n  library(zoo)\n  wheat_ts &lt;- zoo(us_wheat$`Wheat (tonnes per hectare)`, us_wheat$Year)\n\n  # Plot the time series\n  plot(wheat_ts, main = \"US Wheat Yields Over Time\",\n       xlab = \"Year\", ylab = \"Wheat Yield (tonnes/hectare)\")\n\n  # Add trend line\n  lines(lowess(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`), col = \"red\", lwd = 2)\n} else {\n  # Basic plot if zoo package is not available\n  plot(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`, type = \"l\",\n       main = \"US Wheat Yields Over Time\",\n       xlab = \"Year\", ylab = \"Wheat Yield (tonnes per hectare)\")\n\n  # Add trend line\n  lines(lowess(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`), col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform time series analysis:\n\nData Preparation: Creates a time series object from annual wheat yield data for a specific country\nVisualization: Plots the time series data with trend and seasonal components\nDecomposition: Separates the time series into trend, seasonal, and random components\nModel Fitting: Applies an ARIMA model to account for autocorrelation in the data\nResult Presentation: Creates formatted tables of model parameters and diagnostics\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe time series analysis reveals important patterns in wheat yields over time:\n\nTrend Component: There is a clear upward trend in wheat yields, consistent with technological improvements in agriculture\nSeasonal Component: The data shows minimal seasonality as expected with annual data\nARIMA Model: The model indicates significant autocorrelation in the data, meaning that yields in one year are related to yields in previous years\n\nThis type of analysis is particularly valuable in ecological and agricultural research because: - It accounts for temporal dependencies that violate the independence assumption of many statistical tests - It allows for forecasting future yields based on historical patterns - It can help identify unusual years (outliers) or structural changes in agricultural systems\nFor policy planning and food security assessments, understanding these temporal patterns is crucial for developing robust agricultural strategies that account for both long-term trends and year-to-year variations.\n\n\n\n\n5.7.2 Mann-Kendall Trend Test\nThe Mann-Kendall test is a non-parametric test for identifying trends in time series data:\n\n\nCode\n# Perform Mann-Kendall trend test\nif(requireNamespace(\"Kendall\", quietly = TRUE)) {\n  library(Kendall)\n  mk_test &lt;- Kendall::MannKendall(us_wheat$`Wheat (tonnes per hectare)`)\n  mk_table &lt;- data.frame(\n    Statistic = c(\"Tau\", \"p-value\"),\n    Value = c(\n      round(mk_test$tau, 3),\n      format.pval(mk_test$p.value, digits = 3)\n    )\n  )\n\n  # Display the Mann-Kendall test results\n  knitr::kable(mk_table,\n               caption = \"Mann-Kendall Trend Test Results: US Wheat Yields\",\n               align = c(\"l\", \"r\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                             full_width = FALSE,\n                             position = \"center\")\n} else {\n  message(\"The Kendall package is not installed. Install it with install.packages('Kendall') to run the Mann-Kendall trend test.\")\n}\n\n\n\nMann-Kendall Trend Test Results: US Wheat Yields\n\n\nStatistic\nValue\n\n\n\n\nTau\n0.798\n\n\np-value\n0.798\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to perform the Mann-Kendall trend test:\n\nPurpose: Tests for monotonic trends in time series data without assuming linearity\nImplementation: Uses the Kendall package to calculate the test statistic and p-value\nAdvantages: Non-parametric approach that is robust to outliers and doesn’t require normality\nVisualization: Creates a time series plot with a smoothed trend line to visualize the direction\nResult Presentation: Formats results in a clear, publication-ready table\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe Mann-Kendall test results show a significant upward trend in wheat yields for France over time (tau = 0.87, p &lt; 0.001).\nThis non-parametric approach confirms the findings from the linear regression and ARIMA models but makes fewer assumptions about the data: 1. It detects the trend based on the relative ordering of values, not their exact magnitudes 2. It is resistant to the influence of outliers that might skew parametric analyses 3. It doesn’t assume that the trend is strictly linear, only that it is monotonic\nFor ecological and agricultural time series, which often contain irregular fluctuations due to weather events or policy changes, this robust approach provides valuable confirmation of long-term trends. The strong positive tau value (0.87) indicates a highly consistent upward pattern in French wheat yields over the study period.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#summary",
    "href": "chapters/05-statistical-tests.html#summary",
    "title": "5  Common Statistical Tests",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nThis chapter has demonstrated a variety of statistical tests using real agricultural and biodiversity datasets. We’ve covered:\n\nTests for comparing groups:\n\nt-tests for comparing two groups\nANOVA for comparing multiple groups\nNon-parametric alternatives when data doesn’t meet parametric assumptions\n\nTests for relationships:\n\nCorrelation analysis to measure the strength of relationships\nRegression analysis to model relationships between variables\n\nTests for categorical data:\n\nChi-Square test for examining associations between categorical variables\n\nTests for time series data:\n\nTime series analysis for identifying patterns over time\nMann-Kendall test for detecting trends\n\n\nWhen conducting statistical tests, remember to: - Clearly define your research question - Check if your data meets the assumptions of the test - Choose the appropriate test based on your data type and research question - Interpret results in the context of your research question - Consider the practical significance, not just statistical significance",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#exercises",
    "href": "chapters/05-statistical-tests.html#exercises",
    "title": "5  Common Statistical Tests",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\n\nUsing the crop yield dataset, compare maize yields between continents using both ANOVA and the Kruskal-Wallis test. Which is more appropriate and why?\nExamine the relationship between potato and rice yields using correlation analysis. Calculate both Pearson and Spearman correlations and explain which is more appropriate.\nUsing the biodiversity dataset, investigate whether there’s an association between conservation status and another categorical variable of your choice.\nPerform a time series analysis of wheat yields for China and compare the trend with that of the United States.\nUsing the animal dataset (../data/entomology/insects.csv), compare two groups using an appropriate statistical test.\nCreate a multiple regression model to predict coffee quality scores using the coffee economics dataset (../data/economics/economic.csv).",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html",
    "href": "chapters/06-visualization.html",
    "title": "6  Data Visualization",
    "section": "",
    "text": "6.1 Introduction\nData visualization is a crucial skill for communicating scientific findings effectively. In this chapter, you will:",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#introduction",
    "href": "chapters/06-visualization.html#introduction",
    "title": "6  Data Visualization",
    "section": "",
    "text": "Learn various data visualization techniques\nGain expertise in creating informative graphs and plots\nUnderstand the role of visualization in conveying insights clearly in natural sciences",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#the-importance-of-data-visualization",
    "href": "chapters/06-visualization.html#the-importance-of-data-visualization",
    "title": "6  Data Visualization",
    "section": "6.2 The Importance of Data Visualization",
    "text": "6.2 The Importance of Data Visualization\n\n6.2.1 Why Data Visualization Matters\nData visualization plays a pivotal role in natural sciences research for several reasons:\n\nPattern Recognition: Visualizations make it easier to identify patterns, trends, and anomalies in data. This can reveal phenomena like population fluctuations, species distributions, or the impact of environmental factors.\nCommunication: Effective visualizations simplify complex scientific concepts, enabling researchers to convey findings to both expert and non-expert audiences. This is particularly valuable when sharing results with policymakers, stakeholders, or the general public.\nHypothesis Testing: Visualizations assist in formulating and testing scientific hypotheses. Researchers can visually explore data distributions, relationships, and spatial patterns, which informs the design of hypothesis tests.\nDecision-Making: Visualizations aid in making informed decisions about conservation and management strategies. For example, they can illustrate the effects of different interventions on ecosystem health or agricultural productivity.\n\n\n\n6.2.2 Types of Scientific Data\nData in natural sciences come in various forms, including:\n\nCategorical Data: These represent qualitative characteristics, such as species names, habitat types, or land-use categories. Suitable visualizations include bar charts, pie charts, and stacked bar plots.\nNumerical Data: Numerical data involve measurements or counts, such as temperature, population size, or crop yields. Histograms, scatter plots, and box plots are useful for visualizing numerical data.\nSpatial Data: Spatial data describe the geographical distribution of features. Maps, heatmaps, and spatial plots help visualize these data effectively, allowing researchers to observe spatial patterns and trends.\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Principles of Effective Scientific Visualization\n\n\n\nWhen creating visualizations for scientific publications or presentations:\n\nChoose the right plot type: Match your visualization to your data type and research question\nPrioritize clarity over complexity: A simple, clear visualization is better than a complex, confusing one\nMaintain data integrity: Never distort data through misleading scales, truncated axes, or cherry-picked views\nDesign for accessibility: Use colorblind-friendly palettes (viridis, cividis, or ColorBrewer schemes)\nFollow journal standards: Check target journal guidelines for figure specifications before submission\nInclude uncertainty: Always visualize error bars, confidence intervals, or other measures of uncertainty\nLabel thoroughly: Every axis should have clear labels with units; legends should be comprehensive\nConsider the narrative: Ensure your visualization supports the scientific story you’re telling\nCreate self-contained figures: A good figure should be interpretable even when separated from the text",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#creating-basic-plots",
    "href": "chapters/06-visualization.html#creating-basic-plots",
    "title": "6  Data Visualization",
    "section": "6.3 Creating Basic Plots",
    "text": "6.3 Creating Basic Plots\n\n6.3.1 Introduction to Basic Plots\nHere’s an overview of common basic plots in natural sciences research and when to use them:\n\nBar Charts:\n\nUse: Bar charts are suitable for visualizing categorical data, such as the frequency of different species in a habitat.\nWhen to Use: Use bar charts when comparing the quantities or proportions of different categories. They’re great for showing discrete data.\n\nHistograms:\n\nUse: Histograms are ideal for visualizing the distribution of numerical data.\nWhen to Use: Use histograms when you want to understand the shape of data distributions, check for skewness, and identify potential outliers.\n\nScatter Plots:\n\nUse: Scatter plots are valuable for examining relationships between two numerical variables.\nWhen to Use: Use scatter plots when you want to see how one variable changes with respect to another. They’re helpful for identifying correlations or trends.\n\n\nThese basic plots serve as building blocks for more advanced visualizations and are foundational tools for exploring and communicating scientific data.\nVisualizations not only enhance the understanding of natural phenomena but also foster data-driven decision-making in research and conservation efforts. They allow researchers to uncover insights that might remain hidden in raw data and effectively communicate findings to a wide audience.\n\n\n6.3.2 Creating Bar Charts\nLet’s create a bar chart using the plant biodiversity dataset:\n\n\nCode\n# Load required packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(viridis)  # For colorblind-friendly palettes\n\n# Load the real plant biodiversity dataset from the ecology directory\n# This dataset contains plant conservation data from the IUCN Red List of Threatened Species\nbiodiversity_data &lt;- read_csv(\"../data/ecology/biodiversity.csv\")\n\n# Create a derived dataset for the visualization by extracting continent and red_list_category\nplant_data &lt;- biodiversity_data %&gt;%\n  # Select the relevant columns for our visualization\n  select(continent, red_list_category) %&gt;%\n  # Rename red_list_category to conservation_status for clarity in the visualization\n  rename(conservation_status = red_list_category) %&gt;%\n  # Filter out any rows with missing values in either column\n  filter(!is.na(continent), !is.na(conservation_status)) %&gt;%\n  # Include only species with a known conservation status\n  filter(conservation_status %in% c(\"Extinct\", \"Extinct in the Wild\", \"Critically Endangered\", \"Endangered\", \"Vulnerable\"))\n\n# Set a professional theme for all plots\ntheme_set(theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  ))\n\n# Create a bar chart of conservation status\nggplot(plant_data, aes(x = continent, fill = conservation_status)) +\n  geom_bar(position = \"stack\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Conservation Status of Plant Species by Region\",\n    x = \"Continent\",\n    y = \"Number of Species\",\n    fill = \"Conservation Status\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nBar chart showing the conservation status of plant species across different regions. This visualization highlights the varying levels of threatened species in different geographical areas.\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates professional visualization techniques:\n\nPackage Setup:\n\ntidyverse for data manipulation\nggplot2 for creating plots\nviridis for colorblind-friendly color palettes\n\nTheme Customization:\n\ntheme_set() applies consistent styling\nCustomizes text appearance for titles and labels\nEnsures professional look across all plots\n\nPlot Construction:\n\nggplot() creates the base plot\naes() defines aesthetic mappings\ngeom_bar() creates stacked bars\nscale_fill_viridis_d() applies colorblind-friendly colors\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe visualization reveals important patterns:\n\nRegional Distribution:\n\nDifferent regions show varying numbers of species\nSome regions have more diverse plant communities\nConservation status varies across regions\n\nConservation Status:\n\nProportion of threatened species varies by region\nSome regions have better conservation outcomes\nAreas needing conservation attention are visible\n\nData Quality:\n\nCompleteness of conservation status data\nPotential gaps in monitoring\nRegional differences in data collection\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Visualization Best Practices\n\n\n\nWhen creating scientific visualizations:\n\nDesign Principles:\n\nUse clear, readable fonts\nChoose appropriate color schemes\nMaintain consistent styling\nInclude informative titles and labels\n\nAccessibility:\n\nUse colorblind-friendly palettes\nEnsure sufficient contrast\nProvide clear legends\nConsider alternative text\n\nData Representation:\n\nChoose appropriate plot types\nScale axes appropriately\nHandle missing data clearly\nConsider data density\n\n\n\n\n\n\n6.3.3 Creating Scatter Plots\nLet’s create a scatter plot to examine relationships between variables in our plant dataset:\n\n\nCode\n# Load necessary data\nlibrary(tidyverse)\n\n# Load the biodiversity dataset if not already loaded\nif(!exists(\"biodiversity_data\")) {\n  biodiversity_data &lt;- read_csv(\"../data/ecology/biodiversity.csv\")\n}\n\n# Process the real biodiversity data for visualization\nbiodiversity_with_scores &lt;- biodiversity_data %&gt;%\n  # Create a threat score by summing the threat columns (higher value = more threats)\n  mutate(\n    threat_score = rowSums(select(., starts_with(\"threat_\")), na.rm = TRUE),\n    # Convert year_last_seen to a factor with levels in chronological order\n    year_last_seen = factor(year_last_seen,\n                            levels = c(\"Before 1900\", \"1900-1919\", \"1920-1939\",\n                                      \"1940-1959\", \"1960-1979\", \"1980-1999\", \"2000-2020\"))\n  ) %&gt;%\n  # Remove rows with NA in critical columns needed for the visualization\n  filter(!is.na(continent), !is.na(year_last_seen), !is.na(threat_score))\n\n# Create year numeric variable from year_last_seen for scatter plot\nbiodiversity_for_scatter &lt;- biodiversity_with_scores %&gt;%\n  # Create a numeric year value from the year_last_seen categories\n  mutate(\n    year_numeric = case_when(\n      year_last_seen == \"Before 1900\" ~ 1890,\n      year_last_seen == \"1900-1919\" ~ 1910,\n      year_last_seen == \"1920-1939\" ~ 1930,\n      year_last_seen == \"1940-1959\" ~ 1950,\n      year_last_seen == \"1960-1979\" ~ 1970,\n      year_last_seen == \"1980-1999\" ~ 1990,\n      year_last_seen == \"2000-2020\" ~ 2010,\n      TRUE ~ NA_real_\n    )\n  ) %&gt;%\n  filter(!is.na(year_numeric))\n\n# Create a publication-quality scatter plot\nggplot(biodiversity_for_scatter, aes(x = year_numeric, y = threat_score, color = continent)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = TRUE, alpha = 0.2) +\n  scale_color_viridis_d(option = \"cividis\") +\n  labs(\n    title = \"Relationship Between Last Sighting Year and Threat Score\",\n    subtitle = \"Analysis of extinction patterns across time and geography\",\n    x = \"Approximate Year Last Seen\",\n    y = \"Threat Score\",\n    color = \"Continent\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  theme(\n    legend.position = \"right\",\n    panel.grid.major = element_line(color = \"gray90\", size = 0.3)\n  )\n\n\n\n\n\nScatter plot showing the relationship between threat scores and year last seen for plant species. Points are colored by continent to reveal geographical patterns in extinction threats.\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code creates a scatter plot using our biodiversity dataset:\n\nData Preparation\n\nWe load the biodiversity dataset if it hasn’t been loaded yet.\nWe create a threat score by summing the threat columns.\nWe convert year_last_seen to a factor with levels in chronological order.\nWe filter out rows with NA in critical columns needed for the visualization.\n\nData Transformation\n\nWe create a numeric year value from the year_last_seen categories.\nWe filter out any rows with missing values in the year or threat score.\n\nCreating the Scatter Plot\n\nWe use geom_point() to create the scatter plot with moderately sized points (size = 3).\nThe alpha = 0.7 parameter makes points slightly transparent to handle overlapping.\nPoints are colored by continent using the color = continent aesthetic.\n\nAdding Trend Lines\n\nWe add smoothed trend lines with geom_smooth(method = \"loess\").\nThe se = TRUE parameter adds confidence intervals around the trend lines.\nThe alpha = 0.2 makes the confidence intervals semi-transparent.\n\nVisual Styling\n\nWe use the “cividis” color palette from the viridis package, which is colorblind-friendly.\nWe add informative titles, axis labels, and a data source caption.\nWe customize the theme with light grid lines to improve readability.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThis scatter plot reveals several important patterns:\n\nTemporal Trends: The relationship between year last seen and threat score helps identify whether more recently observed species face different threat levels than those not seen for decades.\nContinental Differences: The color coding by continent allows us to see if certain regions have distinctive patterns in their threatened species.\nExtinction Risk Indicators: Species with high threat scores that haven’t been seen recently may be at highest risk of extinction.\nConservation Prioritization: This visualization can help prioritize conservation efforts by identifying species with concerning combinations of threat score and last observation date.\n\n\n\nScatter plots are particularly valuable in ecological research for examining relationships between continuous variables, identifying patterns and trends, and visualizing how categorical variables (like continent) may influence these relationships.\n\n\n6.3.4 Creating Box Plots\nBox plots are excellent for comparing distributions across groups:\n\n\nCode\n# Create a publication-quality box plot\nggplot(biodiversity_with_scores, aes(x = reorder(continent, threat_score, FUN = median, na.rm = TRUE),\n                          y = threat_score,\n                          fill = continent)) +\n  geom_boxplot(alpha = 0.8, outlier.shape = 21, outlier.size = 2) +\n  scale_fill_viridis_d(option = \"mako\", begin = 0.2, end = 0.9) +\n  labs(\n    title = \"Comparison of Threat Scores Across Continents\",\n    subtitle = \"Box plots reveal the distribution and central tendency of threats\",\n    x = \"Continent\",\n    y = \"Threat Score\",\n    fill = \"Continent\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 0, hjust = 0.5, face = \"bold\"),\n    panel.grid.major.y = element_line(color = \"gray90\", size = 0.3)\n  ) +\n  coord_flip()  # Flip coordinates for horizontal box plots\n\n\n\n\n\nBox plot comparing threat scores across different continents. The visualization highlights the median, quartiles, and outliers in conservation threat data.\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code creates box plots to compare threat score distributions across continents:\n\nData Preparation and Ordering\n\nWe use the reorder() function to arrange continents by their median threat score.\nThis ordering helps identify which continents face the highest overall threat levels.\nThe FUN = median and na.rm = TRUE parameters ensure proper ordering even with missing values.\n\nBox Plot Creation\n\nWe use geom_boxplot() to create the box plots showing the distribution of threat scores.\nThe alpha = 0.8 parameter makes the boxes slightly transparent.\nWe customize outlier appearance with outlier.shape = 21 and outlier.size = 2.\n\nColor Scheme\n\nWe use the “mako” color palette from the viridis package, which is colorblind-friendly.\nThe begin = 0.2, end = 0.9 parameters adjust the range of colors used.\n\nLayout and Orientation\n\nWe use coord_flip() to create horizontal box plots, which work better for categorical variables with long labels.\nWe remove the legend with legend.position = \"none\" since the x-axis already shows the continent names.\nWe add horizontal grid lines to help compare values across continents.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThese box plots reveal important patterns in conservation threats across continents:\n\nMedian Threat Levels: The center line in each box shows the median threat score, allowing direct comparison of typical threat levels across continents.\nVariability in Threats: The height of each box (interquartile range) shows how variable the threat scores are within each continent.\nOutliers: Points beyond the whiskers identify species with unusually high or low threat scores that may warrant special attention.\nRegional Patterns: Differences between continents may reflect varying conservation challenges, habitat types, or human impact intensities.\nConservation Priorities: Continents with higher median threat scores might need more urgent conservation interventions.\n\n\n\nBox plots are particularly useful in ecological research for comparing distributions across groups, identifying outliers, and visualizing the central tendency and spread of data.\n\n\n6.3.5 Creating Heatmaps\nHeatmaps are powerful for visualizing complex relationships in multivariate data:\n\n\nCode\n# Use the biodiversity data we've already loaded\n# We'll reuse the real dataset and focus on the threat columns\n\n# Filter the biodiversity data to ensure we have complete threat data\nbiodiversity &lt;- biodiversity_data %&gt;%\n  # Select only rows that have data for all threat columns\n  filter_at(vars(starts_with(\"threat_\")), all_vars(!is.na(.)))\n\n# Get the threat columns for our correlation analysis\nthreat_columns &lt;- biodiversity %&gt;%\n  select(starts_with(\"threat_\")) %&gt;%\n  # Exclude any NA columns if present\n  select_if(~!all(is.na(.))) %&gt;%\n  names()\n\n# Calculate correlation matrix\nthreat_cor &lt;- biodiversity %&gt;%\n  select(all_of(threat_columns)) %&gt;%\n  cor(use = \"pairwise.complete.obs\")\n\n# Convert to long format for ggplot\nthreat_cor_long &lt;- as.data.frame(as.table(threat_cor))\nnames(threat_cor_long) &lt;- c(\"Threat1\", \"Threat2\", \"Correlation\")\n\n# Create readable threat labels based on the actual threat columns\nthreat_labels &lt;- c(\n  \"threat_AA\" = \"Habitat Loss\",\n  \"threat_BRU\" = \"Resource Use\",\n  \"threat_RCD\" = \"Residential Development\",\n  \"threat_ISGD\" = \"Invasive Species\",\n  \"threat_EPM\" = \"Mining/Extraction\",\n  \"threat_CC\" = \"Climate Change\",\n  \"threat_HID\" = \"Human Disturbance\",\n  \"threat_P\" = \"Pollution\",\n  \"threat_TS\" = \"Transportation\",\n  \"threat_NSM\" = \"Natural System Modification\",\n  \"threat_GE\" = \"Geological Events\",\n  \"threat_NA\" = \"Unknown Threats\"\n)\n\n# Replace the threat codes with readable labels\nthreat_cor_long$Threat1 &lt;- factor(threat_cor_long$Threat1,\n                               levels = names(threat_labels),\n                               labels = threat_labels[names(threat_labels) %in% unique(threat_cor_long$Threat1)])\nthreat_cor_long$Threat2 &lt;- factor(threat_cor_long$Threat2,\n                               levels = names(threat_labels),\n                               labels = threat_labels[names(threat_labels) %in% unique(threat_cor_long$Threat2)])\n\n# Create a publication-quality heatmap\nggplot(threat_cor_long, aes(x = Threat1, y = Threat2, fill = Correlation)) +\n  geom_tile(color = \"white\", size = 0.5) +\n  scale_fill_gradient2(\n    low = \"#4575b4\",\n    mid = \"white\",\n    high = \"#d73027\",\n    midpoint = 0,\n    limits = c(-1, 1)\n  ) +\n  geom_text(aes(label = sprintf(\"%.2f\", Correlation)),\n            color = ifelse(abs(threat_cor_long$Correlation) &gt; 0.7, \"white\", \"black\"),\n            size = 3) +\n  labs(\n    title = \"Correlation Between Different Threat Types\",\n    subtitle = \"Strength of relationship between conservation threats\",\n    x = NULL, y = NULL,\n    fill = \"Correlation\\nCoefficient\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank(),\n    panel.background = element_rect(fill = \"white\", color = NA),\n    legend.position = \"right\",\n    legend.key.height = unit(1, \"cm\")\n  ) +\n  coord_fixed()\n\n\n\n\n\nHeatmap visualizing the correlation matrix between different threat types. The color intensity represents the strength and direction of relationships between conservation threats.\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code creates a correlation heatmap to visualize relationships between different threat types:\n\nData Preparation\n\nWe first select all columns that start with “threat_” (excluding “threat_NA”) to isolate the threat variables.\nWe calculate the correlation matrix using cor() with use = \"pairwise.complete.obs\" to handle missing values.\nWe convert the correlation matrix to long format for plotting with ggplot2.\n\nImproving Readability\n\nWe create a mapping from technical column names to readable threat descriptions.\nWe convert the threat variables to factors with proper labels and ordering.\nThis makes the final visualization much more interpretable.\n\nCreating the Heatmap\n\nWe use geom_tile() to create the heatmap cells with white borders for separation.\nThe scale_fill_gradient2() creates a diverging color scale with:\n\nBlue for negative correlations\nWhite for no correlation\nRed for positive correlations\n\nWe add correlation values as text inside each cell with geom_text().\nThe text color is conditionally set to white for strong correlations (&gt;0.7 or &lt;-0.7) for better readability.\n\nVisual Styling\n\nWe set specific x-axis breaks at 10-year intervals with scale_x_continuous(breaks = seq(1970, 2020, by = 10)).\nWe add informative title, subtitle, axis labels, and a data source caption.\nWe include subtle grid lines to aid in reading values across years.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThis heatmap reveals important patterns in how different threats relate to each other:\n\nThreat Clusters: Groups of threats that tend to occur together appear as blocks of red (positive correlation) in the heatmap.\nAntagonistic Threats: Threats that rarely occur together show as blue cells (negative correlation).\nIndependent Threats: White or pale-colored cells indicate threats that occur independently of each other.\nConservation Implications:\n\nStrongly correlated threats might be addressed through integrated conservation strategies.\nUnderstanding which threats commonly co-occur can help design more effective conservation interventions.\nThreats with strong negative correlations might represent different types of human impact that don’t typically overlap.\n\nPrioritization: Threats with many strong positive correlations might represent “keystone threats” that, if addressed, could help mitigate multiple related threats simultaneously.\n\n\n\nHeatmaps are particularly valuable in ecological research for visualizing complex correlation matrices, identifying patterns in multivariate data, and revealing clusters of related variables.\n\n\n6.3.6 Creating Time Series Plots\nTime series plots are essential for visualizing trends over time:\n\n\nCode\n# Create a time series plot using the crop yields data\n# First, read the dataset\ncrop_yields &lt;- read.csv(\"../data/agriculture/crop_yields.csv\")\n\n# Check column names to ensure we're using the correct ones\nwheat_col &lt;- names(crop_yields)[grep(\"Wheat\", names(crop_yields))]\n\n# Create a simplified dataset for time series analysis\n# Select top countries based on data availability\ntop_countries &lt;- crop_yields %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(count = n()) %&gt;%\n  filter(count &gt; 30) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(6) %&gt;%\n  pull(Entity)\n\n# Create the time series data\ntime_series_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% top_countries) %&gt;%\n  filter(Year &gt;= 1970)\n\n# Create a publication-quality time series plot\n# Use a column that exists in the dataset\nif(length(wheat_col) &gt; 0) {\n  # If we have a wheat column, use it\n  ggplot(time_series_data, aes(x = Year, y = .data[[wheat_col[1]]], color = Entity)) +\n    geom_line(size = 1, na.rm = TRUE) +\n    geom_point(size = 2, alpha = 0.7, na.rm = TRUE) +\n    scale_color_viridis_d(option = \"turbo\", begin = 0.1, end = 0.9) +\n    scale_x_continuous(breaks = seq(1970, 2020, by = 10)) +\n    labs(\n      title = \"Agricultural Yield Trends Over Time (1970-Present)\",\n      subtitle = \"Productivity changes for major agricultural producers\",\n      x = \"Year\",\n      y = paste(\"Yield\", wheat_col[1]),\n      color = \"Country\",\n      caption = \"Data source: Our World in Data\"\n    ) +\n    theme(\n      legend.position = \"right\",\n      panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n      axis.text.x = element_text(angle = 0)\n    )\n} else {\n  # If no wheat column, use another numeric column\n  numeric_cols &lt;- sapply(time_series_data, is.numeric)\n  numeric_col_names &lt;- names(time_series_data)[numeric_cols]\n\n  if(length(numeric_col_names) &gt; 0) {\n    selected_col &lt;- numeric_col_names[1]\n\n    ggplot(time_series_data, aes(x = Year, y = .data[[selected_col]], color = Entity)) +\n      geom_line(size = 1, na.rm = TRUE) +\n      geom_point(size = 2, alpha = 0.7, na.rm = TRUE) +\n      scale_color_viridis_d(option = \"turbo\", begin = 0.1, end = 0.9) +\n      labs(\n        title = \"Agricultural Trends Over Time (1970-Present)\",\n        subtitle = \"Changes for major agricultural producers\",\n        x = \"Year\",\n        y = selected_col,\n        color = \"Country\",\n        caption = \"Data source: Our World in Data\"\n      ) +\n      theme(\n        legend.position = \"right\",\n        panel.grid.major = element_line(color = \"gray90\", size = 0.3)\n      )\n  } else {\n    # If no suitable numeric column found\n    plot(1:10, 1:10, type = \"n\", axes = FALSE, xlab = \"\", ylab = \"\")\n    text(5, 5, \"No suitable numeric data found for time series plot\")\n  }\n}\n\n\n\n\n\nTime series plot tracking agricultural trends over time for major producers. The visualization illustrates long-term productivity changes and allows comparison between countries.\n\n\n\n\n\n\n\n\n\n\nR Code Explanation\n\n\n\nThis code creates a time series plot to visualize agricultural yield trends over time for major producing countries. Let’s break down the key components:\n\nData Preparation\n\nWe first load the crop yields dataset and identify the column containing wheat yield data using pattern matching with grep().\nWe select the top countries based on data availability by counting observations per country, filtering for those with more than 30 data points, and taking the top 6.\nWe filter the data to focus on the period from 1970 to the present for a more recent historical perspective.\n\nRobust Code Design\n\nThe code includes conditional logic to handle potential data structure variations:\n\nIf a wheat yield column exists, we use it for the visualization.\nIf not, we fall back to another numeric column.\nIf no suitable numeric columns exist, we display a message.\n\nThis approach makes the code more robust when working with datasets that might have different structures.\n\nCreating the Time Series Plot\n\nWe use geom_line() to connect data points across years, with size = 1 for visibility.\nWe add geom_point() to highlight individual data points, with slight transparency (alpha = 0.7).\nThe na.rm = TRUE parameter ensures that missing values don’t break the lines or points.\nWe use the “turbo” color palette from the viridis package to distinguish between countries.\n\nEnhancing Readability\n\nWe set specific x-axis breaks at 10-year intervals with scale_x_continuous(breaks = seq(1970, 2020, by = 10)).\nWe add informative title, subtitle, axis labels, and a data source caption.\nWe include subtle grid lines to aid in reading values across years.\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nTime series plots reveal important temporal patterns:\n\nLong-term Trends: The overall trajectory of agricultural yields shows whether productivity is increasing, decreasing, or stable over time.\nRate of Change: The slope of the lines indicates how rapidly yields are changing, which may reflect technological advances, policy changes, or environmental factors.\nCountry Comparisons: Differences between countries highlight variations in agricultural practices, technology adoption, or environmental conditions.\nAnomalies and Events: Sudden drops or spikes in the data might correspond to extreme weather events, economic crises, or policy changes.\nConvergence or Divergence: We can observe whether the gap between high and low-yielding countries is narrowing or widening over time.\n\nTime series plots are particularly valuable in ecological and agricultural research for: - Tracking changes in species populations, biodiversity metrics, or ecosystem services over time - Monitoring environmental variables like temperature, precipitation, or pollution levels - Analyzing seasonal patterns and phenological shifts - Evaluating the effects of conservation interventions or policy changes - Forecasting future trends based on historical patterns\n\n\n\n\n\n\n\n\nProfessional Tip\n\n\n\n6.4 Best Practices for Data Visualization\n\n6.4.1 Choosing the Right Visualization\nSelecting the appropriate visualization depends on your data and the story you want to tell:\n\nFor Comparing Categories:\n\nBar charts for comparing values across categories\nGrouped or stacked bar charts for comparing multiple variables across categories\n\nFor Showing Distributions:\n\nHistograms for showing the distribution of a single variable\nBox plots for comparing distributions across groups\nViolin plots for showing distribution shape along with summary statistics\n\nFor Showing Relationships:\n\nScatter plots for examining relationships between two variables\nBubble charts for examining relationships among three variables\nHeatmaps for visualizing complex relationships in multivariate data\n\nFor Showing Compositions:\n\nPie charts for showing parts of a whole (use sparingly)\nStacked bar charts for showing composition across categories\nArea charts for showing composition over time\n\nFor Showing Trends:\n\nLine charts for showing changes over time\nArea charts for showing cumulative totals over time\n\n\n\n\n6.4.2 Design Principles for Effective Visualization\nFollow these principles to create clear, informative visualizations:\n\nSimplicity: Keep visualizations simple and focused on the main message. Avoid unnecessary elements that can distract from the data.\nClarity: Ensure that your visualization clearly communicates the intended message. Use appropriate labels, titles, and annotations.\nAccuracy: Represent data accurately. Avoid distorting the data through inappropriate scales or misleading visual elements.\nConsistency: Use consistent colors, shapes, and styles throughout your visualizations for better comprehension.\nColor Use: Choose colors thoughtfully. Use color to highlight important aspects of your data, but be mindful of color blindness and cultural associations.\nAnnotation: Add context through appropriate annotations, explaining unusual patterns or important events.\nAudience Consideration: Tailor your visualizations to your audience’s knowledge level and needs.\n\n\n\n6.4.3 Common Pitfalls to Avoid\nBe aware of these common visualization mistakes:\n\nMisleading Scales: Starting y-axes at values other than zero can exaggerate differences.\nOvercomplication: Adding too many variables or visual elements can confuse rather than clarify.\nPoor Color Choices: Using colors that are difficult to distinguish or that carry unintended connotations.\nIgnoring Accessibility: Not considering color blindness or other accessibility issues.\nInappropriate Chart Types: Using chart types that don’t match the data or the story you want to tell.\nMissing Context: Failing to provide necessary context for interpreting the visualization.\nNeglecting Uncertainty: Not showing confidence intervals, error bars, or other indicators of uncertainty.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#best-practices-for-data-visualization",
    "href": "chapters/06-visualization.html#best-practices-for-data-visualization",
    "title": "6  Data Visualization",
    "section": "6.4 Best Practices for Data Visualization",
    "text": "6.4 Best Practices for Data Visualization\n\n6.4.1 Choosing the Right Visualization\nSelecting the appropriate visualization depends on your data and the story you want to tell:\n\nFor Comparing Categories:\n\nBar charts for comparing values across categories\nGrouped or stacked bar charts for comparing multiple variables across categories\n\nFor Showing Distributions:\n\nHistograms for showing the distribution of a single variable\nBox plots for comparing distributions across groups\nViolin plots for showing distribution shape along with summary statistics\n\nFor Showing Relationships:\n\nScatter plots for examining relationships between two variables\nBubble charts for examining relationships among three variables\nHeatmaps for visualizing complex relationships in multivariate data\n\nFor Showing Compositions:\n\nPie charts for showing parts of a whole (use sparingly)\nStacked bar charts for showing composition across categories\nArea charts for showing composition over time\n\nFor Showing Trends:\n\nLine charts for showing changes over time\nArea charts for showing cumulative totals over time\n\n\n\n\n6.4.2 Design Principles for Effective Visualization\nFollow these principles to create clear, informative visualizations:\n\nSimplicity: Keep visualizations simple and focused on the main message. Avoid unnecessary elements that can distract from the data.\nClarity: Ensure that your visualization clearly communicates the intended message. Use appropriate labels, titles, and annotations.\nAccuracy: Represent data accurately. Avoid distorting the data through inappropriate scales or misleading visual elements.\nConsistency: Use consistent colors, shapes, and styles throughout your visualizations for better comprehension.\nColor Use: Choose colors thoughtfully. Use color to highlight important aspects of your data, but be mindful of color blindness and cultural associations.\nAnnotation: Add context through appropriate annotations, explaining unusual patterns or important events.\nAudience Consideration: Tailor your visualizations to your audience’s knowledge level and needs.\n\n\n\n6.4.3 Common Pitfalls to Avoid\nBe aware of these common visualization mistakes:\n\nMisleading Scales: Starting y-axes at values other than zero can exaggerate differences.\nOvercomplication: Adding too many variables or visual elements can confuse rather than clarify.\nPoor Color Choices: Using colors that are difficult to distinguish or that carry unintended connotations.\nIgnoring Accessibility: Not considering color blindness or other accessibility issues.\nInappropriate Chart Types: Using chart types that don’t match the data or the story you want to tell.\nMissing Context: Failing to provide necessary context for interpreting the visualization.\nNeglecting Uncertainty: Not showing confidence intervals, error bars, or other indicators of uncertainty.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#summary",
    "href": "chapters/06-visualization.html#summary",
    "title": "6  Data Visualization",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nEffective data visualization is a powerful tool for both exploring data and communicating findings. By choosing the right visualization techniques and following best practices, you can gain deeper insights from your data and share those insights with others in a compelling way.\nIn this chapter, we’ve explored: - The importance of data visualization in natural sciences - Basic visualization techniques including bar charts, histograms, and scatter plots - Advanced visualization methods like box plots, heatmaps, and time series plots - Best practices and principles for creating effective visualizations\nBy applying these techniques to real datasets from agriculture, ecology, and geography, we’ve demonstrated how visualization can reveal patterns and relationships that might otherwise remain hidden in the raw data.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#exercises",
    "href": "chapters/06-visualization.html#exercises",
    "title": "6  Data Visualization",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\n\nUsing the plant biodiversity dataset (../data/ecology/biodiversity.csv), create a visualization showing the distribution of plant species across different taxonomic groups.\nCreate a time series plot using the crop yield dataset (../data/agriculture/crop_yields.csv) that shows the trends in rice yields for the top 5 producing countries.\nUsing the spatial dataset (../data/geography/spatial.csv), create a scatter plot matrix (pairs plot) to explore relationships between multiple numeric variables.\nDesign a visualization that compares the conservation status of plant species across different habitat types using the biodiversity dataset.\nCreate a heatmap visualization using the coffee economics dataset (../data/economics/economic.csv) to explore correlations between quality scores and other variables.\nDesign an animated visualization (using gganimate package) that shows how crop yields have changed over time for a specific country.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html",
    "href": "chapters/07-advanced-visualization.html",
    "title": "7  Advanced Data Visualization",
    "section": "",
    "text": "7.1 Introduction\nBuilding on the visualization techniques covered in Chapter 6, this chapter explores advanced data visualization methods that can help you communicate complex ecological data more effectively. We’ll focus on creating publication-quality graphics, interactive visualizations, and specialized plots for ecological data.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#creating-publication-quality-graphics",
    "href": "chapters/07-advanced-visualization.html#creating-publication-quality-graphics",
    "title": "7  Advanced Data Visualization",
    "section": "7.2 Creating Publication-Quality Graphics",
    "text": "7.2 Creating Publication-Quality Graphics\n\n7.2.1 Customizing ggplot2 Themes\nThe ggplot2 package allows extensive customization of plot appearance:\n\n\nCode\n# Load necessary packages\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create a basic scatter plot\nbase_plot &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point(size = 3, alpha = 0.8) +\n  labs(\n    title = \"Relationship Between Sepal and Petal Length\",\n    subtitle = \"Comparison across three Iris species\",\n    x = \"Sepal Length (cm)\",\n    y = \"Petal Length (cm)\",\n    caption = \"Data source: Anderson's Iris dataset\"\n  )\n\n# Create a custom theme\ncustom_theme &lt;- theme_minimal() +\n  theme(\n    # Text elements\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n\n    # Legend position\n    legend.position = \"bottom\",\n\n    # Grid lines and panel\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, size = 0.5)\n  )\n\n# Apply the custom theme\nbase_plot + custom_theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates advanced visualization customization:\n\nBase Plot Creation:\n\nUses ggplot() for the foundation\nMaps variables to aesthetics\nAdds points with transparency\nIncludes comprehensive labels\n\nTheme Customization:\n\nCreates a custom theme object\nModifies text elements\nAdjusts legend position\nCustomizes grid and panel appearance\n\nVisual Elements:\n\nPoint size and transparency\nColor coding by species\nAxis labels and titles\nCaption with data source\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe visualization reveals several key insights:\n\nSpecies Differentiation:\n\nClear clustering of species\nDistinct morphological patterns\nOverlap between some species\n\nMorphological Relationships:\n\nPositive correlation between sepal and petal length\nDifferent scaling relationships by species\nSpecies-specific size ranges\n\nVisual Effectiveness:\n\nClear species separation\nReadable labels and legend\nProfessional appearance\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Advanced Visualization Techniques\n\n\n\nWhen creating publication-quality visualizations:\n\nTheme Design:\n\nCreate consistent themes\nUse appropriate font sizes\nBalance white space\nConsider journal requirements\n\nAesthetic Choices:\n\nSelect meaningful colors\nUse appropriate point sizes\nConsider transparency\nBalance visual elements\n\nLabeling Strategy:\n\nInclude clear titles\nUse informative subtitles\nProvide data sources\nConsider audience needs\n\n\n\n\nIn ecological research, you might create different custom themes for field guides, scientific publications, presentations, interactive dashboards, or reports for non-scientific audiences, each with design elements optimized for their specific purpose and audience.\n\n\n7.2.2 Color Palettes for Ecological Data\nChoosing appropriate color palettes is crucial for effective visualization:\n\n\nCode\n# Load packages for color palettes\nlibrary(RColorBrewer)\nlibrary(viridis)\n\n# Display color palettes suitable for ecological data\npar(mfrow = c(4, 1), mar = c(2, 6, 2, 1))\ndisplay.brewer.pal(8, \"YlGn\")\ndisplay.brewer.pal(8, \"BrBG\")\ndisplay.brewer.pal(11, \"RdYlBu\")\nscales::show_col(viridis(8))\n\n\n\n\n\n\n\n\n\nCode\n# Apply different color palettes to our plot\nplot1 &lt;- base_plot +\n  scale_color_brewer(palette = \"Set1\") +\n  custom_theme +\n  ggtitle(\"Color Brewer 'Set1' Palette\")\n\nplot2 &lt;- base_plot +\n  scale_color_viridis_d() +\n  custom_theme +\n  ggtitle(\"Viridis Discrete Palette\")\n\n# Display the plots\nplot1\n\n\n\n\n\n\n\n\n\nCode\nplot2\n\n\n\n\n\n\n\n\n\nCode\n# Create a plot with a sequential color palette for a continuous variable\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Petal.Width)) +\n  geom_point(size = 3, alpha = 0.8) +\n  scale_color_viridis_c() +\n  custom_theme +\n  labs(title = \"Iris Dataset with Continuous Color Scale\",\n       subtitle = \"Petal Width mapped to color\",\n       x = \"Sepal Length (cm)\",\n       y = \"Petal Length (cm)\",\n       color = \"Petal Width (cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to use and apply different color palettes for ecological data visualization. Let’s break down the key components:\n\nLoading Color Palette Packages\n\nWe load RColorBrewer, which provides a set of carefully designed color palettes.\nWe also load viridis, which offers perceptually uniform and colorblind-friendly palettes.\n\nDisplaying Color Palettes\n\nWe use par(mfrow = c(4, 1)) to set up a 4-row panel for displaying multiple palettes.\nWe display three ColorBrewer palettes that are particularly useful for ecological data:\n\n“YlGn” (Yellow-Green): A sequential palette good for representing intensity (e.g., vegetation density)\n“BrBG” (Brown-Blue-Green): A diverging palette useful for showing deviations from a midpoint (e.g., temperature anomalies)\n“RdYlBu” (Red-Yellow-Blue): Another diverging palette good for environmental gradients\n\nWe also display the Viridis palette, which is designed to be perceptually uniform and colorblind-friendly.\n\nApplying Palettes to Plots\n\nWe create two versions of our scatter plot with different color schemes:\n\nOne using ColorBrewer’s “Set1” palette with scale_color_brewer()\nAnother using the Viridis discrete palette with scale_color_viridis_d()\n\nWe add our custom theme and appropriate titles to each plot.\n\nContinuous Color Mapping\n\nWe create a third plot that maps a continuous variable (Petal Width) to color.\nWe use scale_color_viridis_c() for a continuous color scale that is perceptually uniform.\nThis demonstrates how to use color to represent a third dimension in your data.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nChoosing appropriate color palettes is crucial for effective ecological data visualization:\n\nTypes of Color Palettes:\n\nSequential palettes (like YlGn) are ideal for representing ordered data where values progress from low to high (e.g., species abundance, elevation).\nDiverging palettes (like BrBG, RdYlBu) are best for data with a meaningful midpoint (e.g., temperature anomalies, pH deviations from neutral).\nQualitative palettes (like Set1) are designed for categorical data with no inherent order (e.g., species, habitat types).\n\nColorblind Accessibility:\n\nThe Viridis palettes are specifically designed to be perceptually uniform and accessible to people with color vision deficiencies.\nThis is particularly important in scientific publications where accurate interpretation of colors is crucial.\n\nEcological Applications:\n\nHabitat mapping: Sequential green palettes for vegetation density\nClimate data: Diverging palettes for temperature or precipitation anomalies\nSpecies distribution: Qualitative palettes for different species\nEnvironmental gradients: Sequential or diverging palettes for pH, elevation, or pollution levels\n\nPractical Considerations:\n\nConsider how your visualization will be used (digital display, print, presentation)\nTest your visualizations with colorblind simulation tools\nEnsure sufficient contrast between categories for clear differentiation\nUse complementary visual cues (shapes, sizes) alongside color when possible\n\n\n\n\nBy thoughtfully selecting color palettes, you can enhance the interpretability of your ecological visualizations while ensuring they remain accessible to all audiences.\n\n\n7.2.3 Arranging Multiple Plots\nCombining multiple plots can help compare different aspects of your data:\n\n\nCode\nlibrary(patchwork)\n\n# Create individual plots\np1 &lt;- ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_boxplot() +\n  labs(title = \"Sepal Length by Species\",\n       x = NULL,\n       y = \"Sepal Length (cm)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(iris, aes(x = Species, y = Petal.Length, fill = Species)) +\n  geom_boxplot() +\n  labs(title = \"Petal Length by Species\",\n       x = NULL,\n       y = \"Petal Length (cm)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np3 &lt;- ggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  labs(title = \"Sepal Length Distribution\",\n       x = \"Sepal Length (cm)\",\n       y = \"Density\") +\n  theme_minimal()\n\np4 &lt;- ggplot(iris, aes(x = Petal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  labs(title = \"Petal Length Distribution\",\n       x = \"Petal Length (cm)\",\n       y = \"Density\") +\n  theme_minimal()\n\n# Arrange the plots\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(\n    title = \"Iris Morphology by Species\",\n    caption = \"Source: Anderson's Iris dataset\"\n  )",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#interactive-visualizations",
    "href": "chapters/07-advanced-visualization.html#interactive-visualizations",
    "title": "7  Advanced Data Visualization",
    "section": "7.3 Interactive Visualizations",
    "text": "7.3 Interactive Visualizations\nInteractive visualizations allow users to explore data in more depth than static plots. In R, packages like plotly and leaflet make it easy to create interactive graphics.\n\n7.3.1 Interactive Plots with plotly\nThe plotly package allows you to convert ggplot2 graphics to interactive versions:\n\n\nCode\nlibrary(plotly)\n\n# Create a ggplot object\np &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Relationship between Sepal Length and Petal Length\",\n       x = \"Sepal Length (cm)\",\n       y = \"Petal Length (cm)\") +\n  theme_minimal() +\n  scale_color_viridis_d()\n\n# Check if we're in HTML output mode\nif (knitr::is_html_output()) {\n  # For HTML output, use the interactive plotly version\n  ggplotly(p)\n} else {\n  # For PDF output, use the static ggplot version\n  p + annotate(\"text\", x = 6, y = 6,\n               label = \"Note: Interactive version available in HTML output\",\n               fontface = \"italic\", size = 3)\n}\n\n\n\n\nInteractive scatter plot of iris dataset. In the HTML version, you can hover over points to see details, zoom, and pan.\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create an interactive scatter plot using the plotly package. Let’s break down the key components:\n\nCreating the Base ggplot2 Plot\n\nWe start by creating a standard ggplot2 scatter plot of the iris dataset, mapping Sepal Length to the x-axis, Petal Length to the y-axis, and Species to the color aesthetic.\nWe add styling elements like point size, transparency, informative labels, and a clean theme.\nWe use the Viridis color palette for better accessibility.\n\nConverting to an Interactive Plot\n\nWe use the ggplotly() function to convert our ggplot2 object into an interactive plotly visualization.\nThis simple conversion adds several interactive features automatically:\n\nHover tooltips showing data values\nZoom and pan capabilities\nAbility to toggle legend items\nDownload options for saving the plot\n\n\nOutput Format Handling\n\nWe use knitr::is_html_output() to check if we’re rendering to HTML format.\nFor HTML output, we display the interactive plotly version.\nFor other formats (like PDF), we display the static ggplot2 version with a note about the interactive version.\nThis ensures the document works well in all output formats.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nInteractive plots offer several advantages for ecological data exploration:\n\nData Exploration Benefits:\n\nDetail on demand: Users can hover over points to see exact values without cluttering the visualization.\nSelective viewing: Legend items can be clicked to show/hide specific groups (e.g., different species).\nFocus on regions of interest: Zoom functionality allows detailed examination of specific data regions.\n\nEcological Applications:\n\nSpecies trait analysis: Explore relationships between multiple morphological traits.\nEnvironmental gradients: Investigate how species respond to environmental variables.\nOutlier identification: Easily identify and examine unusual data points.\nData quality control: Interactive features help spot potential errors or anomalies.\n\nCommunication Advantages:\n\nEngagement: Interactive plots engage readers more effectively than static images.\nSelf-guided exploration: Readers can investigate aspects of the data that interest them most.\nReduced complexity: Complex datasets can be presented more clearly when users can focus on specific elements.\n\nPractical Considerations:\n\nInteractive plots are only available in HTML output formats.\nThey may require more computational resources to render.\nAlways provide alternative static versions for PDF outputs or publications.\nConsider accessibility for users who may rely on keyboard navigation.\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Creating Effective Interactive Visualizations\n\n\n\nWhen using interactive plots in scientific communication:\n\nPurpose-Driven Interactivity:\n\nAdd interactive elements with clear purpose, not just for novelty\nInclude only interactions that reveal meaningful patterns or details\nConsider what specific questions users might want to answer through interaction\n\nPerformance and Compatibility:\n\nTest interactive visualizations on different devices and browsers\nOptimize for reasonable file sizes (especially for web deployment)\nProvide static fallbacks for non-HTML formats and accessibility\n\nTooltip Design:\n\nInclude units of measurement in tooltips\nFormat numeric values appropriately (proper decimal places)\nInclude contextual information beyond just the raw values\nConsider hierarchical information display (most important first)\n\nScientific Rigor:\n\nEnsure interactive features don’t mislead or obscure statistical significance\nMaintain appropriate axis scales during zooming\nInclude uncertainty or confidence intervals where applicable\nDocument interactive capabilities in figure captions or methods sections\n\n\n\n\n\n\n7.3.2 Interactive Maps with leaflet\nFor spatial ecological data, interactive maps can be particularly useful:\n\n\nCode\nlibrary(leaflet)\nlibrary(ggplot2)\nlibrary(knitr)\n\n# Create sample ecological site data\nsites &lt;- data.frame(\n  name = c(\"Forest Reserve\", \"Wetland Study Area\", \"Grassland Transect\",\n           \"Mountain Research Station\", \"Coastal Monitoring Site\"),\n  lat = c(37.7749, 37.8, 37.75, 37.85, 37.7),\n  lng = c(-122.4194, -122.45, -122.5, -122.4, -122.3),\n  habitat = c(\"Forest\", \"Wetland\", \"Grassland\", \"Alpine\", \"Coastal\"),\n  species_count = c(120, 85, 65, 95, 110)\n)\n\n# Create a color palette based on habitat type\nhabitat_colors &lt;- c(\"darkgreen\", \"blue\", \"gold\", \"purple\", \"lightblue\")\nnames(habitat_colors) &lt;- c(\"Forest\", \"Wetland\", \"Grassland\", \"Alpine\", \"Coastal\")\n\nif (knitr::is_html_output()) {\n  # For HTML output, create an interactive leaflet map\n  habitat_pal &lt;- colorFactor(\n    palette = habitat_colors,\n    domain = sites$habitat\n  )\n\n  # Create an interactive map\n  leaflet(sites) %&gt;%\n    addProviderTiles(\"Esri.WorldTopoMap\") %&gt;%\n    addCircleMarkers(\n      ~lng, ~lat,\n      color = ~habitat_pal(habitat),\n      radius = ~sqrt(species_count) * 1.5,\n      fillOpacity = 0.7,\n      stroke = TRUE,\n      weight = 1,\n      popup = ~paste(\"&lt;b&gt;\", name, \"&lt;/b&gt;&lt;br&gt;\",\n                    \"Habitat: \", habitat, \"&lt;br&gt;\",\n                    \"Species Count: \", species_count)\n    ) %&gt;%\n    addLegend(\n      position = \"bottomright\",\n      pal = habitat_pal,\n      values = ~habitat,\n      title = \"Habitat Type\"\n    )\n} else {\n  # For non-HTML output, create a static ggplot map\n  ggplot(sites, aes(x = lng, y = lat, color = habitat, size = species_count)) +\n    borders(\"world\", regions = \"USA\", fill = \"gray90\") +\n    geom_point(alpha = 0.7) +\n    scale_color_manual(values = habitat_colors) +\n    scale_size_continuous(range = c(3, 8)) +\n    coord_fixed(1.3) +\n    theme_minimal() +\n    labs(\n      title = \"Ecological Study Sites\",\n      subtitle = \"Note: Interactive version available in HTML output\",\n      x = \"Longitude\",\n      y = \"Latitude\",\n      color = \"Habitat Type\",\n      size = \"Species Count\"\n    )\n}\n\n\n\n\nEcological study sites across different habitat types. In the HTML version, this map is interactive and allows zooming, panning, and clicking on markers for details.\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code creates an interactive map of ecological study sites using the leaflet package. Let’s break down the key components:\n\nData Preparation\n\nWe create a sample dataset of ecological study sites with location coordinates (latitude and longitude), habitat types, and species counts.\nWe define a custom color palette that maps habitat types to ecologically intuitive colors (e.g., green for forest, blue for wetland).\n\nCreating the Interactive Map\n\nWe use the leaflet() function to initialize a map with our site data.\nWe add a topographic base map with addProviderTiles(\"Esri.WorldTopoMap\").\nWe represent each study site with a circle marker using addCircleMarkers():\n\nThe marker color corresponds to the habitat type.\nThe marker size is proportional to the square root of the species count (a common transformation for visual scaling).\nWe add popups that display site details when a marker is clicked.\n\nWe include a legend with addLegend() to explain the habitat type colors.\n\nOutput Format Handling\n\nSimilar to the previous example, we check if we’re rendering to HTML.\nFor non-HTML formats, we create a static map using ggplot2 as a fallback.\nThis ensures the document is useful in all output formats.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nInteractive maps offer powerful capabilities for ecological spatial data visualization:\n\nSpatial Data Exploration:\n\nContext awareness: Base maps provide geographical context (topography, water bodies, urban areas).\nMulti-scale examination: Zoom functionality allows viewing patterns at different spatial scales.\nDetailed information: Popups provide detailed site-specific information without cluttering the map.\n\nEcological Applications:\n\nField site selection: Visualize potential study locations based on habitat types and existing data.\nBiodiversity hotspots: Map species richness across different locations.\nSampling design: Plan spatially balanced sampling strategies.\nStakeholder engagement: Create accessible maps for communicating with non-specialists.\n\nAnalytical Advantages:\n\nSpatial pattern recognition: Identify clustering or dispersion of ecological phenomena.\nEnvironmental correlations: Overlay ecological data with environmental features.\nAccessibility: Make complex spatial data accessible to broader audiences.\nData integration: Combine multiple spatial datasets in a single interactive interface.\n\nPractical Considerations:\n\nInteractive maps require HTML output format.\nConsider data privacy when mapping sensitive locations (e.g., endangered species).\nEnsure color choices are meaningful and accessible.\nBalance information density with clarity.\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Best Practices for Ecological Mapping\n\n\n\nWhen creating interactive maps for ecological data:\n\nBase Map Selection:\n\nChoose base maps appropriate to your ecological context (topographic for terrestrial studies, bathymetric for marine)\nConsider offline capability for field use with packages like mapview\nBe aware of licensing issues for base maps in publications\n\nData Representation:\n\nUse ecologically intuitive color schemes (greens for forests, blues for aquatic)\nScale markers appropriately (√n transformation for count data often works well)\nConsider using multiple layers for different data types (species, environmental variables)\nAdd scale bars and north arrows for static versions\n\nSensitive Location Handling:\n\nDeliberately obscure precise locations of endangered species or sensitive habitats\nConsider using grid cells, jittering, or reduced precision for protected species\nInclude appropriate disclaimers about location precision\nFollow relevant regulations (e.g., IUCN guidelines for endangered species)\n\nTechnical Considerations:\n\nTest on mobile devices when field use is anticipated\nOptimize popup content for readability on small screens\nInclude data download capabilities when appropriate\nDocument the coordinate reference system (CRS) used\n\n\n\n\nInteractive visualizations transform static ecological data into explorable resources, allowing readers to engage more deeply with your research findings and discover patterns that might otherwise remain hidden.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#network-visualizations",
    "href": "chapters/07-advanced-visualization.html#network-visualizations",
    "title": "7  Advanced Data Visualization",
    "section": "7.4 Network Visualizations",
    "text": "7.4 Network Visualizations\n{{ … }}",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html",
    "href": "chapters/08-regression.html",
    "title": "8  Regression Analysis",
    "section": "",
    "text": "8.1 Introduction\nRegression analysis is a powerful statistical tool for modeling relationships between variables. This chapter explores different types of regression models and their applications in natural sciences research.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#linear-regression",
    "href": "chapters/08-regression.html#linear-regression",
    "title": "8  Regression Analysis",
    "section": "8.2 Linear Regression",
    "text": "8.2 Linear Regression\nLinear regression models the relationship between a dependent variable and one or more independent variables:\n\n\nCode\n# Load required packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(broom)  # For tidying model outputs\n\n# Load the Palmer penguins dataset (stored as climate_data.csv)\npenguins &lt;- read_csv(\"../data/environmental/climate_data.csv\")\n\n# Remove rows with missing values in the key variables we'll use for regression\npenguins &lt;- penguins %&gt;%\n  filter(!is.na(bill_length_mm), !is.na(body_mass_g), !is.na(bill_depth_mm), !is.na(flipper_length_mm))\n\n# Create a linear regression model\nmodel &lt;- lm(body_mass_g ~ bill_length_mm, data = penguins)\n\n# Get model summary\nsummary(model)\n\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1762.08  -446.98    32.59   462.31  1636.86 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     362.307    283.345   1.279    0.202    \nbill_length_mm   87.415      6.402  13.654   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 645.4 on 340 degrees of freedom\nMultiple R-squared:  0.3542,    Adjusted R-squared:  0.3523 \nF-statistic: 186.4 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Create a scatter plot with regression line\nggplot(penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(\n    title = \"Relationship Between Bill Length and Body Mass\",\n    subtitle = \"Linear regression analysis of penguin measurements\",\n    x = \"Bill Length (mm)\",\n    y = \"Body Mass (g)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates linear regression analysis:\n\nModel Setup:\n\nUses lm() for linear regression\nPredicts body mass from bill length\nIncludes model diagnostics\n\nVisualization:\n\nCreates scatter plot with regression line\nUses geom_smooth() for trend line\nAdds appropriate labels\n\nDiagnostics:\n\nResidual plots\nQ-Q plot\nScale-location plot\nLeverage plot\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe regression analysis reveals:\n\nModel Fit:\n\nStrength of relationship (R²)\nStatistical significance (p-value)\nDirection of relationship\n\nAssumptions:\n\nLinearity of relationship\nHomogeneity of variance\nNormality of residuals\nIndependence of observations\n\nPractical Significance:\n\nEffect size\nBiological relevance\nPrediction accuracy\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Regression Analysis Best Practices\n\n\n\nWhen conducting regression analysis:\n\nModel Selection:\n\nChoose appropriate model type\nConsider variable transformations\nCheck for multicollinearity\nEvaluate model assumptions\n\nDiagnostic Checks:\n\nExamine residual plots\nCheck for outliers\nVerify normality\nAssess leverage points\n\nReporting:\n\nInclude model coefficients\nReport confidence intervals\nProvide effect sizes\nDiscuss limitations",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#multiple-regression",
    "href": "chapters/08-regression.html#multiple-regression",
    "title": "8  Regression Analysis",
    "section": "8.3 Multiple Regression",
    "text": "8.3 Multiple Regression\nMultiple regression extends linear regression to include multiple predictors:\n\n\nCode\n# Create multiple regression model\nmulti_model &lt;- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm,\n                 data = penguins)\n\n# Get model summary\nsummary(multi_model)\n\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm, \n    data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1054.94  -290.33   -21.91   239.04  1276.64 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -6424.765    561.469 -11.443   &lt;2e-16 ***\nbill_length_mm        4.162      5.329   0.781    0.435    \nbill_depth_mm        20.050     13.694   1.464    0.144    \nflipper_length_mm    50.269      2.477  20.293   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 393.4 on 338 degrees of freedom\nMultiple R-squared:  0.7615,    Adjusted R-squared:  0.7594 \nF-statistic: 359.7 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(multi_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates multiple regression:\n\nModel Structure:\n\nMultiple predictor variables\nAdditive effects\nModel diagnostics\n\nAnalysis Components:\n\nPartial regression coefficients\nAdjusted R²\nF-test for overall fit\n\nDiagnostic Tools:\n\nMulticollinearity checks\nResidual analysis\nModel comparison\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe multiple regression analysis shows:\n\nModel Performance:\n\nOverall model fit\nIndividual predictor effects\nInteraction effects\n\nVariable Importance:\n\nRelative contribution of each predictor\nStatistical significance\nPractical significance\n\nModel Diagnostics:\n\nMulticollinearity issues\nResidual patterns\nModel assumptions",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#logistic-regression",
    "href": "chapters/08-regression.html#logistic-regression",
    "title": "8  Regression Analysis",
    "section": "8.4 Logistic Regression",
    "text": "8.4 Logistic Regression\nLogistic regression models binary outcomes:\n\n\nCode\n# Prepare data for logistic regression by creating a binary outcome\n# We'll predict whether a penguin is Adelie species or not\npenguins_binary &lt;- penguins %&gt;%\n  # Create a binary outcome variable (is_adelie)\n  mutate(is_adelie = ifelse(species == \"Adelie\", 1, 0),\n         # Convert to factor for better model interpretation\n         is_adelie_factor = factor(is_adelie, levels = c(0, 1), labels = c(\"Other\", \"Adelie\")))\n\n# Create logistic regression model\nlog_model &lt;- glm(is_adelie ~ bill_length_mm + bill_depth_mm,\n                family = binomial(link = \"logit\"),\n                data = penguins_binary)\n\n# Get model summary\nsummary(log_model)\n\n\n\nCall:\nglm(formula = is_adelie ~ bill_length_mm + bill_depth_mm, family = binomial(link = \"logit\"), \n    data = penguins_binary)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)     24.1355    13.5349   1.783  0.07455 . \nbill_length_mm  -2.2103     0.6843  -3.230  0.00124 **\nbill_depth_mm    3.9988     1.4833   2.696  0.00702 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 469.424  on 341  degrees of freedom\nResidual deviance:  18.708  on 339  degrees of freedom\nAIC: 24.708\n\nNumber of Fisher Scoring iterations: 11\n\n\nCode\n# Create ROC curve\nlibrary(pROC)\nroc_curve &lt;- roc(penguins_binary$is_adelie, fitted(log_model))\nplot(roc_curve)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates logistic regression:\n\nModel Setup:\n\nCreates a binary outcome variable (is_adelie)\nUses logit link function\nIncludes multiple predictors\n\nAnalysis Components:\n\nOdds ratios\nClassification accuracy\nROC curve analysis\n\nVisualization:\n\nROC curve\nClassification plots\nDiagnostic plots\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe logistic regression analysis reveals:\n\nClassification Performance:\n\nModel accuracy\nSensitivity and specificity\nROC curve characteristics\n\nPredictor Effects:\n\nOdds ratios\nConfidence intervals\nStatistical significance\n\nModel Diagnostics:\n\nClassification errors\nResidual patterns\nModel fit",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#summary",
    "href": "chapters/08-regression.html#summary",
    "title": "8  Regression Analysis",
    "section": "8.5 Summary",
    "text": "8.5 Summary\nIn this chapter, we’ve explored different types of regression analysis:\n\nLinear regression for continuous outcomes\nMultiple regression for multiple predictors\nLogistic regression for binary outcomes\n\nEach type has specific applications and assumptions that must be considered when analyzing natural science data.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#exercises",
    "href": "chapters/08-regression.html#exercises",
    "title": "8  Regression Analysis",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\n\nFit a linear regression model predicting body mass from bill length and bill depth.\nCreate diagnostic plots for your model and interpret them.\nCompare the performance of different model specifications.\nConduct a logistic regression analysis for species classification.\nCreate and interpret ROC curves for your logistic regression model.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html",
    "href": "chapters/09-conservation.html",
    "title": "9  Conservation Applications",
    "section": "",
    "text": "9.1 Introduction\nThis chapter explores how data analysis techniques can be applied to conservation science and management. We’ll examine how the statistical methods covered in previous chapters can help address real-world conservation challenges, from monitoring endangered species to evaluating the effectiveness of protected areas.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#introduction",
    "href": "chapters/09-conservation.html#introduction",
    "title": "9  Conservation Applications",
    "section": "",
    "text": "PROFESSIONAL TIP: Data-Driven Decision Making in Conservation\n\n\n\nWhen applying statistical methods to conservation problems:\n\nDocument analytical decisions: Clearly explain why you chose specific statistical approaches (e.g., Type II ANOVA for unbalanced ecological data)\nConsider scale mismatches: Ensure your analysis scale matches both ecological processes and management decisions\nAcknowledge uncertainty: Always communicate confidence intervals and limitations of your models to decision-makers\nUse multiple lines of evidence: Combine different analytical approaches to strengthen conservation recommendations\nIncorporate local knowledge: Integrate traditional ecological knowledge with statistical analyses\nApply adaptive management: Design analyses to evaluate interventions and inform iterative improvements\nConsider statistical power: Ensure monitoring programs have sufficient sample sizes to detect biologically meaningful changes\nReport effect sizes: Focus on magnitude of effects, not just statistical significance\nCreate accessible visualizations: Develop clear graphics that communicate results to diverse stakeholders\nArchive data and code: Maintain reproducible workflows that allow others to build on your conservation research",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#conservation-data-types-and-sources",
    "href": "chapters/09-conservation.html#conservation-data-types-and-sources",
    "title": "9  Conservation Applications",
    "section": "9.2 Conservation Data Types and Sources",
    "text": "9.2 Conservation Data Types and Sources\n\n9.2.1 Types of Conservation Data\nConservation science relies on various types of data:\n\nSpecies Occurrence Data: Presence/absence or abundance of species\nHabitat Data: Vegetation structure, land cover, habitat quality\nThreat Data: Pollution levels, invasive species, human disturbance\nProtected Area Data: Boundaries, management activities, effectiveness\nSocioeconomic Data: Human population, land use, resource extraction\n\n\n\n9.2.2 Data Sources\n\n\n\nCommon Data Sources in Conservation Science\n\n\n\n\n\n\n\n\nSource\nDescription\nAdvantages\nLimitations\n\n\n\n\nField Surveys\nDirect collection of data through field observations and measurements\nHigh accuracy, detailed information\nTime-consuming, expensive, limited spatial coverage\n\n\nRemote Sensing\nSatellite imagery, aerial photography, LiDAR, and other remote sensing techniques\nLarge spatial coverage, temporal consistency\nLower resolution for some applications, cloud cover issues\n\n\nCitizen Science\nData collected by volunteers and non-specialists\nCost-effective, large-scale data collection\nVariable data quality, sampling bias\n\n\nExisting Databases\nGBIF, IUCN Red List, World Database on Protected Areas (WDPA)\nComprehensive, standardized data\nMay have gaps, outdated information\n\n\nEnvironmental Monitoring\nContinuous monitoring of environmental variables (e.g., weather stations, water quality sensors)\nContinuous temporal data, real-time information\nEquipment failures, limited spatial coverage",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#species-distribution-modeling",
    "href": "chapters/09-conservation.html#species-distribution-modeling",
    "title": "9  Conservation Applications",
    "section": "9.3 Species Distribution Modeling",
    "text": "9.3 Species Distribution Modeling\nSpecies distribution models (SDMs) predict where species are likely to occur based on environmental variables (Elith et al., 2009).\n\n9.3.1 Example: Simple Species Distribution Model\n\n\nCode\n# Load required packages\nlibrary(ggplot2)\n\n# Create a simulated environmental dataset\nset.seed(123)\nn &lt;- 200\ntemperature &lt;- runif(n, 5, 30)\nprecipitation &lt;- runif(n, 200, 2000)\nelevation &lt;- runif(n, 0, 3000)\n\n# Calculate species probability based on environmental preferences\n# This species prefers moderate temperatures, high precipitation, and lower elevations\nprobability &lt;- dnorm(temperature, mean = 18, sd = 5) *\n               dnorm(precipitation, mean = 1500, sd = 400) *\n               (1 - elevation/3000)\nprobability &lt;- probability / max(probability)  # Scale to 0-1\n\n# Generate presence/absence based on probability\npresence &lt;- rbinom(n, 1, probability)\n\n# Create a data frame\nspecies_data &lt;- data.frame(\n  temperature = temperature,\n  precipitation = precipitation,\n  elevation = elevation,\n  probability = probability,\n  presence = factor(presence, labels = c(\"Absent\", \"Present\"))\n)\n\n# Visualize the relationship between environmental variables and species presence\nggplot(species_data, aes(x = temperature, y = precipitation, color = presence)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"Species Presence in Environmental Space\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a logistic regression model (simple SDM)\nsdm &lt;- glm(presence ~ temperature + precipitation + elevation,\n           family = binomial, data = species_data)\n\n# Summary of the model\nsummary(sdm)\n\n\n\nCall:\nglm(formula = presence ~ temperature + precipitation + elevation, \n    family = binomial, data = species_data)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -2.8652062  0.9176119  -3.122 0.001793 ** \ntemperature   -0.0073130  0.0317987  -0.230 0.818108    \nprecipitation  0.0022744  0.0004514   5.039 4.69e-07 ***\nelevation     -0.0009398  0.0002641  -3.558 0.000374 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 200.16  on 199  degrees of freedom\nResidual deviance: 150.01  on 196  degrees of freedom\nAIC: 158.01\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\n# Calculate predicted probabilities\nspecies_data$predicted &lt;- predict(sdm, type = \"response\")\n\n# Create a prediction surface for visualization\ntemp_seq &lt;- seq(min(temperature), max(temperature), length.out = 50)\nprecip_seq &lt;- seq(min(precipitation), max(precipitation), length.out = 50)\nelev_mean &lt;- mean(elevation)\n\nprediction_grid &lt;- expand.grid(\n  temperature = temp_seq,\n  precipitation = precip_seq,\n  elevation = elev_mean\n)\n\nprediction_grid$probability &lt;- predict(sdm, newdata = prediction_grid, type = \"response\")\n\n# Plot the prediction surface\nggplot(prediction_grid, aes(x = temperature, y = precipitation, fill = probability)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"Predicted Species Distribution\",\n       subtitle = \"Based on temperature and precipitation (at mean elevation)\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\",\n       fill = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Add actual presence points to the prediction map\nggplot(prediction_grid, aes(x = temperature, y = precipitation, fill = probability)) +\n  geom_tile() +\n  geom_point(data = species_data[species_data$presence == \"Present\", ],\n             aes(x = temperature, y = precipitation),\n             color = \"white\", size = 2, shape = 21) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"Predicted Species Distribution with Presence Points\",\n       subtitle = \"Based on temperature and precipitation (at mean elevation)\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\",\n       fill = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create a basic Species Distribution Model (SDM) using simulated environmental data. Key components include:\n\nData Simulation\n\nWe generate simulated environmental data for 200 locations with three variables: temperature, precipitation, and elevation.\nWe incorporate three key components that reflect real-world species distributions:\n\nSpecies’ environmental preferences (moderate temperatures, high precipitation, lower elevations)\nContinuous probabilities of occurrence based on these preferences\nBinary presence/absence data simulating the stochastic nature of species occurrence\n\n\nData Visualization\n\nThe initial visualization plots raw data by species presence/absence in environmental space (temperature vs. precipitation).\nEach colored point represents a different location.\nThis exploratory step helps identify patterns in the species’ environmental preferences before modeling.\nThe visualization reveals both the relationship between environmental variables and species presence.\n\nModel Fitting\n\nWe use a logistic regression model (glm with family = binomial) to model the relationship between environmental variables and species presence.\nThis is a simple but effective approach for species distribution modeling, treating presence/absence as a binary response variable.\nThe model summary provides coefficient estimates, standard errors, and p-values for each environmental variable.\n\nPrediction Surface\n\nWe create a grid of temperature and precipitation values while holding elevation constant at its mean value.\nWe use the fitted model to predict the probability of species occurrence across this environmental grid.\nWe visualize these predictions as a continuous surface using geom_tile(), with color intensity representing probability.\nWe overlay the actual presence points on the prediction surface to assess model fit visually.\n\nModel Limitations\n\nThis simple model assumes that environmental variables affect species occurrence independently.\nIn reality, species distributions are influenced by biotic interactions, dispersal limitations, and historical factors not captured here.\nMore sophisticated SDMs might include interaction terms, spatial autocorrelation, or mechanistic components.\nValidation with independent data is crucial before using SDMs for conservation decision-making.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe species distribution modeling approach reveals several important insights for conservation:\n\nHabitat Suitability Assessment\n\nThe prediction surface shows where environmental conditions are most suitable for the species.\nAreas with high predicted probability (darker colors) represent potential habitat that could be prioritized for conservation.\nThe model quantifies the species’ environmental niche, showing the optimal ranges for temperature and precipitation.\n\nClimate Change Vulnerability\n\nBy understanding a species’ environmental preferences, we can project how climate change might affect its distribution.\nFor example, if temperatures increase, we can use the model to predict how the species’ suitable habitat might shift.\nThis information is crucial for developing climate adaptation strategies for vulnerable species.\n\nConservation Planning\n\nSDMs help identify areas for potential reintroductions or translocations based on environmental suitability.\nThey can guide protected area design by highlighting environmentally suitable areas that may not currently be protected.\nModels can identify potential corridors between suitable habitat patches to maintain connectivity.\n\nModel Limitations and Considerations\n\nThis simple model assumes that environmental variables affect species occurrence independently.\nIn reality, species distributions are influenced by biotic interactions, dispersal limitations, and historical factors not captured here.\nMore sophisticated SDMs might include interaction terms, spatial autocorrelation, or mechanistic components.\nValidation with independent data is crucial before using SDMs for conservation decision-making.\n\n\n\n\nSDMs represent a powerful tool in the conservation biologist’s toolkit, allowing us to translate ecological knowledge into spatial predictions that can directly inform conservation actions and policy.\n\n\n\n\n\n\nBest Practices for Species Distribution Modeling\n\n\n\nWhen developing SDMs for conservation applications:\n\nStart with clear hypotheses: Define which environmental factors are likely to influence the species’ distribution based on ecological knowledge\nConsider sampling bias: Account for uneven sampling effort in presence data through spatial filtering or bias correction\nValidate thoroughly: Use independent data or cross-validation techniques to assess model performance and transferability\nIncorporate uncertainty: Present prediction intervals or ensemble model outputs to communicate uncertainty in predictions\nConsider scale: Match the resolution of environmental data to the species’ ecology and movement patterns\nInclude biotic interactions: When possible, incorporate variables representing key competitors, predators, or mutualists",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#population-trend-analysis",
    "href": "chapters/09-conservation.html#population-trend-analysis",
    "title": "9  Conservation Applications",
    "section": "9.4 Population Trend Analysis",
    "text": "9.4 Population Trend Analysis\nAnalyzing population trends is crucial for conservation planning and evaluating management effectiveness.\n\n9.4.1 Example: Linear Mixed Models for Population Trends\n\n\nCode\n# Simulate population monitoring data\nset.seed(456)\nn_sites &lt;- 10\nn_years &lt;- 15\n\n# Create site and year variables\nsite &lt;- rep(paste0(\"Site\", 1:n_sites), each = n_years)\nyear &lt;- rep(2008:(2008 + n_years - 1), times = n_sites)\n\n# Create random site effects and declining trend\nsite_effect &lt;- rep(rnorm(n_sites, 0, 0.5), each = n_years)\ntime_effect &lt;- -0.05 * (year - 2008)  # Declining trend\nnoise &lt;- rnorm(n_sites * n_years, 0, 0.2)\n\n# Calculate log population size\nlog_pop_size &lt;- 2 + site_effect + time_effect + noise\n\n# Convert to actual counts\npopulation &lt;- round(exp(log_pop_size))\n\n# Create a data frame\npop_data &lt;- data.frame(\n  site = factor(site),\n  year = year,\n  population = population\n)\n\n# Visualize the data\nlibrary(ggplot2)\nggplot(pop_data, aes(x = year, y = population, color = site, group = site)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Population Trends Across Multiple Sites\",\n       x = \"Year\",\n       y = \"Population Size\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a linear mixed model\nlibrary(lme4)\ntrend_model &lt;- lmer(log(population) ~ year + (1|site), data = pop_data)\n\n# Display model summary\nsummary(trend_model)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(population) ~ year + (1 | site)\n   Data: pop_data\n\nREML criterion at convergence: 2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.64610 -0.69998 -0.02039  0.62219  1.92852 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n site     (Intercept) 0.17634  0.4199  \n Residual             0.04223  0.2055  \nNumber of obs: 150, groups:  site, 10\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept) 100.003639   7.826672   12.78\nyear         -0.048800   0.003884  -12.57\n\nCorrelation of Fixed Effects:\n     (Intr)\nyear -1.000\n\n\nCode\n# Calculate overall trend\ntrend_coef &lt;- fixef(trend_model)[\"year\"]\nannual_change &lt;- (exp(trend_coef) - 1) * 100\ncat(\"Annual population change:\", round(annual_change, 2), \"%\\n\")\n\n\nAnnual population change: -4.76 %\n\n\nCode\n# Predict values for visualization\npop_data$predicted &lt;- exp(predict(trend_model))\n\n# Plot observed vs. predicted values\nggplot(pop_data, aes(x = year)) +\n  geom_point(aes(y = population, color = site), alpha = 0.5) +\n  geom_line(aes(y = predicted, group = site), color = \"black\") +\n  labs(title = \"Observed and Predicted Population Sizes\",\n       x = \"Year\",\n       y = \"Population Size\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to analyze population trends across multiple monitoring sites using linear mixed models, a powerful approach for conservation monitoring data. Key components include:\n\nData Simulation\n\nWe simulate 15 years of population monitoring data across 10 different sites.\nWe incorporate three key components that reflect real-world population dynamics:\n\nSite-specific random effects (some sites naturally support larger populations)\nA systematic declining trend over time (the conservation concern)\nRandom noise (natural population fluctuations)\n\nWe use a log-normal model for population size, which is appropriate for count data that can’t be negative.\n\nData Visualization\n\nThe initial visualization plots raw population counts over time for each site.\nEach colored line represents a different monitoring site.\nThis exploratory plot helps identify overall patterns and site-specific variations.\nThe visualization reveals both the declining trend and the between-site variability.\n\nModel Fitting\n\nWe use a linear mixed model (LMM) with the lme4 package to analyze the population trend.\nWe log-transform the population counts to stabilize variance and make the model more appropriate for count data.\nThe fixed effect (year) captures the overall temporal trend shared across all sites.\nThe random effect (1|site) accounts for site-specific variation in baseline population sizes.\nThis approach is more powerful than analyzing each site separately, as it “borrows strength” across sites.\n\nTrend Quantification\n\nWe extract the year coefficient from the model, which represents the average annual change in log population size.\nWe convert this to a percentage change using the formula (exp(coef) - 1) * 100.\nThis transformation makes the result more interpretable for conservation managers and policymakers.\n\nModel Visualization\n\nWe generate predicted values from the model for each site and year.\nWe plot both observed data (colored points) and model predictions (black lines).\nThis helps assess model fit and visualize the estimated trend while accounting for site-specific differences.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe population trend analysis provides several important insights for conservation management:\n\nQuantifying Population Decline\n\nThe model estimates an annual population decline of approximately 5%, which is substantial and concerning from a conservation perspective.\nThe statistical significance of this trend (as shown in the model summary) helps determine whether conservation action is warranted.\nThe confidence interval around this estimate (not explicitly calculated here) would indicate the precision of our trend estimate.\n\nSite-Specific Variation\n\nThe random effects reveal which sites have consistently higher or lower populations than average.\nThis information can help identify potential refuges (sites with larger populations) or areas of concern (sites with smaller populations).\nUnderstanding site-specific variation is crucial for prioritizing conservation efforts and resources.\n\nConservation Decision Support\n\nThis analysis provides quantitative evidence to support conservation decisions:\n\nIs the population declining at a rate that requires intervention?\nWhich sites should be prioritized for management actions?\nHow much would the population need to increase annually to reach recovery targets?\n\n\nMonitoring Program Design\n\nThe approach demonstrates the value of multi-site monitoring programs.\nThe mixed model framework allows detection of trends that might be obscured by site-specific variation.\nThis can inform the design of future monitoring programs, including the number of sites needed and monitoring frequency.\n\nLimitations and Considerations\n\nThis simple model assumes a constant rate of decline across years.\nMore complex models might include non-linear trends, temporal autocorrelation, or environmental covariates.\nFor real conservation applications, additional diagnostics would be needed to validate model assumptions.\n\n\n\n\nThis mixed modeling approach represents a powerful tool for conservation biologists, allowing them to rigorously assess population trends while accounting for the complex, hierarchical nature of ecological monitoring data.\n\n\n\n\n\n\nBest Practices for Population Trend Analysis\n\n\n\nWhen analyzing population trends for conservation decision-making:\n\nUse appropriate temporal scale: Consider the species’ generation time and life history when determining monitoring frequency\nAccount for detection probability: Imperfect detection can bias trend estimates; use occupancy or N-mixture models when detection is &lt; 100%\nConsider environmental covariates: Including climate or habitat variables can help explain population fluctuations and distinguish natural variation from concerning declines\nReport effect sizes, not just p-values: A statistically significant decline might not be biologically significant; focus on magnitude and uncertainty\nEvaluate multiple metrics: Analyze abundance, occupancy, and demographic rates together for a more complete picture of population health\nPlan for statistical power: Design monitoring programs with enough sites and years to detect trends of conservation concern",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#habitat-fragmentation-analysis",
    "href": "chapters/09-conservation.html#habitat-fragmentation-analysis",
    "title": "9  Conservation Applications",
    "section": "9.5 Habitat Fragmentation Analysis",
    "text": "9.5 Habitat Fragmentation Analysis\nHabitat fragmentation is a major threat to biodiversity. Landscape metrics help quantify fragmentation patterns.\n\n9.5.1 Example: Calculating Landscape Metrics\n\n\nCode\n# Load required packages\nlibrary(terra)\nlibrary(ggplot2)\n\n# Create a simple landscape raster\nr &lt;- rast(ncol=30, nrow=30)\nvalues(r) &lt;- sample(c(1, 2, 3, 4), ncell(r), replace=TRUE,\n                   prob=c(0.4, 0.3, 0.2, 0.1))\nnames(r) &lt;- \"landcover\"\n\n# Plot the landscape\nplot(r, main=\"Simulated Landscape\", col=c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"))\n\n\n\n\n\n\n\n\n\nCode\n# Create a data frame with class-level metrics manually\nclass_metrics &lt;- data.frame(\n  class = c(1, 2, 3, 4),\n  class_name = c(\"Forest\", \"Agriculture\", \"Water\", \"Urban\"),\n  percentage = c(40, 30, 20, 10),\n  edge_density = c(0.12, 0.09, 0.06, 0.03),\n  num_patches = c(15, 12, 8, 5)\n)\n\n# Visualize class-level metrics\nggplot(class_metrics, aes(x = factor(class), y = percentage, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Percentage of Landscape by Class\",\n       x = \"Land Cover Class\",\n       y = \"Percentage (%)\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# Visualize number of patches\nggplot(class_metrics, aes(x = factor(class), y = num_patches, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Patches by Class\",\n       x = \"Land Cover Class\",\n       y = \"Number of Patches\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# Visualize edge density\nggplot(class_metrics, aes(x = factor(class), y = edge_density, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Edge Density by Class\",\n       x = \"Land Cover Class\",\n       y = \"Edge Density\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates how to create and analyze a simulated landscape to study habitat fragmentation patterns. Key components include:\n\nLandscape Simulation\n\nWe use the terra package to create a 30×30 cell raster representing a landscape.\nWe randomly assign each cell to one of four land cover classes: Forest (1), Agriculture (2), Water (3), and Urban (4).\nThe probability distribution (40%, 30%, 20%, 10%) creates a landscape dominated by forest and agricultural land.\nThis simulated landscape provides a controlled environment to demonstrate fragmentation analysis techniques.\n\nLandscape Visualization\n\nWe visualize the landscape using appropriate colors for each land cover type (green for forest, yellow for agriculture, blue for water, grey for urban).\nThis spatial representation helps identify patterns of fragmentation visually before quantitative analysis.\nThe mosaic pattern reveals how different land cover types are distributed and potentially fragmented across the landscape.\n\nLandscape Metrics Calculation\n\nIn a real analysis, metrics would be calculated directly from the raster using packages like landscapemetrics.\nFor this example, we manually create a data frame with three key metrics for each land cover class:\n\nPercentage: The proportion of the landscape occupied by each class\nEdge Density: The amount of edge relative to the landscape area (higher values indicate more fragmentation)\nNumber of Patches: Count of discrete patches for each land cover type (more patches suggest higher fragmentation)\n\n\nMetrics Visualization\n\nWe create three bar charts to visualize each landscape metric by land cover class.\nConsistent color coding across all visualizations helps maintain visual connection to the landscape map.\nEach chart focuses on a different aspect of landscape composition and configuration.\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe landscape fragmentation analysis reveals several key insights for conservation planning:\n\nLandscape Composition\n\nThe percentage chart shows that forest covers 40% of the landscape, followed by agriculture (30%), water (20%), and urban areas (10%).\nThis composition analysis helps establish conservation priorities based on habitat availability.\nIn real-world applications, comparing this to historical land cover would reveal habitat loss trends.\n\nFragmentation Assessment\n\nThe number of patches metric reveals that forest (15 patches) is more fragmented than other land cover types.\nDespite having the highest coverage, forest fragmentation may compromise its ecological value for species requiring large, continuous habitat.\nUrban areas have the fewest patches (5), suggesting they form more concentrated developments.\n\nEdge Effects\n\nEdge density is highest for forest (0.12), indicating extensive borders with other land cover types.\nHigh edge density creates “edge effects” that can negatively impact forest-interior species through:\n\nAltered microclimate conditions (light, temperature, humidity)\nIncreased predation and nest parasitism\nInvasive species introduction\n\nWater has relatively low edge density despite moderate patch numbers, suggesting more compact water bodies.\n\nConservation Implications\n\nThese metrics help identify specific conservation needs:\n\nHabitat Connectivity: Forest patches might need corridors to reconnect fragmented habitats.\nBuffer Zones: High edge density suggests the need for buffer zones around sensitive habitats.\nRestoration Priorities: Strategically restoring habitat in areas that would reconnect patches.\nDevelopment Planning: Guiding future development to minimize additional fragmentation.\n\n\nLimitations and Considerations\n\nThis simplified example uses a coarse resolution and random distribution.\nReal landscapes have spatial autocorrelation and are influenced by topography, hydrology, and human infrastructure.\nAdditional metrics like connectivity indices, core area, and shape complexity would provide more comprehensive fragmentation assessment.\nScale dependency is important - fragmentation patterns may differ at different spatial resolutions.\n\n\n\n\nLandscape metrics translate complex spatial patterns into quantifiable measures that conservation biologists can use to assess habitat quality, prioritize conservation efforts, and monitor landscape change over time. These approaches are particularly valuable for addressing habitat fragmentation, one of the primary drivers of biodiversity loss globally.\n\n\n\n\n\n\nBest Practices for Habitat Fragmentation Analysis\n\n\n\nWhen analyzing landscape patterns for conservation planning:\n\nConsider multiple scales: Analyze fragmentation at different spatial scales as species respond to landscape structure at different scales\nUse ecologically relevant metrics: Select metrics that relate to the ecological processes and species of interest\nIncorporate temporal dynamics: Monitor landscape changes over time to detect fragmentation trends and evaluate restoration success\nLink to biodiversity data: Correlate landscape metrics with species occurrence or abundance to validate their ecological relevance\nAccount for matrix quality: Consider the permeability of the landscape matrix between habitat patches, not just patch characteristics\nCombine with connectivity analysis: Supplement fragmentation metrics with explicit connectivity models to identify critical corridors",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#protected-area-effectiveness",
    "href": "chapters/09-conservation.html#protected-area-effectiveness",
    "title": "9  Conservation Applications",
    "section": "9.6 Protected Area Effectiveness",
    "text": "9.6 Protected Area Effectiveness\nEvaluating the effectiveness of protected areas is essential for conservation planning and management.\n\n9.6.1 Example: Before-After-Control-Impact (BACI) Analysis\n\n\nCode\n# Simulate protected area effectiveness data\nset.seed(789)\nn_sites &lt;- 20\nn_years &lt;- 10\n\n# Create site, protection status, and year variables\nsite &lt;- rep(paste0(\"Site\", 1:n_sites), each = n_years)\nprotected &lt;- rep(rep(c(\"Protected\", \"Unprotected\"), each = n_sites/2), each = n_years)\nyear &lt;- rep(2013:(2013 + n_years - 1), times = n_sites)\nperiod &lt;- ifelse(year &lt; 2018, \"Before\", \"After\")  # Protection started in 2018\n\n# Create random site effects and impact of protection\nsite_effect &lt;- rep(rnorm(n_sites, 0, 0.5), each = n_years)\nprotection_effect &lt;- ifelse(protected == \"Protected\" & period == \"After\", 0.3, 0)\ntime_effect &lt;- -0.05 * (year - 2013)  # General declining trend\nnoise &lt;- rnorm(n_sites * n_years, 0, 0.2)\n\n# Calculate biodiversity index\nbiodiversity &lt;- 5 + site_effect + time_effect + protection_effect + noise\n\n# Create a data frame\npa_data &lt;- data.frame(\n  site = factor(site),\n  protected = factor(protected),\n  year = year,\n  period = factor(period),\n  biodiversity = biodiversity\n)\n\n# Visualize the data\nggplot(pa_data, aes(x = year, y = biodiversity, color = protected, group = interaction(site, protected))) +\n  geom_line(alpha = 0.3) +\n  stat_summary(aes(group = protected), fun = mean, geom = \"line\", size = 1.5) +\n  geom_vline(xintercept = 2018, linetype = \"dashed\") +\n  labs(title = \"Biodiversity Trends in Protected and Unprotected Sites\",\n       subtitle = \"Vertical line indicates when protection was implemented\",\n       x = \"Year\",\n       y = \"Biodiversity Index\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a BACI model\nbaci_model &lt;- lm(biodiversity ~ protected * period, data = pa_data)\n\n# Display model summary\nsummary(baci_model)\n\n\n\nCall:\nlm(formula = biodiversity ~ protected * period, data = pa_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18762 -0.25169  0.00786  0.29460  0.93568 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        4.79029    0.05943  80.604  &lt; 2e-16 ***\nprotectedUnprotected              -0.29698    0.08405  -3.534 0.000511 ***\nperiodBefore                      -0.08027    0.08405  -0.955 0.340742    \nprotectedUnprotected:periodBefore  0.33219    0.11886   2.795 0.005709 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4202 on 196 degrees of freedom\nMultiple R-squared:  0.06998,   Adjusted R-squared:  0.05574 \nF-statistic: 4.916 on 3 and 196 DF,  p-value: 0.002578\n\n\nCode\n# Visualize the interaction effect\npa_summary &lt;- aggregate(biodiversity ~ protected + period, data = pa_data, FUN = mean)\n\nggplot(pa_summary, aes(x = period, y = biodiversity, color = protected, group = protected)) +\n  geom_point(size = 3) +\n  geom_line() +\n  labs(title = \"BACI Design: Interaction between Protection Status and Time Period\",\n       x = \"Period\",\n       y = \"Mean Biodiversity Index\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates a Before-After-Control-Impact (BACI) analysis for evaluating protected area effectiveness:\n\nData Simulation:\n\nCreates biodiversity monitoring data for 20 sites over 10 years\nHalf the sites are protected starting in 2018\nIncludes site-specific random effects, a protection effect, and natural variation\n\nBACI Design Components:\n\nBefore-After: Time periods before and after protection implementation\nControl-Impact: Comparison between protected and unprotected sites\nInteraction: The key element that tests whether protection made a difference\n\nVisualization Elements:\n\nIndividual site trajectories shown with thin lines\nMean trends highlighted with thicker lines\nVertical line marking when protection was implemented\nInteraction plot showing the mean values for each combination\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe BACI analysis reveals crucial information about protected area effectiveness:\n\nProtection Impact:\n\nThe interaction term (protected:periodAfter) shows the true effect of protection\nPositive coefficient indicates protection is benefiting biodiversity\nStatistical significance of this term determines whether protection is working\n\nCounterfactual Analysis:\n\nUnprotected sites serve as the counterfactual (what would have happened without protection)\nOverall declining trend in both site types indicates broader environmental pressures\nDifference in slopes represents the conservation value added by protection\n\nManagement Implications:\n\nQuantifies the return on investment for conservation funding\nHelps determine whether current protection strategies are sufficient\nProvides evidence for maintaining or expanding protection efforts\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Designing Effective BACI Studies\n\n\n\nWhen evaluating conservation interventions:\n\nStudy Design:\n\nSelect control sites that match impact sites in key environmental variables\nEnsure sufficient monitoring before intervention implementation\nInclude multiple control and impact sites to account for site-specific variation\nConsider spatial autocorrelation in site selection\n\nAnalysis Approach:\n\nUse linear mixed models for nested or repeated measures designs\nInclude relevant covariates that might affect outcomes\nConsider temporal autocorrelation in time series data\nTest for pre-existing differences between control and impact sites\n\nInterpretation:\n\nFocus on the interaction term (difference in differences)\nReport effect sizes and confidence intervals, not just p-values\nConsider time lags in conservation responses\nDiscuss both statistical and practical significance",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#threat-assessment-and-prioritization",
    "href": "chapters/09-conservation.html#threat-assessment-and-prioritization",
    "title": "9  Conservation Applications",
    "section": "9.7 Threat Assessment and Prioritization",
    "text": "9.7 Threat Assessment and Prioritization\nConservation resources are limited, so prioritizing threats and actions is essential.\n\n9.7.1 Example: Multi-Criteria Decision Analysis\n\n\nCode\n# Create a threat assessment dataset\nthreats &lt;- c(\"Habitat Loss\", \"Invasive Species\", \"Climate Change\", \"Pollution\", \"Overexploitation\")\nseverity &lt;- c(0.9, 0.7, 0.8, 0.6, 0.7)\nscope &lt;- c(0.8, 0.6, 0.9, 0.5, 0.6)\nirreversibility &lt;- c(0.9, 0.7, 0.9, 0.4, 0.5)\n\n# Create a data frame\nthreat_data &lt;- data.frame(\n  threat = threats,\n  severity = severity,\n  scope = scope,\n  irreversibility = irreversibility\n)\n\n# Calculate overall threat magnitude\nthreat_data$magnitude &lt;- with(threat_data, severity * scope * irreversibility)\n\n# Sort by magnitude\nthreat_data &lt;- threat_data[order(threat_data$magnitude, decreasing = TRUE), ]\n\n# Visualize the threat assessment\nggplot(threat_data, aes(x = reorder(threat, magnitude), y = magnitude)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Threat Prioritization Based on Magnitude\",\n       x = \"Threat\",\n       y = \"Magnitude (Severity × Scope × Irreversibility)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Visualize the components\nthreat_data_long &lt;- reshape2::melt(threat_data[, c(\"threat\", \"severity\", \"scope\", \"irreversibility\")],\n                                 id.vars = \"threat\")\n\nggplot(threat_data_long, aes(x = reorder(threat, -value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Components of Threat Assessment\",\n       x = \"Threat\",\n       y = \"Score\",\n       fill = \"Component\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates multi-criteria decision analysis for threat prioritization:\n\nData Structure:\n\nCreates an example dataset of five common conservation threats\nEvaluates each threat using three criteria:\n\nSeverity: The intensity of the threat’s impact\nScope: The proportion of the target affected\nIrreversibility: How difficult it is to reverse the damage\n\n\nAnalysis Process:\n\nCalculates an overall magnitude score by multiplying the three criteria\nRanks threats based on this composite score\nCreates visualizations to compare both overall rankings and component scores\n\nVisualization Techniques:\n\nBar chart of overall threat magnitude\nGrouped bar chart showing the individual criteria for each threat\nConsistent ordering of threats by magnitude\nClear labeling and color-coding\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe threat prioritization analysis reveals important insights for conservation planning:\n\nThreat Ranking:\n\nHabitat Loss emerges as the highest priority threat\nClimate Change ranks second despite its lower severity\nOverexploitation has the lowest composite score\n\nComponent Analysis:\n\nHabitat Loss scores consistently high across all three criteria\nClimate Change has high scope and irreversibility but slightly lower severity\nPollution shows low irreversibility despite moderate severity and scope\n\nConservation Implications:\n\nResources should be allocated according to threat magnitude\nDifferent threats require different intervention strategies:\n\nFor reversible threats: direct mitigation\nFor irreversible threats: prevention and adaptation\n\nComprehensive strategies needed for threats scoring high in all dimensions\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Effective Threat Assessment\n\n\n\nWhen prioritizing conservation threats:\n\nAssessment Process:\n\nInclude diverse stakeholders and experts in evaluations\nDefine criteria explicitly with clear scoring guidelines\nConsider both direct and indirect threats\nDocument uncertainty in threat evaluations\n\nAnalysis Considerations:\n\nTest sensitivity to different scoring methods and weights\nConsider interactions between threats\nEvaluate threats at appropriate spatial and temporal scales\nInclude emerging and potential future threats\n\nApplication to Decision-Making:\n\nLink threat assessment directly to conservation actions\nConsider feasibility and cost-effectiveness of addressing each threat\nRe-evaluate periodically as conditions change\nCommunicate results clearly to decision-makers",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#conservation-planning",
    "href": "chapters/09-conservation.html#conservation-planning",
    "title": "9  Conservation Applications",
    "section": "9.8 Conservation Planning",
    "text": "9.8 Conservation Planning\nSystematic conservation planning helps identify priority areas for conservation.\n\n9.8.1 Example: Complementarity Analysis\n\n\nCode\n# Create a species-by-site matrix\nset.seed(101)\nn_sites &lt;- 10\nn_species &lt;- 15\nspecies_names &lt;- paste0(\"Species\", 1:n_species)\nsite_names &lt;- paste0(\"Site\", 1:n_sites)\n\n# Generate presence/absence data\npresence_prob &lt;- matrix(runif(n_sites * n_species, 0, 1), nrow = n_sites, ncol = n_species)\npresence &lt;- ifelse(presence_prob &gt; 0.7, 0, 1)  # 30% chance of presence\nrownames(presence) &lt;- site_names\ncolnames(presence) &lt;- species_names\n\n# Calculate species richness per site\nrichness &lt;- rowSums(presence)\n\n# Calculate site complementarity\ncomplementarity &lt;- function(selected, candidates, presence_matrix) {\n  if (length(selected) == 0) {\n    # If no sites selected yet, return site richness\n    return(rowSums(presence_matrix[candidates, , drop = FALSE]))\n  } else {\n    # Calculate new species added by each candidate site\n    species_in_selected &lt;- colSums(presence_matrix[selected, , drop = FALSE]) &gt; 0\n    new_species &lt;- function(site) {\n      sum(presence_matrix[site, ] & !species_in_selected)\n    }\n    return(sapply(candidates, new_species))\n  }\n}\n\n# Greedy algorithm for site selection\nselect_sites &lt;- function(presence_matrix, n_to_select) {\n  n_sites &lt;- nrow(presence_matrix)\n  available_sites &lt;- 1:n_sites\n  selected_sites &lt;- integer(0)\n\n  for (i in 1:n_to_select) {\n    if (length(available_sites) == 0) break\n\n    # Calculate complementarity scores\n    scores &lt;- complementarity(selected_sites, available_sites, presence_matrix)\n\n    # Select site with highest score\n    best &lt;- available_sites[which.max(scores)]\n    selected_sites &lt;- c(selected_sites, best)\n    available_sites &lt;- setdiff(available_sites, best)\n  }\n\n  return(selected_sites)\n}\n\n# Select 3 priority sites\npriority_sites &lt;- select_sites(presence, 3)\ncat(\"Priority sites:\", site_names[priority_sites], \"\\n\")\n\n\nPriority sites: Site7 Site8 Site1 \n\n\nCode\n# Calculate species coverage\nspecies_covered &lt;- colSums(presence[priority_sites, , drop = FALSE]) &gt; 0\ncat(\"Species covered:\", sum(species_covered), \"out of\", n_species,\n    \"(\", round(100 * sum(species_covered) / n_species, 1), \"%)\\n\")\n\n\nSpecies covered: 15 out of 15 ( 100 %)\n\n\nCode\n# Visualize the species-site matrix\nlibrary(pheatmap)\npheatmap(presence,\n        cluster_rows = FALSE,\n        cluster_cols = FALSE,\n        main = \"Species Presence by Site\",\n        color = c(\"white\", \"steelblue\"),\n        labels_row = site_names,\n        labels_col = species_names,\n        display_numbers = TRUE,\n        number_color = \"black\",\n        fontsize = 10,\n        fontsize_number = 8)\n\n\n\n\n\n\n\n\n\nCode\n# Highlight priority sites\npriority_data &lt;- data.frame(\n  Priority = factor(ifelse(1:n_sites %in% priority_sites, \"Selected\", \"Not Selected\"))\n)\nrownames(priority_data) &lt;- site_names\n\npheatmap(presence,\n        cluster_rows = FALSE,\n        cluster_cols = FALSE,\n        main = \"Priority Sites for Conservation\",\n        color = c(\"white\", \"steelblue\"),\n        labels_row = site_names,\n        labels_col = species_names,\n        display_numbers = TRUE,\n        number_color = \"black\",\n        annotation_row = priority_data,\n        fontsize = 10,\n        fontsize_number = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates systematic conservation planning using complementarity analysis:\n\nData Preparation:\n\nCreates a simulated presence/absence matrix of 15 species across 10 sites\nEach cell represents whether a species occurs at a site (1) or not (0)\nThe matrix represents the kind of data collected during biodiversity surveys\n\nComplementarity Algorithm:\n\nImplements a greedy algorithm for site selection\nFirst selects the site with highest species richness\nSubsequent selections maximize additional species not already protected\nThis approach efficiently captures maximum biodiversity with minimum sites\n\nVisualization Approach:\n\nUses heatmaps to display the species-site matrix\nColors indicate presence (blue) or absence (white)\nHighlights selected priority sites with annotation\nDisplays numerical values within cells for clarity\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe complementarity analysis provides key insights for conservation prioritization:\n\nEfficiency of Site Selection:\n\nThe algorithm selected just 3 sites that protect most (typically &gt;70%) of the species\nThis demonstrates the efficiency of complementarity-based selection\nTraditional approaches might require more sites to achieve the same coverage\n\nSite Prioritization:\n\nThe selected sites represent the most irreplaceable areas for biodiversity\nThese should be highest priorities for protection or management\nThe visualization clearly shows which species are protected in each site\n\nConservation Planning Applications:\n\nHelps make evidence-based decisions for protected area designation\nMaximizes return on investment when conservation resources are limited\nEnsures representation of different species rather than just protecting species-rich areas\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Effective Conservation Planning\n\n\n\nWhen applying complementarity analysis:\n\nData Considerations:\n\nUse the most comprehensive species data available\nConsider taxonomic, functional, and genetic diversity\nAccount for data quality issues and sampling bias\nInclude threatened species with higher weighting if appropriate\n\nAlgorithm Selection:\n\nSimple greedy algorithms work well for small problems\nConsider optimization algorithms (e.g., simulated annealing) for complex scenarios\nInclude connectivity and spatial considerations when possible\nSet meaningful conservation targets (e.g., protect 30% of each species’ range)\n\nImplementation Strategy:\n\nUse results to inform both formal protection and other conservation measures\nConsider practical constraints like land availability and cost\nEngage stakeholders in the planning process\nUpdate analyses as new data becomes available",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#climate-change-vulnerability-assessment",
    "href": "chapters/09-conservation.html#climate-change-vulnerability-assessment",
    "title": "9  Conservation Applications",
    "section": "9.9 Climate Change Vulnerability Assessment",
    "text": "9.9 Climate Change Vulnerability Assessment\nClimate change poses significant threats to biodiversity. Vulnerability assessments help identify at-risk species and ecosystems.\n\n9.9.1 Example: Trait-Based Vulnerability Analysis\n\n\nCode\n# Create a species trait dataset\nspecies &lt;- paste0(\"Species\", 1:12)\ndispersal_ability &lt;- c(1, 3, 2, 1, 3, 2, 1, 2, 3, 1, 2, 3)  # 1=low, 2=medium, 3=high\nthermal_tolerance &lt;- c(1, 2, 3, 1, 2, 3, 2, 3, 1, 3, 1, 2)  # 1=low, 2=medium, 3=high\nhabitat_specificity &lt;- c(3, 2, 1, 3, 1, 2, 3, 2, 1, 2, 3, 1)  # 1=low, 2=medium, 3=high\npopulation_size &lt;- c(1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 2, 3)  # 1=small, 2=medium, 3=large\n\n# Create a data frame\nvulnerability_data &lt;- data.frame(\n  species = species,\n  dispersal_ability = dispersal_ability,\n  thermal_tolerance = thermal_tolerance,\n  habitat_specificity = habitat_specificity,\n  population_size = population_size\n)\n\n# Calculate vulnerability scores (higher = more vulnerable)\nvulnerability_data$sensitivity &lt;- 4 - thermal_tolerance\nvulnerability_data$adaptive_capacity &lt;- 4 - (dispersal_ability + population_size) / 2\nvulnerability_data$exposure &lt;- habitat_specificity\nvulnerability_data$vulnerability &lt;- with(vulnerability_data,\n                                       (sensitivity + adaptive_capacity + exposure) / 3)\n\n# Sort by vulnerability\nvulnerability_data &lt;- vulnerability_data[order(vulnerability_data$vulnerability, decreasing = TRUE), ]\n\n# Visualize vulnerability scores\nggplot(vulnerability_data, aes(x = reorder(species, -vulnerability), y = vulnerability)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Climate Change Vulnerability by Species\",\n       x = \"Species\",\n       y = \"Vulnerability Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Visualize components\nvulnerability_components &lt;- vulnerability_data[, c(\"species\", \"sensitivity\", \"adaptive_capacity\", \"exposure\")]\nvulnerability_long &lt;- reshape2::melt(vulnerability_components, id.vars = \"species\")\n\nggplot(vulnerability_long, aes(x = reorder(species, -value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Components of Climate Change Vulnerability\",\n       x = \"Species\",\n       y = \"Score\",\n       fill = \"Component\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates a trait-based climate change vulnerability assessment:\n\nAssessment Framework:\n\nCreates a simulated dataset of 12 species with varying traits\nEvaluates species on four key traits that affect climate vulnerability:\n\nDispersal ability: Capacity to move to new suitable areas\nThermal tolerance: Ability to withstand temperature changes\nHabitat specificity: Degree of specialization to particular habitats\nPopulation size: Indicates demographic resilience\n\n\nVulnerability Calculation:\n\nTransforms trait scores into three vulnerability components:\n\nSensitivity: Physiological tolerance to climate changes\nAdaptive capacity: Ability to respond through dispersal or adaptation\nExposure: Likelihood of experiencing significant change\n\nCombines these components into an overall vulnerability score\n\nVisualization Approach:\n\nCreates a ranked bar chart of overall vulnerability\nProvides a component-wise breakdown to show vulnerability drivers\nUses consistent ordering and color-coding for clarity\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe vulnerability assessment reveals important patterns for climate adaptation planning:\n\nSpecies Prioritization:\n\nSome species are clearly more vulnerable than others\nThe most vulnerable species have high scores across multiple components\nThese species should be prioritized for conservation action\n\nVulnerability Drivers:\n\nDifferent species are vulnerable for different reasons:\n\nSome species have low adaptive capacity but moderate sensitivity\nOthers have high sensitivity but better adaptive capacity\nExposure varies across species based on habitat specificity\n\nThis indicates the need for tailored conservation strategies\n\nConservation Implications:\n\nHighly vulnerable species may require:\n\nAssisted migration to suitable habitat\nEx-situ conservation (e.g., captive breeding)\nSpecial protection of climate refugia\n\nSpecies with high sensitivity but good adaptive capacity may benefit from connectivity conservation\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Climate Vulnerability Assessments\n\n\n\nWhen conducting climate vulnerability assessments:\n\nTrait Selection:\n\nChoose traits with demonstrated links to climate vulnerability\nInclude both intrinsic (biological) and extrinsic (exposure) factors\nConsider different climate change aspects (temperature, precipitation, extreme events)\nUse traits that can be measured or estimated with available data\n\nMethodology Considerations:\n\nWeight components based on their relative importance for the taxa\nInclude uncertainty measures for each trait assessment\nValidate results against observed responses when possible\nConsider different climate scenarios to evaluate range of outcomes\n\nApplication to Conservation:\n\nDevelop vulnerability-specific adaptation strategies\nIdentify and protect climate refugia for sensitive species\nDesign conservation corridors oriented along climate gradients\nMonitor highly vulnerable species for early detection of impacts",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#community-based-conservation-monitoring",
    "href": "chapters/09-conservation.html#community-based-conservation-monitoring",
    "title": "9  Conservation Applications",
    "section": "9.10 Community-Based Conservation Monitoring",
    "text": "9.10 Community-Based Conservation Monitoring\nInvolving local communities in conservation monitoring can improve data collection and conservation outcomes.\n\n9.10.1 Example: Analyzing Community Monitoring Data\n\n\nCode\n# Simulate community monitoring data\nset.seed(202)\nn_villages &lt;- 5\nn_months &lt;- 24\n\n# Create variables\nvillage &lt;- rep(paste0(\"Village\", 1:n_villages), each = n_months)\nmonth &lt;- rep(1:n_months, times = n_villages)\nyear &lt;- rep(rep(c(1, 2), each = 12), times = n_villages)\n\n# Generate poaching incidents with seasonal pattern and declining trend\nseason &lt;- sin(month * pi / 6) + 1  # Seasonal pattern\ntrend &lt;- -0.03 * (month - 1)  # Declining trend\nvillage_effect &lt;- rep(rnorm(n_villages, 0, 0.5), each = n_months)\nlambda &lt;- exp(1 + 0.5 * season + trend + village_effect)\npoaching &lt;- rpois(n_villages * n_months, lambda)\n\n# Create a data frame\nmonitoring_data &lt;- data.frame(\n  village = factor(village),\n  month = month,\n  year = factor(year),\n  poaching = poaching\n)\n\n# Visualize the data\nggplot(monitoring_data, aes(x = month, y = poaching, color = village, group = village)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~year, scales = \"free_x\", labeller = labeller(year = c(\"1\" = \"Year 1\", \"2\" = \"Year 2\"))) +\n  labs(title = \"Poaching Incidents Reported by Community Monitors\",\n       x = \"Month\",\n       y = \"Number of Incidents\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Analyze trends\nlibrary(MASS)\ntrend_model &lt;- glm.nb(poaching ~ month + village, data = monitoring_data)\nsummary(trend_model)\n\n\n\nCall:\nglm.nb(formula = poaching ~ month + village, data = monitoring_data, \n    init.theta = 21.97524464, link = log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      1.229650   0.181336   6.781 1.19e-11 ***\nmonth           -0.057015   0.008742  -6.522 6.94e-11 ***\nvillageVillage2  0.545243   0.201645   2.704 0.006852 ** \nvillageVillage3  0.558968   0.201193   2.778 0.005465 ** \nvillageVillage4  0.270966   0.211843   1.279 0.200866    \nvillageVillage5  0.716424   0.196360   3.649 0.000264 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(21.9752) family taken to be 1)\n\n    Null deviance: 199.13  on 119  degrees of freedom\nResidual deviance: 136.01  on 114  degrees of freedom\nAIC: 468.75\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  22.0 \n          Std. Err.:  23.9 \n\n 2 x log-likelihood:  -454.752 \n\n\nCode\n# Calculate overall trend\ntrend_coef &lt;- coef(trend_model)[\"month\"]\nmonthly_change &lt;- (exp(trend_coef) - 1) * 100\ncat(\"Monthly change in poaching incidents:\", round(monthly_change, 2), \"%\\n\")\n\n\nMonthly change in poaching incidents: -5.54 %\n\n\nCode\n# Analyze seasonal patterns\nseason_model &lt;- glm.nb(poaching ~ sin(2 * pi * month / 12) + cos(2 * pi * month / 12) + village,\n                      data = monitoring_data)\nsummary(season_model)\n\n\n\nCall:\nglm.nb(formula = poaching ~ sin(2 * pi * month/12) + cos(2 * \n    pi * month/12) + village, data = monitoring_data, init.theta = 40.9900692, \n    link = log)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.48468    0.15815   3.065 0.002180 ** \nsin(2 * pi * month/12)  0.64312    0.08539   7.531 5.03e-14 ***\ncos(2 * pi * month/12)  0.08170    0.08155   1.002 0.316459    \nvillageVillage2         0.55310    0.19722   2.805 0.005039 ** \nvillageVillage3         0.57202    0.19658   2.910 0.003617 ** \nvillageVillage4         0.28057    0.20754   1.352 0.176412    \nvillageVillage5         0.72313    0.19186   3.769 0.000164 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(40.9901) family taken to be 1)\n\n    Null deviance: 209.78  on 119  degrees of freedom\nResidual deviance: 129.01  on 113  degrees of freedom\nAIC: 457.46\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  41.0 \n          Std. Err.:  70.8 \n\n 2 x log-likelihood:  -441.462 \n\n\nCode\n# Compare models\nanova(trend_model, season_model)\n\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: poaching\n                                                      Model    theta Resid. df\n1                                           month + village 21.97524       114\n2 sin(2 * pi * month/12) + cos(2 * pi * month/12) + village 40.99007       113\n     2 x log-lik.   Test    df LR stat.      Pr(Chi)\n1       -454.7522                                   \n2       -441.4616 1 vs 2     1  13.2906 0.0002667407\n\n\n\n\n\n\n\n\nCode Explanation\n\n\n\nThis code demonstrates analysis of community-based conservation monitoring data:\n\nData Simulation:\n\nCreates a simulated dataset of poaching incidents reported by 5 villages over 24 months\nIncorporates three key components of real monitoring data:\n\nSeasonal patterns (using sine functions)\nOverall trend (declining poaching incidents)\nVillage-specific variation (random effects)\n\n\nAnalysis Approach:\n\nUses a negative binomial model appropriate for count data\nTests both linear trend and seasonal components\nAccounts for different baseline rates across villages\nCompares models to determine the best explanation for patterns\n\nVisualization Techniques:\n\nTime series plots showing raw incident counts\nFaceting by year to compare patterns\nColor-coding by village to show site-specific variations\nClear marking of temporal patterns\n\n\n\n\n\n\n\n\n\n\nResults Interpretation\n\n\n\nThe community monitoring analysis reveals important patterns for conservation management:\n\nTrend Assessment:\n\nThe model indicates a declining trend in poaching incidents\nThe calculated monthly change quantifies this decline\nStatistical significance helps evaluate whether the trend is reliable\n\nSeasonal Patterns:\n\nThe seasonal model reveals cyclical patterns in poaching\nThese patterns may correlate with:\n\nWildlife migration or breeding seasons\nAgricultural cycles affecting human behavior\nSeasonal changes in patrol effectiveness\n\n\nVillage Differences:\n\nDifferent villages show varying baseline levels of poaching\nSome villages may have stronger declines than others\nThis spatial heterogeneity can inform targeted interventions\n\nConservation Implications:\n\nProvides evidence for the effectiveness of anti-poaching efforts\nHelps identify when and where to focus patrol resources\nDemonstrates the value of community participation in monitoring\n\n\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Community-Based Monitoring\n\n\n\nWhen implementing community-based conservation monitoring:\n\nProgram Design:\n\nDevelop simple, standardized protocols that community members can follow\nProvide adequate training and ongoing support\nUse local knowledge to determine what and where to monitor\nCombine different data types (quantitative and qualitative)\n\nData Analysis:\n\nAccount for detection bias in volunteer-collected data\nIncorporate uncertainty in both data collection and analysis\nValidate with professional monitoring when possible\nConsider both spatial and temporal patterns\n\nProgram Sustainability:\n\nProvide tangible benefits to participating communities\nCreate feedback loops so communities see the impact of their data\nBuild local capacity for data analysis and interpretation\nDevelop long-term funding strategies and institutional support",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#summary",
    "href": "chapters/09-conservation.html#summary",
    "title": "9  Conservation Applications",
    "section": "9.11 Summary",
    "text": "9.11 Summary\nIn this chapter, we’ve explored how data analysis techniques can be applied to conservation challenges:\n\nSpecies distribution modeling to predict habitat suitability\nPopulation trend analysis to monitor species status\nHabitat fragmentation analysis to assess landscape connectivity\nProtected area effectiveness evaluation using BACI designs\nThreat assessment and prioritization for conservation planning\nSystematic conservation planning using complementarity analysis\nClimate change vulnerability assessment based on species traits\nCommunity-based conservation monitoring to track threats\n\nThese applications demonstrate how the statistical methods covered throughout this book can help address real-world conservation problems, inform management decisions, and ultimately contribute to biodiversity conservation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#exercises",
    "href": "chapters/09-conservation.html#exercises",
    "title": "9  Conservation Applications",
    "section": "9.12 Exercises",
    "text": "9.12 Exercises\n\nImport a dataset on species occurrences and environmental variables, then build a simple species distribution model.\nAnalyze population monitoring data to detect trends and assess conservation status.\nCalculate basic landscape metrics for a land cover map to quantify habitat fragmentation.\nDesign and analyze a BACI study to evaluate the effectiveness of a conservation intervention.\nConduct a threat assessment for a species or ecosystem of your choice.\nUse complementarity analysis to identify priority sites for conservation.\nPerform a climate change vulnerability assessment for a group of species.\nAnalyze community monitoring data to detect trends in threats or biodiversity.\n\n\n\n\n\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species distribution models: Ecological explanation and prediction across space and time. Annual Review of Ecology, Evolution, and Systematics, 40, 677–697.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bolker, B. et al. (2009). Generalized linear mixed models: A\npractical guide. Trends in Ecology & Evolution.\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species\ndistribution models: Ecological explanation and prediction across space\nand time. Annual Review of Ecology, Evolution, and Systematics,\n40, 677–697.\n\n\nGotelli, N. J., & Ellison, A. M. (2004). Null model analysis of\nspecies co-occurrence patterns. Sinauer Associates.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import,\ntidy, transform, visualize, and model data. O’Reilly Media, Inc.\n\n\nZuur, A., Ieno, E. N., & Smith, G. M. (2007). Analyzing\necological data. Springer.\n\n\nZuur, A., Ieno, E. N., Walker, N., Saveliev, A. A., & Smith, G. M.\n(2009). Mixed effects models and extensions in ecology with r.\nSpringer Science & Business Media.",
    "crumbs": [
      "References"
    ]
  }
]