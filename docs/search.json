[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "",
    "text": "Preface\nWelcome to Data Analysis in Natural Sciences: An R-Based Approach, a comprehensive guide designed for students, professionals, and researchers across the natural sciences. This book provides practical methods for analyzing and visualizing data using R, with applications spanning forestry, agriculture, ecology, marine biology, environmental science, geology, atmospheric science, hydrology, and more.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "About the Author",
    "text": "About the Author\nThis book has been developed by Dr. Jimmy Moses (PhD) from the School of Forestry, Faculty of Natural Resources, Papua New Guinea University of Technology. With extensive experience in ecological research and data analysis, Dr. Moses has created this resource to support students and researchers in developing essential analytical skills for natural science disciplines.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Target Audience",
    "text": "Target Audience\nThis book is designed for:\n\nUndergraduate and postgraduate students in natural science disciplines\nResearchers seeking to enhance their data analysis capabilities\nTechnicians working in laboratories and field settings\nProfessionals in government agencies, NGOs, and private sector\nHobbyists with an interest in analyzing scientific data\n\nThe content is relevant to those working in:\n\nForestry and agroforestry\nAgriculture and agronomy\nEcology and conservation\nEnvironmental science\nGeography and GIS/remote sensing\nMarine biology and fisheries\nBotany and plant sciences\nEntomology and zoology\nEpidemiology and veterinary sciences\nGeology and earth sciences\nAtmospheric and climate sciences\nHydrology and water resources\nNatural resource management\nConservation biology",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nThis book will guide you through:\n\nThe fundamentals of data analysis with R\nData preparation and management techniques\nExploratory data analysis approaches\nStatistical hypothesis testing\nAdvanced visualization methods\nSpecialized analyses for environmental and scientific data\nReproducible research practices",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nThis book is designed to be both a learning resource and a reference guide. You can read it from start to finish to build your skills progressively, or use specific chapters as needed for particular tasks.\nCode examples are provided throughout, and you can run them directly in R or RStudio. Each chapter includes practical examples using real datasets from various natural science disciplines.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo get the most out of this book, you should have:\n\nBasic computer skills\nR and RStudio installed (instructions provided in Chapter 1)\nA basic understanding of statistics (helpful but not required)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to thank all those who contributed to the development of this book, including colleagues, students, and the open-source community that makes tools like R and RStudio possible.\nLet’s begin our journey into the world of data analysis for natural sciences!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "About This Book",
    "section": "",
    "text": "About the Author\nDr. Jimmy Moses is a Papua New Guinean entomologist and lecturer at the Papua New Guinea University of Technology’s School of Forestry, specializing in ant ecology, biostatistics, and geospatial analysis. He holds a Ph.D. in Entomology from the University of South Bohemia (2021) and has extensive experience in tropical ecology research, with a particular focus on ant communities along elevational gradients.\nAs an active researcher and educator, Dr. Moses currently supervises several master’s students and co-supervises a Ph.D. student, bringing substantial expertise to both research methodology and educational practices. His research combines ecological field studies with modern analytical approaches, bridging the gap between traditional field ecology and contemporary data science. This interdisciplinary approach has resulted in several publications in high-impact journals, including Global Ecology and Biogeography and Proceedings of the Royal Society B.\nDr. Moses possesses a diverse technical skillset that uniquely qualifies him to author this book:\nHis international research collaborations span institutions in the Czech Republic, Germany, and Belgium. He has been an integral contributor to the New Guinea Binatang Research Center, supporting both research initiatives and educational programs in Papua New Guinea.\nWhen not teaching or conducting research, Dr. Moses enjoys reading across disciplines—from technical manuals to ecological texts—and experimenting with code to solve practical problems in data analysis. His passion for making complex analytical methods accessible to researchers in the natural sciences drives both his teaching and his writing, including this book.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#about-the-author",
    "href": "preface.html#about-the-author",
    "title": "About This Book",
    "section": "",
    "text": "Advanced proficiency in R and Python for statistical computing and data science\nExpert knowledge of GIS and satellite remote sensing for spatial data analysis\nStrong foundation in biostatistics and experimental design applied to ecological research\nDeveloping expertise in full-stack application development for scientific computing",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#purpose-and-scope",
    "href": "preface.html#purpose-and-scope",
    "title": "About This Book",
    "section": "Purpose and Scope",
    "text": "Purpose and Scope\nThis book is designed to serve as both a learning resource and a reference guide for data analysis in the natural sciences, with applications spanning forestry, agriculture, ecology, environmental science, marine biology, and related disciplines. Whether you’re a student, researcher, technician, professional, or hobbyist in these fields, this book will help you develop the skills needed to analyze and visualize data effectively using R.\nThe focus is on practical applications rather than theoretical statistics, with an emphasis on techniques commonly used across natural science disciplines. By working through this book, you will:\n\nMaster the fundamentals of data analysis in R\nLearn to import, clean, and organize various types of scientific data\nDevelop skills in exploratory data analysis and visualization\nApply appropriate statistical tests for different research questions\nCreate publication-quality visualizations\nImplement reproducible research workflows\nInterpret and communicate results effectively",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#features-of-this-book",
    "href": "preface.html#features-of-this-book",
    "title": "About This Book",
    "section": "Features of This Book",
    "text": "Features of This Book\nThis book includes:\n\nStep-by-step instructions for R with complete code examples\nPractical examples using real datasets from various natural science disciplines\nExercises to reinforce learning and build skills\nTips and best practices from experienced researchers\nReproducible code that can be adapted for your own research\nProfessional formatting of data and model outputs",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#professional-data-and-model-output-formatting",
    "href": "preface.html#professional-data-and-model-output-formatting",
    "title": "About This Book",
    "section": "Professional Data and Model Output Formatting",
    "text": "Professional Data and Model Output Formatting\nThroughout this book, we use several R packages to create professionally formatted tables and model outputs suitable for publications:\n\nknitr: The core package for dynamic report generation, allowing seamless integration of R code with text\nkableExtra: For creating elegant, publication-quality tables with customizable styling\ngt: For producing beautiful, highly customizable tables with advanced formatting options\nbroom: For converting statistical model outputs into tidy data frames that are easier to work with\nsjPlot: For creating publication-ready tables and plots from statistical models\ngtsummary: For creating publication-ready analytical and summary tables\nflextable: For creating tables that work well across different output formats (HTML, PDF, Word)\n\nThese tools help transform raw data and complex statistical outputs into clear, professional presentations. Each chapter demonstrates how to use these packages to format your results effectively, following best practices in scientific publishing. You’ll learn to:\n\nFormat regression tables with proper statistical notation\nCreate elegant summary tables with appropriate precision and units\nGenerate publication-ready ANOVA tables\nDesign custom table themes that match your publication requirements\nExport formatted tables to various formats (HTML, PDF, Word)\n\nThe code examples throughout the book show not just how to perform analyses, but how to present the results professionally—a critical skill for scientific communication.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#how-to-use-the-code-examples",
    "href": "preface.html#how-to-use-the-code-examples",
    "title": "About This Book",
    "section": "How to Use the Code Examples",
    "text": "How to Use the Code Examples\nAll code examples in this book are written in R and can be executed in RStudio. To use the examples:\n\nMake sure you have R and RStudio installed (see Chapter 1 for installation instructions)\nInstall the required packages mentioned at the beginning of each chapter\nCopy and paste the code into your R console or script editor\nModify the code as needed for your own data\n\nThe datasets used in the examples are available in the docs/data directory of the book’s repository and are properly cited throughout the text.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#software-requirements",
    "href": "preface.html#software-requirements",
    "title": "About This Book",
    "section": "Software Requirements",
    "text": "Software Requirements\nThis book uses:\n\nR (version 4.0.0 or higher)\nRStudio (latest version recommended)\nVarious R packages (installation instructions provided in each chapter)",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#feedback-and-contributions",
    "href": "preface.html#feedback-and-contributions",
    "title": "About This Book",
    "section": "Feedback and Contributions",
    "text": "Feedback and Contributions\nYour feedback is valuable for improving future editions of this book. If you find errors, have suggestions, or want to contribute examples, please submit them through the book’s repository or contact the author directly.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#acknowledgments",
    "href": "preface.html#acknowledgments",
    "title": "About This Book",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to express my gratitude to colleagues, students, and the broader R community whose insights and feedback have contributed to the development of this book. Special thanks to the creators and maintainers of the R packages used throughout this book, as well as the data providers whose datasets make the examples both practical and relevant.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "1  Introduction to Data Analysis",
    "section": "",
    "text": "1.1 Overview\nData analysis is a critical skill in modern natural sciences research (Wickham & Grolemund, 2016; Zuur et al., 2009). This chapter introduces the fundamental concepts, tools, and approaches that form the foundation of effective data analysis across various scientific disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-data-analysis-matters-in-natural-sciences",
    "href": "chapters/01-introduction.html#why-data-analysis-matters-in-natural-sciences",
    "title": "1  Introduction to Data Analysis",
    "section": "1.2 Why Data Analysis Matters in Natural Sciences",
    "text": "1.2 Why Data Analysis Matters in Natural Sciences\nData analysis plays a pivotal role in natural sciences research for several reasons:\n\nEvidence-Based Decision Making: Data analysis transforms raw observations into actionable insights, enabling researchers and practitioners to make informed decisions about conservation strategies, resource management practices, agricultural planning, environmental interventions, and more (Bolker et al., 2009).\nPattern Recognition: Through statistical analysis, researchers can identify patterns, trends, and relationships within natural systems that might not be apparent from casual observation alone (Zuur et al., 2007). This applies to diverse fields including ecology, geology, marine biology, atmospheric science, and agriculture.\nHypothesis Testing: Data analysis provides rigorous methods to test hypotheses about natural phenomena, allowing researchers to build and refine scientific theories about how natural systems function (Gotelli & Ellison, 2004). This is fundamental across all scientific disciplines.\nPrediction and Modeling: Advanced analytical techniques enable the development of predictive models that can forecast changes in natural systems, such as species distribution shifts under climate change, crop yield predictions, geological processes, weather patterns, and more (Elith et al., 2009).\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Principles of Robust Experimental Design\n\n\n\nBefore diving into data analysis, ensure your experimental design follows these key principles:\n\nFormulate clear hypotheses: Define specific, testable hypotheses before collecting data\nControl for confounding variables: Identify and account for factors that might influence your results\nRandomize appropriately: Randomly assign treatments to experimental units to reduce bias\nInclude adequate replication: Ensure sufficient sample sizes for statistical power (use power analysis)\nConsider spatial and temporal scales: Match your sampling design to the scales of the processes being studied\nPlan for appropriate controls: Include positive, negative, and procedural controls as needed\nUse factorial designs when appropriate: Efficiently test multiple factors and their interactions\nConsider blocking: Group experimental units to account for known sources of variation\nPre-register your study: Document your hypotheses and analysis plan before collecting data\nPlan for appropriate statistical analysis: Select statistical methods based on your design, not just your results",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#tools-for-data-analysis",
    "href": "chapters/01-introduction.html#tools-for-data-analysis",
    "title": "1  Introduction to Data Analysis",
    "section": "1.3 Tools for Data Analysis",
    "text": "1.3 Tools for Data Analysis\nThis book focuses on R and RStudio as the primary tools for data analysis:\n\n1.3.1 R and RStudio\nR is a powerful programming language and environment specifically designed for statistical computing and graphics. RStudio is an integrated development environment (IDE) that makes working with R more accessible and efficient.\nKey advantages of R include:\n\nOpen-source and free: Available to anyone without cost\nExtensive package ecosystem: Thousands of specialized packages for various types of analyses across all scientific disciplines\nReproducibility: Code-based approach ensures analyses can be repeated and verified\nFlexibility: Can be adapted to virtually any analytical need in the natural sciences\nActive community: Large user base provides support and continuous development\n\n\n\nCode\n# A simple example of R code using real-world data\n# Load the Palmer penguins dataset (a subset of climate_data.csv)\npenguins &lt;- read.csv(\"../data/environmental/climate_data.csv\")\n\n# View the first few rows\nhead(penguins)\n\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n6  Adelie Torgersen           39.3          20.6               190        3650\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n6   male 2007\n\n\nCode\n# Get a summary of bill length measurements\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#setting-up-your-environment",
    "href": "chapters/01-introduction.html#setting-up-your-environment",
    "title": "1  Introduction to Data Analysis",
    "section": "1.4 Setting Up Your Environment",
    "text": "1.4 Setting Up Your Environment\n\n1.4.1 Installing R and RStudio\nTo install R and RStudio:\n\nDownload and install R from CRAN\nDownload and install RStudio from RStudio’s website\n\n\n\n1.4.2 Essential R Packages\nFor the analyses in this book, you’ll need several R packages. You can install them with the following code:\n\n\nCode\ninstall.packages(c(\n  \"tidyverse\",  # Data manipulation and visualization\n  \"rstatix\",    # Statistical tests\n  \"ggplot2\",    # Advanced plotting\n  \"knitr\",      # Document generation\n  \"rmarkdown\"   # Document formatting\n))",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#the-data-analysis-workflow",
    "href": "chapters/01-introduction.html#the-data-analysis-workflow",
    "title": "1  Introduction to Data Analysis",
    "section": "1.5 The Data Analysis Workflow",
    "text": "1.5 The Data Analysis Workflow\nEffective data analysis typically follows a structured workflow:\n\nDefine the Question: Clearly articulate what you want to learn from your data\nCollect Data: Gather the necessary data through fieldwork, experiments, laboratory measurements, or existing datasets\nClean and Prepare Data: Handle missing values, correct errors, and format data appropriately\nExplore Data: Conduct exploratory data analysis to understand patterns and distributions\nAnalyze Data: Apply appropriate statistical methods to address your research questions\nInterpret Results: Draw conclusions based on your analysis\nCommunicate Findings: Present your results through visualizations, reports, or publications\n\nThroughout this book, we’ll follow this workflow as we explore various datasets from across the natural sciences.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#types-of-data-in-natural-sciences-research",
    "href": "chapters/01-introduction.html#types-of-data-in-natural-sciences-research",
    "title": "1  Introduction to Data Analysis",
    "section": "1.6 Types of Data in Natural Sciences Research",
    "text": "1.6 Types of Data in Natural Sciences Research\nResearch across the natural sciences involves several types of data:\n\n1.6.1 Categorical Data\nCategorical data represent qualitative characteristics, such as: - Species names or taxonomic classifications - Habitat or ecosystem types - Rock or soil classifications - Land-use categories - Treatment groups in experiments - Genetic markers\n\n\n1.6.2 Numerical Data\nNumerical data involve measurements or counts: - Continuous measurements (e.g., temperature, pH, concentration, biomass, wavelength) - Discrete counts (e.g., number of individuals, species richness, occurrence frequency) - Rates (e.g., growth rates, reaction rates, decomposition rates) - Ratios and indices (e.g., diversity indices, chemical ratios)\n\n\n1.6.3 Spatial Data\nSpatial data describe geographical distributions: - Coordinates (latitude/longitude) - Elevation or depth - Topographic features - Land cover maps - Remote sensing data - Geological formations\n\n\n1.6.4 Temporal Data\nTemporal data track changes over time: - Time series of measurements - Seasonal patterns - Long-term monitoring data - Growth curves - Decay rates - Historical records\nUnderstanding the type of data you’re working with is crucial for selecting appropriate analytical methods across all natural science disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#summary",
    "href": "chapters/01-introduction.html#summary",
    "title": "1  Introduction to Data Analysis",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nIn this chapter, we’ve introduced the importance of data analysis in natural sciences research and the tools we’ll be using throughout this book. We’ve also outlined the typical data analysis workflow and the types of data commonly encountered across scientific disciplines.\nIn the next chapter, we’ll dive deeper into data basics, learning how to import, clean, and prepare data for analysis.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#exercises",
    "href": "chapters/01-introduction.html#exercises",
    "title": "1  Introduction to Data Analysis",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\n\nInstall R and RStudio on your computer.\nInstall the required R packages listed in this chapter.\nOpen RStudio and create a new R script. Try running a simple command like summary(iris).\nThink about a research question in your field of natural science that interests you. What type of data would you need to address this question?\nExplore one of R’s built-in datasets (e.g., mtcars, iris, or trees) using functions like head(), summary(), and plot().\n\n\n\n\n\n\n\nBolker, B. et al. (2009). Generalized linear mixed models: A practical guide. Trends in Ecology & Evolution.\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species distribution models: Ecological explanation and prediction across space and time. Annual Review of Ecology, Evolution, and Systematics, 40, 677–697.\n\n\nGotelli, N. J., & Ellison, A. M. (2004). Null model analysis of species co-occurrence patterns. Sinauer Associates.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.\n\n\nZuur, A., Ieno, E. N., & Smith, G. M. (2007). Analyzing ecological data. Springer.\n\n\nZuur, A., Ieno, E. N., Walker, N., Saveliev, A. A., & Smith, G. M. (2009). Mixed effects models and extensions in ecology with r. Springer Science & Business Media.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html",
    "href": "chapters/02-data-basics.html",
    "title": "2  Data Basics",
    "section": "",
    "text": "2.1 Introduction\nThis chapter covers the fundamental concepts of working with data in R. You’ll learn how to import, clean, and prepare data for analysis, which are essential skills for any data analysis project across all natural science disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#understanding-data-structures",
    "href": "chapters/02-data-basics.html#understanding-data-structures",
    "title": "2  Data Basics",
    "section": "2.2 Understanding Data Structures",
    "text": "2.2 Understanding Data Structures\nBefore diving into data analysis, it’s important to understand the basic data structures in R:\n\n2.2.1 Data Types\nR has several basic data types:\n\nNumeric: Decimal values (e.g., measurements of temperature, pH, concentration, or distance)\nInteger: Whole numbers (e.g., counts of organisms, samples, or observations)\nCharacter: Text strings (e.g., species names, site descriptions, or treatment labels)\nLogical: TRUE/FALSE values (e.g., presence/absence data or condition met/not met)\nFactor: Categorical variables with levels (e.g., experimental treatments, taxonomic classifications, or soil types)\nDate/Time: Temporal data (e.g., sampling dates, observation times, or seasonal markers)\n\n\n\nCode\n# Examples of different data types\nnumeric_example &lt;- 25.4  # Temperature in Celsius\ncharacter_example &lt;- \"Adelie\"  # Penguin species\nlogical_example &lt;- TRUE  # Presence/absence data\nfactor_example &lt;- factor(c(\"Control\", \"Treatment\", \"Control\"), \n                         levels = c(\"Control\", \"Treatment\"))\ndate_example &lt;- as.Date(\"2020-07-15\")  # Sampling date\n\n# Print examples\nprint(numeric_example)\n\n\n[1] 25.4\n\n\nCode\nprint(character_example)\n\n\n[1] \"Adelie\"\n\n\nCode\nprint(logical_example)\n\n\n[1] TRUE\n\n\nCode\nprint(factor_example)\n\n\n[1] Control   Treatment Control  \nLevels: Control Treatment\n\n\nCode\nprint(date_example)\n\n\n[1] \"2020-07-15\"\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Data Management Best Practices\n\n\n\nProper data management is critical for reproducible research in natural sciences:\n\nDocument metadata: Always maintain detailed records about data collection methods, units, and variable definitions\nUse consistent naming conventions: Create clear, consistent file and variable names (e.g., site_01_temp_2023.csv instead of data1.csv)\nPreserve raw data: Never modify your original data files; always work with copies for cleaning and analysis\nVersion control: Use Git or similar tools to track changes to your data processing scripts\nImplement quality control: Create automated checks for impossible values, outliers, and inconsistencies\nPlan for missing data: Develop a consistent strategy for handling missing values before analysis begins\nCreate tidy data: Structure data with one observation per row and one variable per column\nUse open formats: Store data in non-proprietary formats (CSV, TSV) for long-term accessibility\nBack up regularly: Maintain multiple copies of your data in different physical locations\nConsider data repositories: Share your data through repositories like Dryad, Zenodo, or discipline-specific databases\n\n\n\n\n\n2.2.2 Data Structures in R\nR has several data structures for organizing information:\n\n\nCode\n# Load real datasets\nlibrary(readr)\npenguins &lt;- read_csv(\"../data/environmental/climate_data.csv\")\ncrops &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# Vector example - penguin bill lengths\nbill_lengths &lt;- na.omit(penguins$bill_length_mm[1:10])\nprint(bill_lengths)\n\n\n[1] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 34.1 42.0\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n\nCode\n# Matrix example - create a matrix from penguin measurements\npenguin_matrix &lt;- as.matrix(penguins[1:5, 3:6])\nprint(penguin_matrix)\n\n\n     bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n[1,]           39.1          18.7               181        3750\n[2,]           39.5          17.4               186        3800\n[3,]           40.3          18.0               195        3250\n[4,]             NA            NA                NA          NA\n[5,]           36.7          19.3               193        3450\n\n\nCode\n# Data frame example - first few rows of penguin data\npenguin_data &lt;- penguins[1:5, ]\nprint(penguin_data)\n\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# List example - store different aspects of the dataset\npenguin_summary &lt;- list(\n  species = unique(penguins$species),\n  avg_bill_length = mean(penguins$bill_length_mm, na.rm = TRUE),\n  sample_size = nrow(penguins),\n  years = unique(penguins$year)\n)\nprint(penguin_summary)\n\n\n$species\n[1] \"Adelie\"    \"Gentoo\"    \"Chinstrap\"\n\n$avg_bill_length\n[1] 43.92193\n\n$sample_size\n[1] 344\n\n$years\n[1] 2007 2008 2009",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#importing-data",
    "href": "chapters/02-data-basics.html#importing-data",
    "title": "2  Data Basics",
    "section": "2.3 Importing Data",
    "text": "2.3 Importing Data\n\n2.3.1 Reading Data Files\nR provides several functions for importing data from different file formats:\n\n\nCode\n# CSV files - Palmer Penguins dataset\npenguins_csv &lt;- read.csv(\"../data/environmental/climate_data.csv\")\nhead(penguins_csv, 3)\n\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n\n\nCode\n# Using the tidyverse approach for better handling\nlibrary(tidyverse)\npenguins_tidy &lt;- readr::read_csv(\"../data/environmental/climate_data.csv\")\nhead(penguins_tidy, 3)\n\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# Crop yields dataset\ncrops_csv &lt;- read.csv(\"../data/agriculture/crop_yields.csv\")\nhead(crops_csv, 3)\n\n\n       Entity Code Year Wheat..tonnes.per.hectare. Rice..tonnes.per.hectare.\n1 Afghanistan  AFG 1961                     1.0220                     1.519\n2 Afghanistan  AFG 1962                     0.9735                     1.519\n3 Afghanistan  AFG 1963                     0.8317                     1.519\n  Maize..tonnes.per.hectare. Soybeans..tonnes.per.hectare.\n1                      1.400                            NA\n2                      1.400                            NA\n3                      1.426                            NA\n  Potatoes..tonnes.per.hectare. Beans..tonnes.per.hectare.\n1                        8.6667                         NA\n2                        7.6667                         NA\n3                        8.1333                         NA\n  Peas..tonnes.per.hectare. Cassava..tonnes.per.hectare.\n1                        NA                           NA\n2                        NA                           NA\n3                        NA                           NA\n  Barley..tonnes.per.hectare. Cocoa.beans..tonnes.per.hectare.\n1                        1.08                               NA\n2                        1.08                               NA\n3                        1.08                               NA\n  Bananas..tonnes.per.hectare.\n1                           NA\n2                           NA\n3                           NA\n\n\n\n\n2.3.2 Exploring Real-World Datasets\nLet’s explore some of the real-world datasets we have available:\n\n\nCode\n# Palmer Penguins dataset\npenguins &lt;- read_csv(\"../data/environmental/climate_data.csv\")\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nCode\n# Basic summary statistics\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCode\nsummary(penguins$flipper_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nCode\n# Crop yields dataset\ncrops &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\nglimpse(crops)\n\n\nRows: 13,075\nColumns: 14\n$ Entity                             &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afgh…\n$ Code                               &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", …\n$ Year                               &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966,…\n$ `Wheat (tonnes per hectare)`       &lt;dbl&gt; 1.0220, 0.9735, 0.8317, 0.9510, 0.9…\n$ `Rice (tonnes per hectare)`        &lt;dbl&gt; 1.5190, 1.5190, 1.5190, 1.7273, 1.7…\n$ `Maize (tonnes per hectare)`       &lt;dbl&gt; 1.4000, 1.4000, 1.4260, 1.4257, 1.4…\n$ `Soybeans (tonnes per hectare)`    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Potatoes (tonnes per hectare)`    &lt;dbl&gt; 8.6667, 7.6667, 8.1333, 8.6000, 8.8…\n$ `Beans (tonnes per hectare)`       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Peas (tonnes per hectare)`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Cassava (tonnes per hectare)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Barley (tonnes per hectare)`      &lt;dbl&gt; 1.0800, 1.0800, 1.0800, 1.0857, 1.0…\n$ `Cocoa beans (tonnes per hectare)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Bananas (tonnes per hectare)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#data-cleaning-and-preparation",
    "href": "chapters/02-data-basics.html#data-cleaning-and-preparation",
    "title": "2  Data Basics",
    "section": "2.4 Data Cleaning and Preparation",
    "text": "2.4 Data Cleaning and Preparation\n\n2.4.1 Handling Missing Values\nMissing values are common in scientific datasets and need to be addressed before analysis:\n\n\nCode\n# Check for missing values in the penguins dataset\nsum(is.na(penguins))\n\n\n[1] 19\n\n\nCode\ncolSums(is.na(penguins))\n\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0 \n\n\nCode\n# Create a complete cases dataset\npenguins_complete &lt;- na.omit(penguins)\nprint(paste(\"Original dataset rows:\", nrow(penguins)))\n\n\n[1] \"Original dataset rows: 344\"\n\n\nCode\nprint(paste(\"Complete cases rows:\", nrow(penguins_complete)))\n\n\n[1] \"Complete cases rows: 333\"\n\n\nCode\n# Replace missing values with the mean for numeric columns\npenguins_imputed &lt;- penguins\npenguins_imputed$bill_length_mm[is.na(penguins_imputed$bill_length_mm)] &lt;- \n  mean(penguins_imputed$bill_length_mm, na.rm = TRUE)\npenguins_imputed$bill_depth_mm[is.na(penguins_imputed$bill_depth_mm)] &lt;- \n  mean(penguins_imputed$bill_depth_mm, na.rm = TRUE)\n\n# Check if missing values were replaced\nsum(is.na(penguins_imputed$bill_length_mm))\n\n\n[1] 0\n\n\n\n\n2.4.2 Data Transformation\nOften, you’ll need to transform variables to meet statistical assumptions or for better visualization:\n\n\nCode\n# Load the biodiversity dataset\nbiodiversity &lt;- read_csv(\"../data/ecology/biodiversity.csv\")\nglimpse(biodiversity)\n\n\nRows: 500\nColumns: 24\n$ binomial_name     &lt;chr&gt; \"Abutilon pitcairnense\", \"Acaena exigua\", \"Acalypha …\n$ country           &lt;chr&gt; \"Pitcairn\", \"United States\", \"Congo\", \"Saint Helena,…\n$ continent         &lt;chr&gt; \"Oceania\", \"North America\", \"Africa\", \"Africa\", \"Oce…\n$ group             &lt;chr&gt; \"Flowering Plant\", \"Flowering Plant\", \"Flowering Pla…\n$ year_last_seen    &lt;chr&gt; \"2000-2020\", \"1980-1999\", \"1940-1959\", \"Before 1900\"…\n$ threat_AA         &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1…\n$ threat_BRU        &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0…\n$ threat_RCD        &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n$ threat_ISGD       &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_EPM        &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_CC         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_HID        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_P          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_TS         &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_NSM        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_GE         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_NA         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0…\n$ action_LWP        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ action_SM         &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_LP         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_RM         &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_EA         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_NA         &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ red_list_category &lt;chr&gt; \"Extinct in the Wild\", \"Extinct\", \"Extinct\", \"Extinc…\n\n\nCode\n# Log transformation of a skewed variable (if available)\nif(\"n\" %in% colnames(biodiversity)) {\n  biodiversity$log_n &lt;- log(biodiversity$n + 1)  # Add 1 to handle zeros\n  \n  # Compare original and transformed\n  summary(biodiversity$n)\n  summary(biodiversity$log_n)\n}\n\n# Standardization (z-score) of penguin measurements\npenguins_std &lt;- penguins %&gt;%\n  mutate(\n    bill_length_std = scale(bill_length_mm),\n    flipper_length_std = scale(flipper_length_mm),\n    body_mass_std = scale(body_mass_g)\n  )\n\n# View the first few rows of the transformed data\nhead(select(penguins_std, species, bill_length_mm, bill_length_std, \n             flipper_length_mm, flipper_length_std), 5)\n\n\n# A tibble: 5 × 5\n  species bill_length_mm bill_length_std[,1] flipper_length_mm\n  &lt;chr&gt;            &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie            39.1              -0.883               181\n2 Adelie            39.5              -0.810               186\n3 Adelie            40.3              -0.663               195\n4 Adelie            NA                NA                    NA\n5 Adelie            36.7              -1.32                193\n# ℹ 1 more variable: flipper_length_std &lt;dbl[,1]&gt;\n\n\n\n\n2.4.3 Creating New Variables\nCreating new variables from existing ones is a common data preparation task:\n\n\nCode\n# Create new variables in the penguins dataset\npenguins_derived &lt;- penguins %&gt;%\n  filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm)) %&gt;%\n  mutate(\n    bill_ratio = bill_length_mm / bill_depth_mm,\n    size_category = case_when(\n      body_mass_g &lt; 3500 ~ \"Small\",\n      body_mass_g &lt; 4500 ~ \"Medium\",\n      TRUE ~ \"Large\"\n    )\n  )\n\n# View the new variables\nhead(select(penguins_derived, species, bill_length_mm, bill_depth_mm, \n             bill_ratio, body_mass_g, size_category), 5)\n\n\n# A tibble: 5 × 6\n  species bill_length_mm bill_depth_mm bill_ratio body_mass_g size_category\n  &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;        \n1 Adelie            39.1          18.7       2.09        3750 Medium       \n2 Adelie            39.5          17.4       2.27        3800 Medium       \n3 Adelie            40.3          18         2.24        3250 Small        \n4 Adelie            36.7          19.3       1.90        3450 Small        \n5 Adelie            39.3          20.6       1.91        3650 Medium",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#data-manipulation-with-dplyr",
    "href": "chapters/02-data-basics.html#data-manipulation-with-dplyr",
    "title": "2  Data Basics",
    "section": "2.5 Data Manipulation with dplyr",
    "text": "2.5 Data Manipulation with dplyr\nThe dplyr package provides a powerful grammar for data manipulation:\n\n\nCode\nlibrary(dplyr)\n\n# Filter rows - only Adelie penguins\nadelie_penguins &lt;- penguins %&gt;%\n  filter(species == \"Adelie\")\nhead(adelie_penguins, 3)\n\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# Select columns - focus on measurements\npenguin_measurements &lt;- penguins %&gt;%\n  select(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)\nhead(penguin_measurements, 3)\n\n\n# A tibble: 3 × 6\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n\n\nCode\n# Create new variables\npenguins_analyzed &lt;- penguins %&gt;%\n  mutate(\n    bill_ratio = bill_length_mm / bill_depth_mm,\n    body_mass_kg = body_mass_g / 1000\n  )\nhead(select(penguins_analyzed, species, bill_ratio, body_mass_kg), 3)\n\n\n# A tibble: 3 × 3\n  species bill_ratio body_mass_kg\n  &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Adelie        2.09         3.75\n2 Adelie        2.27         3.8 \n3 Adelie        2.24         3.25\n\n\nCode\n# Summarize data by species\npenguin_summary &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(\n    count = n(),\n    avg_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    avg_bill_depth = mean(bill_depth_mm, na.rm = TRUE),\n    avg_body_mass = mean(body_mass_g, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(avg_body_mass))\nprint(penguin_summary)\n\n\n# A tibble: 3 × 5\n  species   count avg_bill_length avg_bill_depth avg_body_mass\n  &lt;chr&gt;     &lt;int&gt;           &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1 Gentoo      124            47.5           15.0         5076.\n2 Chinstrap    68            48.8           18.4         3733.\n3 Adelie      152            38.8           18.3         3701.\n\n\nCode\n# Analyze crop yields data\ncrop_summary &lt;- crops %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(\n    years_recorded = n(),\n    avg_wheat_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    max_wheat_yield = max(`Wheat (tonnes per hectare)`, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(avg_wheat_yield)) %&gt;%\n  head(10)  # Top 10 countries by average wheat yield\n\nprint(crop_summary)\n\n\n# A tibble: 10 × 4\n   Entity          years_recorded avg_wheat_yield max_wheat_yield\n   &lt;chr&gt;                    &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 Belgium                     19            8.54           10.0 \n 2 Netherlands                 58            7.03            9.29\n 3 Ireland                     58            6.83           10.7 \n 4 United Kingdom              58            6.37            8.98\n 5 Denmark                     58            6.18            8.24\n 6 Luxembourg                  19            5.98            6.82\n 7 Germany                     58            5.89            8.63\n 8 Europe, Western             58            5.72            7.88\n 9 France                      58            5.65            7.80\n10 Northern Europe             58            5.59            7.21",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#exploratory-data-analysis",
    "href": "chapters/02-data-basics.html#exploratory-data-analysis",
    "title": "2  Data Basics",
    "section": "2.6 Exploratory Data Analysis",
    "text": "2.6 Exploratory Data Analysis\nBefore diving into formal statistical tests, it’s essential to explore your data:\n\n\nCode\n# Basic summary statistics\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCode\nsummary(penguins$flipper_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nCode\nsummary(penguins$body_mass_g)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\n\nCode\n# Correlation between variables\ncor_matrix &lt;- cor(\n  penguins %&gt;% \n    select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g),\n  use = \"complete.obs\"\n)\nprint(cor_matrix)\n\n\n                  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\nbill_length_mm         1.0000000    -0.2350529         0.6561813   0.5951098\nbill_depth_mm         -0.2350529     1.0000000        -0.5838512  -0.4719156\nflipper_length_mm      0.6561813    -0.5838512         1.0000000   0.8712018\nbody_mass_g            0.5951098    -0.4719156         0.8712018   1.0000000\n\n\nCode\n# Basic visualization - histogram of bill lengths\nlibrary(ggplot2)\nggplot(penguins, aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Penguin Bill Lengths\",\n       x = \"Bill Length (mm)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Boxplot of body mass by species\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Body Mass by Penguin Species\",\n       x = \"Species\",\n       y = \"Body Mass (g)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# Scatterplot of bill length vs. flipper length\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.7) +\n  labs(title = \"Bill Length vs. Flipper Length\",\n       x = \"Flipper Length (mm)\",\n       y = \"Bill Length (mm)\") +\n  theme_minimal()",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#summary",
    "href": "chapters/02-data-basics.html#summary",
    "title": "2  Data Basics",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nIn this chapter, we’ve covered the basics of working with data in R:\n\nUnderstanding different data types and structures\nImporting data from various file formats\nCleaning and preparing data for analysis\nCreating new variables\nUsing dplyr for powerful data manipulation\nConducting initial exploratory data analysis\n\nThese skills form the foundation for all the analyses we’ll perform in the subsequent chapters. By mastering these basics, you’ll be well-prepared to tackle more complex analytical challenges in various scientific fields.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#exercises",
    "href": "chapters/02-data-basics.html#exercises",
    "title": "2  Data Basics",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\n\nLoad the Palmer Penguins dataset (../data/environmental/climate_data.csv) and create a summary of the number of penguins by species and island.\nCalculate the mean and standard deviation of bill length, bill depth, and body mass for each penguin species.\nCreate a new variable that represents the ratio of flipper length to body mass. Interpret what this ratio might represent biologically.\nCreate a visualization that shows the relationship between bill length and bill depth, colored by species.\nLoad the crop yields dataset (../data/agriculture/crop_yields.csv) and analyze trends in wheat yields over time for a country of your choice.\nCompare the distributions of body mass between male and female penguins using appropriate visualizations.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html",
    "href": "chapters/03-exploratory-analysis.html",
    "title": "3  Exploratory Data Analysis",
    "section": "",
    "text": "3.1 Introduction\nExploratory Data Analysis (EDA) is a critical first step in any data analysis project. In this chapter, you’ll learn how to systematically explore your data to understand its structure, identify patterns, detect anomalies, and generate hypotheses for further investigation.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#the-purpose-of-exploratory-data-analysis",
    "href": "chapters/03-exploratory-analysis.html#the-purpose-of-exploratory-data-analysis",
    "title": "3  Exploratory Data Analysis",
    "section": "3.2 The Purpose of Exploratory Data Analysis",
    "text": "3.2 The Purpose of Exploratory Data Analysis\nExploratory Data Analysis serves several important purposes in natural sciences research:\n\nUnderstanding Data Structure: Gain insights into the basic properties of your dataset\nChecking Data Quality: Identify missing values, outliers, and potential errors\nDiscovering Patterns: Detect relationships, trends, and distributions\nGenerating Hypotheses: Develop questions and hypotheses for formal testing\nInforming Analysis Choices: Guide decisions about appropriate statistical methods\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Creating Reproducible EDA Workflows\n\n\n\nTo ensure your exploratory data analysis is reproducible and transparent:\n\nDocument all data transformations: Record every cleaning step, filter, and transformation applied to raw data\nUse R Markdown or Quarto: Create executable documents that combine code, output, and narrative explanation\nVersion control your analysis: Track changes to your EDA scripts using Git or similar tools\nSave exploratory outputs: Store key visualizations and summary statistics in organized directories\nCreate clear data lineage: Document the origin of each dataset and how it connects to derived datasets\nUse consistent naming conventions: Apply systematic naming to files, variables, and functions\nSeparate exploration from confirmation: Clearly distinguish exploratory analyses from confirmatory hypothesis testing\nInclude data validation checks: Incorporate automated checks for data integrity and quality\nProvide detailed method documentation: Document statistical approaches like ANOVA types (e.g., Type II tests for unbalanced designs)\nShare your EDA code: Make your exploratory scripts available alongside final analyses for complete transparency",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#summarizing-data",
    "href": "chapters/03-exploratory-analysis.html#summarizing-data",
    "title": "3  Exploratory Data Analysis",
    "section": "3.3 Summarizing Data",
    "text": "3.3 Summarizing Data\n\n3.3.1 Descriptive Statistics\nDescriptive statistics provide a concise summary of your data’s central tendency, dispersion, and shape:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the crop yield dataset\ncrop_yields &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# View the first few rows\nhead(crop_yields)\n\n\n# A tibble: 6 × 14\n  Entity      Code   Year `Wheat (tonnes per hectare)` Rice (tonnes per hectar…¹\n  &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;                        &lt;dbl&gt;                     &lt;dbl&gt;\n1 Afghanistan AFG    1961                        1.02                       1.52\n2 Afghanistan AFG    1962                        0.974                      1.52\n3 Afghanistan AFG    1963                        0.832                      1.52\n4 Afghanistan AFG    1964                        0.951                      1.73\n5 Afghanistan AFG    1965                        0.972                      1.73\n6 Afghanistan AFG    1966                        0.867                      1.52\n# ℹ abbreviated name: ¹​`Rice (tonnes per hectare)`\n# ℹ 9 more variables: `Maize (tonnes per hectare)` &lt;dbl&gt;,\n#   `Soybeans (tonnes per hectare)` &lt;dbl&gt;,\n#   `Potatoes (tonnes per hectare)` &lt;dbl&gt;, `Beans (tonnes per hectare)` &lt;dbl&gt;,\n#   `Peas (tonnes per hectare)` &lt;dbl&gt;, `Cassava (tonnes per hectare)` &lt;dbl&gt;,\n#   `Barley (tonnes per hectare)` &lt;dbl&gt;,\n#   `Cocoa beans (tonnes per hectare)` &lt;dbl&gt;, …\n\n\nCode\n# Get summary statistics for wheat yields\nwheat_summary &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  summarize(\n    Mean = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Median = median(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    StdDev = sd(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Min = min(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Max = max(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Q1 = quantile(`Wheat (tonnes per hectare)`, 0.25, na.rm = TRUE),\n    Q3 = quantile(`Wheat (tonnes per hectare)`, 0.75, na.rm = TRUE)\n  )\n\n# Display the summary statistics\nknitr::kable(wheat_summary, caption = \"Summary Statistics for Global Wheat Yields\")\n\n\n\nSummary Statistics for Global Wheat Yields\n\n\nMean\nMedian\nStdDev\nMin\nMax\nQ1\nQ3\n\n\n\n\n2.434914\n1.99\n1.687949\n0\n10.6677\n1.228\n3.1245\n\n\n\n\n\nCode\n# Visualize the distribution of wheat yields\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(bins = 30, fill = \"forestgreen\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Global Wheat Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Identify top wheat-producing countries (by average yield)\ntop_wheat_countries &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(Avg_Yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE)) %&gt;%\n  arrange(desc(Avg_Yield)) %&gt;%\n  head(10)\n\n# Display the top countries\nknitr::kable(top_wheat_countries, caption = \"Top 10 Countries by Average Wheat Yield\")\n\n\n\nTop 10 Countries by Average Wheat Yield\n\n\nEntity\nAvg_Yield\n\n\n\n\nBelgium\n8.544200\n\n\nNetherlands\n7.030172\n\n\nIreland\n6.829840\n\n\nUnited Kingdom\n6.366400\n\n\nDenmark\n6.175285\n\n\nLuxembourg\n5.977411\n\n\nGermany\n5.893978\n\n\nEurope, Western\n5.723267\n\n\nFrance\n5.645341\n\n\nNorthern Europe\n5.589988\n\n\n\n\n\n\n\n3.3.2 Frequency Tables\nFrequency tables are useful for understanding the distribution of categorical variables:\n\n\nCode\n# Let's create a categorical variable based on wheat yield levels\ncrop_yields_with_categories &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(yield_category = case_when(\n    `Wheat (tonnes per hectare)` &lt; 2 ~ \"Low\",\n    `Wheat (tonnes per hectare)` &gt;= 2 & `Wheat (tonnes per hectare)` &lt; 4 ~ \"Medium\",\n    `Wheat (tonnes per hectare)` &gt;= 4 ~ \"High\"\n  ))\n\n# Frequency table for yield categories\ntable(crop_yields_with_categories$yield_category)\n\n\n\n  High    Low Medium \n  1279   4081   2741 \n\n\nCode\n# Proportions\nprop.table(table(crop_yields_with_categories$yield_category))\n\n\n\n     High       Low    Medium \n0.1578817 0.5037650 0.3383533 \n\n\nCode\n# Create a decade variable for temporal analysis\ncrop_yields_with_categories &lt;- crop_yields_with_categories %&gt;%\n  mutate(decade = floor(Year / 10) * 10)\n\n# Two-way frequency table: yield category by decade\nyield_decade_table &lt;- table(crop_yields_with_categories$yield_category, \n                            crop_yields_with_categories$decade)\nyield_decade_table\n\n\n        \n         1960 1970 1980 1990 2000 2010\n  High     34  102  200  261  326  356\n  Low     838  833  760  681  563  406\n  Medium  239  335  344  550  656  617\n\n\nCode\n# Convert to proportions (by row)\nprop.table(yield_decade_table, margin = 1)\n\n\n        \n               1960       1970       1980       1990       2000       2010\n  High   0.02658327 0.07974980 0.15637217 0.20406568 0.25488663 0.27834246\n  Low    0.20534183 0.20411664 0.18622887 0.16687086 0.13795638 0.09948542\n  Medium 0.08719445 0.12221817 0.12550164 0.20065669 0.23932871 0.22510033",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#visualizing-distributions",
    "href": "chapters/03-exploratory-analysis.html#visualizing-distributions",
    "title": "3  Exploratory Data Analysis",
    "section": "3.4 Visualizing Distributions",
    "text": "3.4 Visualizing Distributions\n\n3.4.1 Histograms and Density Plots\nHistograms and density plots help visualize the distribution of continuous variables:\n\n\nCode\n# Histogram of wheat yields\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", color = \"white\", na.rm = TRUE) +\n  labs(title = \"Histogram of Wheat Yields\", \n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Density plot\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_density(fill = \"darkgreen\", alpha = 0.5, na.rm = TRUE) +\n  labs(title = \"Density Plot of Wheat Yields\", \n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Histogram with density overlay\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"darkgreen\", color = \"white\", na.rm = TRUE) +\n  geom_density(color = \"darkgreen\", linewidth = 1, na.rm = TRUE) +\n  labs(title = \"Distribution of Wheat Yields\", \n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Box Plots\nBox plots are excellent for comparing distributions across groups:\n\n\nCode\n# Select a few major countries for comparison\nmajor_wheat_producers &lt;- c(\"United States\", \"China\", \"India\", \"Russia\", \"France\", \"Australia\")\n\n# Filter data for these countries and recent years\nrecent_wheat_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% major_wheat_producers, \n         Year &gt;= 2000,\n         !is.na(`Wheat (tonnes per hectare)`))\n\n# Box plot of wheat yields by country\nggplot(recent_wheat_data, aes(x = Entity, y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Wheat Yields by Country (2000-present)\", \n       x = \"Country\", \n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Enhanced box plot with jittered points\nggplot(recent_wheat_data, aes(x = Entity, y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.5) +\n  geom_jitter(width = 0.2, alpha = 0.5, color = \"darkgreen\") +\n  labs(title = \"Wheat Yields by Country (2000-present)\", \n       x = \"Country\", \n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 Bar Charts\nBar charts are useful for visualizing categorical data:\n\n\nCode\n# Calculate average wheat yield by country for the last decade\nrecent_avg_yields &lt;- crop_yields %&gt;%\n  filter(Year &gt;= 2010, !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(avg_wheat_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_wheat_yield)) %&gt;%\n  head(10)  # Top 10 countries\n\n# Bar chart of average wheat yields\nggplot(recent_avg_yields, aes(x = reorder(Entity, avg_wheat_yield), y = avg_wheat_yield)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  labs(title = \"Top 10 Countries by Average Wheat Yield (2010-present)\", \n       x = \"Country\", \n       y = \"Average Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#exploring-relationships",
    "href": "chapters/03-exploratory-analysis.html#exploring-relationships",
    "title": "3  Exploratory Data Analysis",
    "section": "3.5 Exploring Relationships",
    "text": "3.5 Exploring Relationships\n\n3.5.1 Scatter Plots\nScatter plots help visualize relationships between two continuous variables:\n\n\nCode\n# Let's compare wheat and rice yields\ncrop_yields_filtered &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`), !is.na(`Rice (tonnes per hectare)`)) %&gt;%\n  filter(Year &gt;= 2000)\n\n# Basic scatter plot\nggplot(crop_yields_filtered, aes(x = `Wheat (tonnes per hectare)`, y = `Rice (tonnes per hectare)`)) +\n  geom_point(alpha = 0.5, color = \"darkgreen\") +\n  labs(title = \"Relationship between Wheat and Rice Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Rice Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Scatter plot with color by continent (we'll need to add continent information)\n# For demonstration, let's create a simple mapping for a few countries\ncontinent_mapping &lt;- tibble(\n  Entity = c(\"United States\", \"Canada\", \"Mexico\", \n             \"China\", \"India\", \"Japan\", \n             \"Germany\", \"France\", \"United Kingdom\", \n             \"Brazil\", \"Argentina\", \"Chile\",\n             \"Egypt\", \"Nigeria\", \"South Africa\",\n             \"Australia\", \"New Zealand\"),\n  Continent = c(rep(\"North America\", 3), \n                rep(\"Asia\", 3), \n                rep(\"Europe\", 3), \n                rep(\"South America\", 3),\n                rep(\"Africa\", 3),\n                rep(\"Oceania\", 2))\n)\n\n# Join with our dataset\ncrop_yields_with_continent &lt;- crop_yields_filtered %&gt;%\n  inner_join(continent_mapping, by = \"Entity\")\n\n# Scatter plot with color by continent\nggplot(crop_yields_with_continent, aes(x = `Wheat (tonnes per hectare)`, y = `Rice (tonnes per hectare)`, color = Continent)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Relationship between Wheat and Rice Yields by Continent\",\n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Rice Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Correlation Analysis\nCorrelation analysis quantifies the strength and direction of relationships between variables:\n\n\nCode\n# Select numeric columns for correlation analysis\ncrop_numeric &lt;- crop_yields %&gt;%\n  select(`Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`, `Maize (tonnes per hectare)`, `Soybeans (tonnes per hectare)`, `Potatoes (tonnes per hectare)`, `Beans (tonnes per hectare)`) %&gt;%\n  na.omit()\n\n# Correlation matrix\ncor_matrix &lt;- cor(crop_numeric)\nround(cor_matrix, 2)\n\n\n                              Wheat (tonnes per hectare)\nWheat (tonnes per hectare)                          1.00\nRice (tonnes per hectare)                           0.43\nMaize (tonnes per hectare)                          0.57\nSoybeans (tonnes per hectare)                       0.47\nPotatoes (tonnes per hectare)                       0.57\nBeans (tonnes per hectare)                          0.44\n                              Rice (tonnes per hectare)\nWheat (tonnes per hectare)                         0.43\nRice (tonnes per hectare)                          1.00\nMaize (tonnes per hectare)                         0.73\nSoybeans (tonnes per hectare)                      0.58\nPotatoes (tonnes per hectare)                      0.67\nBeans (tonnes per hectare)                         0.46\n                              Maize (tonnes per hectare)\nWheat (tonnes per hectare)                          0.57\nRice (tonnes per hectare)                           0.73\nMaize (tonnes per hectare)                          1.00\nSoybeans (tonnes per hectare)                       0.65\nPotatoes (tonnes per hectare)                       0.74\nBeans (tonnes per hectare)                          0.63\n                              Soybeans (tonnes per hectare)\nWheat (tonnes per hectare)                             0.47\nRice (tonnes per hectare)                              0.58\nMaize (tonnes per hectare)                             0.65\nSoybeans (tonnes per hectare)                          1.00\nPotatoes (tonnes per hectare)                          0.59\nBeans (tonnes per hectare)                             0.41\n                              Potatoes (tonnes per hectare)\nWheat (tonnes per hectare)                             0.57\nRice (tonnes per hectare)                              0.67\nMaize (tonnes per hectare)                             0.74\nSoybeans (tonnes per hectare)                          0.59\nPotatoes (tonnes per hectare)                          1.00\nBeans (tonnes per hectare)                             0.46\n                              Beans (tonnes per hectare)\nWheat (tonnes per hectare)                          0.44\nRice (tonnes per hectare)                           0.46\nMaize (tonnes per hectare)                          0.63\nSoybeans (tonnes per hectare)                       0.41\nPotatoes (tonnes per hectare)                       0.46\nBeans (tonnes per hectare)                          1.00\n\n\nCode\n# Visualize correlation matrix\nlibrary(corrplot)\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45,\n         title = \"Correlation Matrix of Crop Yields\")\n\n\n\n\n\n\n\n\n\n\n\n3.5.3 Pair Plots\nPair plots provide a comprehensive view of relationships between multiple variables:\n\n\nCode\n# Basic pair plot\npairs(crop_numeric, pch = 19, col = \"darkgreen\")\n\n\n\n\n\n\n\n\n\nCode\n# Enhanced pair plot with GGally\nlibrary(GGally)\nggpairs(crop_numeric) +\n  theme_minimal() +\n  labs(title = \"Relationships Between Different Crop Yields\")",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#identifying-outliers-and-anomalies",
    "href": "chapters/03-exploratory-analysis.html#identifying-outliers-and-anomalies",
    "title": "3  Exploratory Data Analysis",
    "section": "3.6 Identifying Outliers and Anomalies",
    "text": "3.6 Identifying Outliers and Anomalies\n\n3.6.1 Box Plots for Outlier Detection\nBox plots can help identify potential outliers:\n\n\nCode\n# Box plot to identify outliers in wheat yield\nggplot(crop_yields, aes(y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.7, na.rm = TRUE) +\n  labs(title = \"Box Plot of Wheat Yields with Potential Outliers\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Identify potential outliers\nwheat_outliers &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(\n    q1 = quantile(`Wheat (tonnes per hectare)`, 0.25),\n    q3 = quantile(`Wheat (tonnes per hectare)`, 0.75),\n    iqr = q3 - q1,\n    lower_bound = q1 - 1.5 * iqr,\n    upper_bound = q3 + 1.5 * iqr,\n    is_outlier = `Wheat (tonnes per hectare)` &lt; lower_bound | `Wheat (tonnes per hectare)` &gt; upper_bound\n  ) %&gt;%\n  filter(is_outlier) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`)\n\n# Display the outliers\nhead(wheat_outliers, 10)\n\n\n# A tibble: 10 × 3\n   Entity   Year `Wheat (tonnes per hectare)`\n   &lt;chr&gt;   &lt;dbl&gt;                        &lt;dbl&gt;\n 1 Austria  2016                         6.25\n 2 Belgium  2000                         7.92\n 3 Belgium  2001                         8.05\n 4 Belgium  2002                         8.28\n 5 Belgium  2003                         8.58\n 6 Belgium  2004                         8.98\n 7 Belgium  2005                         8.27\n 8 Belgium  2006                         8.25\n 9 Belgium  2007                         7.89\n10 Belgium  2008                         8.76\n\n\n\n\n3.6.2 Z-Scores for Outlier Detection\nZ-scores can also help identify outliers:\n\n\nCode\n# Calculate z-scores for wheat yields\nwheat_z_scores &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(\n    wheat_mean = mean(`Wheat (tonnes per hectare)`),\n    wheat_sd = sd(`Wheat (tonnes per hectare)`),\n    z_score = (`Wheat (tonnes per hectare)` - wheat_mean) / wheat_sd,\n    is_extreme = abs(z_score) &gt; 3\n  )\n\n# Display extreme values (z-score &gt; 3 or &lt; -3)\nwheat_extremes &lt;- wheat_z_scores %&gt;%\n  filter(is_extreme) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, z_score) %&gt;%\n  arrange(desc(abs(z_score)))\n\nhead(wheat_extremes, 10)\n\n\n# A tibble: 10 × 4\n   Entity       Year `Wheat (tonnes per hectare)` z_score\n   &lt;chr&gt;       &lt;dbl&gt;                        &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ireland      2015                        10.7     4.88\n 2 Ireland      2017                        10.2     4.58\n 3 Belgium      2015                        10.0     4.49\n 4 Ireland      2014                        10.0     4.49\n 5 Zambia       2008                         9.94    4.45\n 6 Ireland      2004                         9.92    4.44\n 7 New Zealand  2017                         9.86    4.40\n 8 Ireland      2011                         9.86    4.40\n 9 Ireland      2016                         9.54    4.21\n10 Belgium      2009                         9.47    4.16",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#time-series-exploration",
    "href": "chapters/03-exploratory-analysis.html#time-series-exploration",
    "title": "3  Exploratory Data Analysis",
    "section": "3.7 Time Series Exploration",
    "text": "3.7 Time Series Exploration\nAgricultural data often contains important temporal patterns:\n\n\nCode\n# Select a few countries for time series analysis\ncountries_for_ts &lt;- c(\"United States\", \"China\", \"India\", \"France\")\n\n# Filter data for these countries\nwheat_ts_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% countries_for_ts, !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  filter(Year &gt;= 1960)\n\n# Time series plot\nggplot(wheat_ts_data, aes(x = Year, y = `Wheat (tonnes per hectare)`, color = Entity)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  labs(title = \"Wheat Yield Trends Over Time\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\")",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#missing-data-analysis",
    "href": "chapters/03-exploratory-analysis.html#missing-data-analysis",
    "title": "3  Exploratory Data Analysis",
    "section": "3.8 Missing Data Analysis",
    "text": "3.8 Missing Data Analysis\nUnderstanding patterns of missing data is crucial:\n\n\nCode\n# Check for missing values in each column\ncolSums(is.na(crop_yields))\n\n\n                          Entity                             Code \n                               0                             1919 \n                            Year       Wheat (tonnes per hectare) \n                               0                             4974 \n       Rice (tonnes per hectare)       Maize (tonnes per hectare) \n                            4604                             2301 \n   Soybeans (tonnes per hectare)    Potatoes (tonnes per hectare) \n                            7114                             3059 \n      Beans (tonnes per hectare)        Peas (tonnes per hectare) \n                            5066                             6840 \n    Cassava (tonnes per hectare)      Barley (tonnes per hectare) \n                            5887                             6342 \nCocoa beans (tonnes per hectare)     Bananas (tonnes per hectare) \n                            8466                             4166 \n\n\nCode\n# Visualize missing data patterns\nif(requireNamespace(\"naniar\", quietly = TRUE)) {\n  library(naniar)\n  \n  # Create a visualization of missing data\n  gg_miss_var(crop_yields)\n  \n  # Create a matrix showing missing data patterns\n  vis_miss(crop_yields[, c(\"Entity\", \"Year\", \"Wheat (tonnes per hectare)\", \"Rice (tonnes per hectare)\", \"Maize (tonnes per hectare)\")])\n} else {\n  message(\"The 'naniar' package is not installed. Install it with install.packages('naniar') to visualize missing data patterns.\")\n  \n  # Alternative: simple summary of missing data\n  missing_summary &lt;- sapply(crop_yields, function(x) sum(is.na(x)))\n  missing_df &lt;- data.frame(\n    Variable = names(missing_summary),\n    Missing_Count = missing_summary,\n    Missing_Percent = round(missing_summary / nrow(crop_yields) * 100, 2)\n  )\n  \n  # Display the summary\n  missing_df &lt;- missing_df[order(-missing_df$Missing_Count), ]\n  head(missing_df, 10)\n}\n\n\n                                                         Variable Missing_Count\nCocoa beans (tonnes per hectare) Cocoa beans (tonnes per hectare)          8466\nSoybeans (tonnes per hectare)       Soybeans (tonnes per hectare)          7114\nPeas (tonnes per hectare)               Peas (tonnes per hectare)          6840\nBarley (tonnes per hectare)           Barley (tonnes per hectare)          6342\nCassava (tonnes per hectare)         Cassava (tonnes per hectare)          5887\nBeans (tonnes per hectare)             Beans (tonnes per hectare)          5066\nWheat (tonnes per hectare)             Wheat (tonnes per hectare)          4974\nRice (tonnes per hectare)               Rice (tonnes per hectare)          4604\nBananas (tonnes per hectare)         Bananas (tonnes per hectare)          4166\nPotatoes (tonnes per hectare)       Potatoes (tonnes per hectare)          3059\n                                 Missing_Percent\nCocoa beans (tonnes per hectare)           64.75\nSoybeans (tonnes per hectare)              54.41\nPeas (tonnes per hectare)                  52.31\nBarley (tonnes per hectare)                48.50\nCassava (tonnes per hectare)               45.02\nBeans (tonnes per hectare)                 38.75\nWheat (tonnes per hectare)                 38.04\nRice (tonnes per hectare)                  35.21\nBananas (tonnes per hectare)               31.86\nPotatoes (tonnes per hectare)              23.40",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#summary",
    "href": "chapters/03-exploratory-analysis.html#summary",
    "title": "3  Exploratory Data Analysis",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nThis chapter has demonstrated various techniques for exploratory data analysis using a real agricultural dataset. We’ve covered:\n\nComputing and interpreting descriptive statistics\nCreating and analyzing frequency tables\nVisualizing distributions with histograms, density plots, and box plots\nExploring relationships with scatter plots and correlation analysis\nIdentifying outliers and anomalies\nAnalyzing time series patterns\nExamining missing data\n\nThese techniques provide a foundation for understanding your data before proceeding to more advanced analyses. By thoroughly exploring your data, you can make informed decisions about appropriate statistical methods and generate meaningful hypotheses for testing.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#exercises",
    "href": "chapters/03-exploratory-analysis.html#exercises",
    "title": "3  Exploratory Data Analysis",
    "section": "3.10 Exercises",
    "text": "3.10 Exercises\n\nLoad the plant biodiversity dataset from docs/data/ecology/biodiversity.csv and perform a comprehensive exploratory analysis.\nCreate a histogram and density plot for another crop in the dataset. How does its distribution compare to wheat?\nInvestigate the relationship between potato yields and latitude (you’ll need to find or create a dataset with latitude information).\nIdentify countries with the most significant improvement in crop yields over time.\nCreate a time series plot showing the ratio of wheat to rice yields over time for major producing countries.\nPerform the same exploratory analyses in R for the spatial dataset in docs/data/geography/spatial.csv.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html",
    "href": "chapters/04-hypothesis-testing.html",
    "title": "4  Hypothesis Testing",
    "section": "",
    "text": "4.1 Introduction\nHypothesis testing is a fundamental statistical approach used to make inferences about populations based on sample data. In ecological and forestry research, hypothesis testing helps researchers determine whether observed patterns or differences are statistically significant or merely due to random chance.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "href": "chapters/04-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "title": "4  Hypothesis Testing",
    "section": "4.2 The Logic of Hypothesis Testing",
    "text": "4.2 The Logic of Hypothesis Testing\n\n4.2.1 Null and Alternative Hypotheses\nThe foundation of hypothesis testing involves two competing hypotheses:\n\nNull Hypothesis (H₀): This is the default position that assumes no effect, no difference, or no relationship exists. For example, “There is no difference in tree height between two forest types.”\nAlternative Hypothesis (H₁ or Hₐ): This is the hypothesis that the researcher typically wants to provide evidence for. For example, “There is a significant difference in tree height between two forest types.”\n\n\n\n4.2.2 Example in Ecological Research\nLet’s consider a specific example from forestry research:\n\nResearch Question: Is there a difference in the average height of oak trees between Site A and Site B?\nNull Hypothesis (H₀): There is no difference in the average height of oak trees between Site A and Site B.\nAlternative Hypothesis (H₁): There is a significant difference in the average height of oak trees between Site A and Site B.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#understanding-p-values-and-significance-levels",
    "href": "chapters/04-hypothesis-testing.html#understanding-p-values-and-significance-levels",
    "title": "4  Hypothesis Testing",
    "section": "4.3 Understanding P-values and Significance Levels",
    "text": "4.3 Understanding P-values and Significance Levels\n\n4.3.1 The P-value\nThe p-value is the probability of obtaining results at least as extreme as the observed results, assuming that the null hypothesis is true. In simpler terms, it measures the strength of evidence against the null hypothesis.\n\nA small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, leading to its rejection.\nA large p-value (&gt; 0.05) indicates weak evidence against the null hypothesis, leading to a failure to reject it.\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Interpreting P-values\n\n\n\nWhen reporting p-values in scientific publications:\n\nAvoid describing results as “statistically significant” or “not significant” without providing the actual p-value\nReport exact p-values (e.g., p = 0.032) rather than just p &lt; 0.05, when possible\nRemember that p-values do not measure the size or importance of an effect—only the evidence against the null hypothesis\nConsider reporting effect sizes and confidence intervals alongside p-values for more comprehensive interpretation\nBe cautious about p-values just above or below the significance threshold (e.g., p = 0.049 vs. p = 0.051)—they represent similar levels of evidence\n\n\n\n\n\n4.3.2 Significance Level (α)\nThe significance level, often denoted as α (alpha), represents the threshold for statistical significance. In most research, it is set at 0.05 (5%). This value signifies the maximum acceptable probability of making a Type I error — wrongly rejecting the null hypothesis when it is true.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#types-of-errors-in-hypothesis-testing",
    "href": "chapters/04-hypothesis-testing.html#types-of-errors-in-hypothesis-testing",
    "title": "4  Hypothesis Testing",
    "section": "4.4 Types of Errors in Hypothesis Testing",
    "text": "4.4 Types of Errors in Hypothesis Testing\n\n4.4.1 Type I and Type II Errors\nIn hypothesis testing, two types of errors can occur:\n\nType I Error: Rejecting a true null hypothesis (false positive).\n\nProbability = α (significance level)\nExample: Concluding there’s a difference in tree heights when there actually isn’t.\n\nType II Error: Failing to reject a false null hypothesis (false negative).\n\nProbability = β\nExample: Failing to detect a real difference in tree heights.\n\n\n\n\n4.4.2 Experimental Design\n\n\n\n\n\n\nPROFESSIONAL TIP: Improving Statistical Power\n\n\n\nTo reduce Type II errors and increase the power of your study:\n\nIncrease sample size: Larger samples provide more precise estimates and greater power\nReduce measurement variability: Use standardized protocols and calibrated instruments\nUse paired or repeated measures designs when appropriate: These control for individual variation\nConduct a power analysis before data collection: This helps determine the minimum sample size needed\nConsider using one-tailed tests when appropriate: These provide more power than two-tailed tests when the direction of effect is known\nReport confidence intervals: These provide information about effect size and precision",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#common-hypothesis-tests-in-ecological-research",
    "href": "chapters/04-hypothesis-testing.html#common-hypothesis-tests-in-ecological-research",
    "title": "4  Hypothesis Testing",
    "section": "4.5 Common Hypothesis Tests in Ecological Research",
    "text": "4.5 Common Hypothesis Tests in Ecological Research",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#example-two-sample-t-test",
    "href": "chapters/04-hypothesis-testing.html#example-two-sample-t-test",
    "title": "4  Hypothesis Testing",
    "section": "4.6 Example: Two-Sample t-test",
    "text": "4.6 Example: Two-Sample t-test\n\n\nCode\n# Simulate tree height data for two sites\nset.seed(123)\nsite_A &lt;- rnorm(30, mean = 25, sd = 5)  # 30 trees with mean height 25m\nsite_B &lt;- rnorm(30, mean = 28, sd = 5)  # 30 trees with mean height 28m\n\n# Create a data frame\ntree_data &lt;- data.frame(\n  height = c(site_A, site_B),\n  site = factor(rep(c(\"A\", \"B\"), each = 30))\n)\n\n# Visualize the data\nlibrary(ggplot2)\nggplot(tree_data, aes(x = site, y = height, fill = site)) +\n  geom_boxplot() +\n  labs(title = \"Tree Heights by Site\",\n       x = \"Site\",\n       y = \"Height (m)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform a t-test\nt_test_result &lt;- t.test(height ~ site, data = tree_data)\nprint(t_test_result)\n\n\n\n    Welch Two Sample t-test\n\ndata:  height by site\nt = -3.5092, df = 56.559, p-value = 0.0008892\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -6.482713 -1.771708\nsample estimates:\nmean in group A mean in group B \n       24.76448        28.89169 \n\n\nCode\n# Interpret the result\nalpha &lt;- 0.05\nif (t_test_result$p.value &lt; alpha) {\n  cat(\"With a p-value of\", round(t_test_result$p.value, 4), \n      \"we reject the null hypothesis.\\n\",\n      \"There is a statistically significant difference in tree heights between sites.\")\n} else {\n  cat(\"With a p-value of\", round(t_test_result$p.value, 4), \n      \"we fail to reject the null hypothesis.\\n\",\n      \"There is not enough evidence to conclude a significant difference in tree heights.\")\n}\n\n\nWith a p-value of 9e-04 we reject the null hypothesis.\n There is a statistically significant difference in tree heights between sites.\n\n\nCode\n# Create a formatted table of the results\nt_test_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(t_test_result$statistic, 3),\n    round(t_test_result$parameter, 1),\n    format.pval(t_test_result$p.value, digits = 3),\n    round(diff(t_test_result$estimate), 2),\n    round(t_test_result$conf.int[1], 2),\n    round(t_test_result$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(t_test_table, \n             caption = \"Two-Sample t-Test Results: Tree Heights by Site\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nTwo-Sample t-Test Results: Tree Heights by Site\n\n\nStatistic\nValue\n\n\n\n\nt-value\n-3.509\n\n\nDegrees of Freedom\n56.6\n\n\np-value\n0.000889\n\n\nMean Difference\n4.13\n\n\n95% CI Lower\n-6.48\n\n\n95% CI Upper\n-1.77",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#example-using-marine-dataset-for-two-sample-t-test",
    "href": "chapters/04-hypothesis-testing.html#example-using-marine-dataset-for-two-sample-t-test",
    "title": "4  Hypothesis Testing",
    "section": "4.7 Example: Using Marine Dataset for Two-Sample t-test",
    "text": "4.7 Example: Using Marine Dataset for Two-Sample t-test\nLet’s apply the t-test to analyze real data. We’ll use our marine dataset to compare fishing yields between different regions:\n\n\nCode\n# Load necessary packages\nlibrary(tidyverse)\n\n# Load the marine dataset\nmarine_data &lt;- read_csv(\"../data/marine/ocean_data.csv\")\n\n# View the structure of the dataset\nstr(marine_data)\n\n\nspc_tbl_ [65,706 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ year       : num [1:65706] 1991 1991 1991 1991 1991 ...\n $ lake       : chr [1:65706] \"Erie\" \"Erie\" \"Erie\" \"Erie\" ...\n $ species    : chr [1:65706] \"American Eel\" \"American Eel\" \"American Eel\" \"American Eel\" ...\n $ grand_total: num [1:65706] 1 1 1 1 1 1 0 0 0 0 ...\n $ comments   : chr [1:65706] NA NA NA NA ...\n $ region     : chr [1:65706] \"Michigan (MI)\" \"New York (NY)\" \"Ohio (OH)\" \"Pennsylvania (PA)\" ...\n $ values     : num [1:65706] 0 0 0 0 0 1 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   year = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   grand_total = col_double(),\n  ..   comments = col_character(),\n  ..   region = col_character(),\n  ..   values = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\n# Let's compare fishing yields between two lakes\nif(\"lake\" %in% colnames(marine_data) & \"values\" %in% colnames(marine_data)) {\n  # Select two lakes for comparison\n  lake_comparison &lt;- marine_data %&gt;%\n    filter(lake %in% c(\"Michigan\", \"Superior\")) %&gt;%\n    select(lake, values)\n  \n  # Perform t-test\n  t_test_result &lt;- t.test(values ~ lake, data = lake_comparison)\n  \n  # Display the results\n  print(t_test_result)\n  \n  # Visualize the comparison\n  ggplot(lake_comparison, aes(x = lake, y = values)) +\n    geom_boxplot(fill = \"lightblue\") +\n    labs(title = \"Comparison of Fishing Yields Between Lakes\",\n         x = \"Lake\", y = \"Yield Values\") +\n    theme_minimal()\n} else {\n  # If the columns don't match exactly, adapt to the actual structure\n  # This is a fallback to ensure the code runs with the actual data\n  print(\"Column names don't match expected structure. Adapting...\")\n  \n  # Identify numeric columns for analysis\n  numeric_cols &lt;- sapply(marine_data, is.numeric)\n  if(sum(numeric_cols) &gt; 0) {\n    numeric_col &lt;- names(marine_data)[numeric_cols][1]\n    \n    # Identify a categorical column for grouping\n    cat_cols &lt;- sapply(marine_data, function(x) is.character(x) || is.factor(x))\n    if(sum(cat_cols) &gt; 0) {\n      cat_col &lt;- names(marine_data)[cat_cols][1]\n      \n      # Get the two most frequent categories\n      top_categories &lt;- names(sort(table(marine_data[[cat_col]]), decreasing = TRUE)[1:2])\n      \n      # Filter data for these categories\n      comparison_data &lt;- marine_data %&gt;%\n        filter(!!sym(cat_col) %in% top_categories) %&gt;%\n        select(!!sym(cat_col), !!sym(numeric_col))\n      \n      # Rename columns for easier formula creation\n      names(comparison_data) &lt;- c(\"category\", \"value\")\n      \n      # Perform t-test\n      t_test_result &lt;- t.test(value ~ category, data = comparison_data)\n      \n      # Display the results\n      print(t_test_result)\n      \n      # Visualize the comparison\n      ggplot(comparison_data, aes(x = category, y = value)) +\n        geom_boxplot(fill = \"lightblue\") +\n        labs(title = paste(\"Comparison of\", numeric_col, \"Between Groups\"),\n             x = cat_col, y = numeric_col) +\n        theme_minimal()\n    }\n  }\n}\n\n\n\n    Welch Two Sample t-test\n\ndata:  values by lake\nt = 7.0924, df = 16555, p-value = 1.371e-12\nalternative hypothesis: true difference in means between group Michigan and group Superior is not equal to 0\n95 percent confidence interval:\n 164.1330 289.5019\nsample estimates:\nmean in group Michigan mean in group Superior \n              759.5080               532.6905",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#types-of-errors-in-hypothesis-testing-1",
    "href": "chapters/04-hypothesis-testing.html#types-of-errors-in-hypothesis-testing-1",
    "title": "4  Hypothesis Testing",
    "section": "4.8 Types of Errors in Hypothesis Testing",
    "text": "4.8 Types of Errors in Hypothesis Testing\n\n4.8.1 Type I and Type II Errors\nIn hypothesis testing, two types of errors can occur:\n\nType I Error: Rejecting a true null hypothesis (false positive).\n\nProbability = α (significance level)\nExample: Concluding there’s a difference in tree heights when there actually isn’t.\n\nType II Error: Failing to reject a false null hypothesis (false negative).\n\nProbability = β\nExample: Failing to detect a real difference in tree heights.\n\n\n\n\n4.8.2 Statistical Power\nStatistical power is the probability of correctly rejecting a false null hypothesis (1 - β). Factors affecting power include:\n\nSample size\nEffect size\nSignificance level (α)\nVariability in the data\n\n\n\nCode\n# Demonstrate power calculation for a t-test\nlibrary(pwr)\n\n# Calculate power for our example\neffect_size &lt;- (28 - 25) / 5  # (mean difference) / standard deviation\npower_result &lt;- pwr.t.test(\n  n = 30,                    # Sample size per group\n  d = effect_size,           # Cohen's d effect size\n  sig.level = 0.05,          # Significance level\n  type = \"two.sample\",       # Two-sample t-test\n  alternative = \"two.sided\"  # Two-sided alternative\n)\n\nprint(power_result)\n\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.6\n      sig.level = 0.05\n          power = 0.6275046\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\n# Calculate required sample size for 80% power\nsample_size_result &lt;- pwr.t.test(\n  d = effect_size,           # Cohen's d effect size\n  sig.level = 0.05,          # Significance level\n  power = 0.8,               # Desired power\n  type = \"two.sample\",       # Two-sample t-test\n  alternative = \"two.sided\"  # Two-sided alternative\n)\n\nprint(sample_size_result)\n\n\n\n     Two-sample t test power calculation \n\n              n = 44.58577\n              d = 0.6\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#one-sample-tests",
    "href": "chapters/04-hypothesis-testing.html#one-sample-tests",
    "title": "4  Hypothesis Testing",
    "section": "4.9 One-Sample Tests",
    "text": "4.9 One-Sample Tests\nOne-sample tests compare a sample mean to a known or hypothesized population value.\n\n4.9.1 One-Sample t-Test\nThe one-sample t-test is used when: - The sample is approximately normally distributed - The population standard deviation is unknown\n\n\nCode\n# Simulate tree diameter data\nset.seed(456)\ntree_diameters &lt;- rnorm(25, mean = 32, sd = 5)  # 25 trees with mean diameter 32cm\n\n# Known reference value (e.g., from previous studies)\nreference_diameter &lt;- 30  # cm\n\n# Visualize the data\nggplot(data.frame(diameter = tree_diameters), aes(x = diameter)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = reference_diameter, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Tree Diameters with Reference Value\",\n       x = \"Diameter (cm)\",\n       y = \"Frequency\") +\n  annotate(\"text\", x = reference_diameter + 2, y = 5, \n           label = \"Reference\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform a one-sample t-test\none_sample_result &lt;- t.test(tree_diameters, mu = reference_diameter)\n\n# Create a formatted table of the results\none_sample_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(one_sample_result$statistic, 3),\n    round(one_sample_result$parameter, 1),\n    format.pval(one_sample_result$p.value, digits = 3),\n    round(mean(tree_diameters) - reference_diameter, 2),\n    round(one_sample_result$conf.int[1], 2),\n    round(one_sample_result$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(one_sample_table, \n             caption = \"One-Sample t-Test Results: Tree Diameters vs. Reference Value\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nOne-Sample t-Test Results: Tree Diameters vs. Reference Value\n\n\nStatistic\nValue\n\n\n\n\nt-value\n2.731\n\n\nDegrees of Freedom\n24\n\n\np-value\n0.0116\n\n\nMean Difference\n3.24\n\n\n95% CI Lower\n30.79\n\n\n95% CI Upper\n35.69",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#two-sample-tests",
    "href": "chapters/04-hypothesis-testing.html#two-sample-tests",
    "title": "4  Hypothesis Testing",
    "section": "4.10 Two-Sample Tests",
    "text": "4.10 Two-Sample Tests\nTwo-sample tests compare means between two independent groups.\n\n4.10.1 Independent Samples t-Test\nThe independent samples t-test is used when: - Both samples are approximately normally distributed - The two samples are independent\n\n\nCode\n# We already performed this test in our initial example\n# Let's visualize it differently\n\n# Create density plots\nggplot(tree_data, aes(x = height, fill = site)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Distribution of Tree Heights by Site\",\n       x = \"Height (m)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Add mean lines\nggplot(tree_data, aes(x = height, fill = site)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(xintercept = mean(site_A), color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean(site_B), color = \"blue\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Tree Heights by Site\",\n       x = \"Height (m)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform the t-test again for demonstration\nt_test_result &lt;- t.test(height ~ site, data = tree_data, var.equal = TRUE)\n\n# Create a formatted table of the results\nt_test_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(t_test_result$statistic, 3),\n    round(t_test_result$parameter, 1),\n    format.pval(t_test_result$p.value, digits = 3),\n    round(diff(t_test_result$estimate), 2),\n    round(t_test_result$conf.int[1], 2),\n    round(t_test_result$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(t_test_table, \n             caption = \"Independent Samples t-Test Results: Tree Heights by Site\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nIndependent Samples t-Test Results: Tree Heights by Site\n\n\nStatistic\nValue\n\n\n\n\nt-value\n-3.509\n\n\nDegrees of Freedom\n58\n\n\np-value\n0.000876\n\n\nMean Difference\n4.13\n\n\n95% CI Lower\n-6.48\n\n\n95% CI Upper\n-1.77\n\n\n\n\n\n\n\nCode\n# Create a summary statistics table\nsite_summary &lt;- tree_data %&gt;%\n  group_by(site) %&gt;%\n  summarize(\n    n = n(),\n    Mean = round(mean(height), 2),\n    SD = round(sd(height), 2),\n    Min = round(min(height), 2),\n    Max = round(max(height), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(site_summary, \n             caption = \"Summary Statistics: Tree Heights by Site\",\n             align = c(\"l\", \"c\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Tree Heights by Site\n\n\nsite\nn\nMean\nSD\nMin\nMax\n\n\n\n\nA\n30\n24.76\n4.91\n15.17\n33.93\n\n\nB\n30\n28.89\n4.18\n20.26\n38.84\n\n\n\n\n\n\n\n\n\n4.10.2 Paired Samples t-Test\nThe paired samples t-test is used when: - Measurements are taken from the same subjects under different conditions - The differences between pairs are approximately normally distributed\n\n\nCode\n# Simulate paired data (e.g., tree growth before and after treatment)\nset.seed(789)\nbefore_treatment &lt;- rnorm(20, mean = 15, sd = 3)\nafter_treatment &lt;- before_treatment + rnorm(20, mean = 2.5, sd = 1)  # Growth effect\n\n# Create a data frame\ngrowth_data &lt;- data.frame(\n  tree_id = 1:20,\n  before = before_treatment,\n  after = after_treatment,\n  difference = after_treatment - before_treatment\n)\n\n# Visualize paired data\ngrowth_long &lt;- reshape2::melt(growth_data[, c(\"tree_id\", \"before\", \"after\")], \n                             id.vars = \"tree_id\", \n                             variable.name = \"time\", \n                             value.name = \"height\")\n\nggplot(growth_long, aes(x = time, y = height, group = tree_id)) +\n  geom_line(alpha = 0.3) +\n  geom_point(aes(color = time), size = 3) +\n  labs(title = \"Tree Heights Before and After Treatment\",\n       x = \"Time\",\n       y = \"Height (m)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform a paired t-test\npaired_result &lt;- t.test(growth_data$after, growth_data$before, paired = TRUE)\n\n# Create a formatted table of the results\npaired_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(paired_result$statistic, 3),\n    round(paired_result$parameter, 1),\n    format.pval(paired_result$p.value, digits = 3),\n    round(mean(growth_data$difference), 2),\n    round(paired_result$conf.int[1], 2),\n    round(paired_result$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(paired_table, \n             caption = \"Paired Samples t-Test Results: Tree Heights Before and After Treatment\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nPaired Samples t-Test Results: Tree Heights Before and After Treatment\n\n\nStatistic\nValue\n\n\n\n\nt-value\n13.843\n\n\nDegrees of Freedom\n19\n\n\np-value\n2.24e-11\n\n\nMean Difference\n2.21\n\n\n95% CI Lower\n1.87\n\n\n95% CI Upper\n2.54\n\n\n\n\n\n\n\nCode\n# Create a summary statistics table\ngrowth_summary &lt;- growth_data %&gt;%\n  summarize(\n    `Median Before` = median(before),\n    `Mean Before` = round(mean(before), 2),\n    `Median After` = median(after),\n    `Mean After` = round(mean(after), 2),\n    `Median Difference` = median(difference),\n    `Mean Difference` = round(mean(difference), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(growth_summary, \n             caption = \"Summary Statistics: Tree Heights Before and After Treatment\",\n             align = rep(\"r\", 6),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Tree Heights Before and After Treatment\n\n\nMedian Before\nMean Before\nMedian After\nMean After\nMedian Difference\nMean Difference\n\n\n\n\n13.85453\n14.07\n16.25036\n16.28\n2.178109\n2.21",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#non-parametric-tests",
    "href": "chapters/04-hypothesis-testing.html#non-parametric-tests",
    "title": "4  Hypothesis Testing",
    "section": "4.11 Non-Parametric Tests",
    "text": "4.11 Non-Parametric Tests\nNon-parametric tests are used when the assumptions of parametric tests (like normality) are violated.\n\n4.11.1 Mann-Whitney U Test (Wilcoxon Rank-Sum Test)\nThis is a non-parametric alternative to the independent samples t-test.\n\n\nCode\n# Simulate non-normal data (e.g., species counts in two habitats)\nset.seed(101)\nhabitat_A &lt;- rpois(25, lambda = 8)  # Poisson distribution for count data\nhabitat_B &lt;- rpois(25, lambda = 12)\n\n# Create a data frame\nspecies_data &lt;- data.frame(\n  count = c(habitat_A, habitat_B),\n  habitat = factor(rep(c(\"A\", \"B\"), each = 25))\n)\n\n# Visualize the data\nggplot(species_data, aes(x = habitat, y = count, fill = habitat)) +\n  geom_boxplot() +\n  labs(title = \"Species Counts by Habitat\",\n       x = \"Habitat\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Check for normality\nshapiro_A &lt;- shapiro.test(habitat_A)\nshapiro_B &lt;- shapiro.test(habitat_B)\n\n# Create a table for normality test results\nnormality_table &lt;- data.frame(\n  Habitat = c(\"Habitat A\", \"Habitat B\"),\n  `W Statistic` = c(round(shapiro_A$statistic, 3), round(shapiro_B$statistic, 3)),\n  `p-value` = c(format.pval(shapiro_A$p.value, digits = 3), format.pval(shapiro_B$p.value, digits = 3)),\n  Interpretation = c(\n    ifelse(shapiro_A$p.value &lt; 0.05, \"Non-normal distribution\", \"Normal distribution\"),\n    ifelse(shapiro_B$p.value &lt; 0.05, \"Non-normal distribution\", \"Normal distribution\")\n  )\n)\n\n# Display the normality test results\nknitr::kable(normality_table, \n             caption = \"Shapiro-Wilk Test for Normality\",\n             align = c(\"l\", \"c\", \"c\", \"l\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nShapiro-Wilk Test for Normality\n\n\nHabitat\nW.Statistic\np.value\nInterpretation\n\n\n\n\nHabitat A\n0.972\n0.689\nNormal distribution\n\n\nHabitat B\n0.976\n0.787\nNormal distribution\n\n\n\n\n\n\n\nCode\n# Perform Mann-Whitney U test\nwilcox_result &lt;- wilcox.test(count ~ habitat, data = species_data)\n\n# Create a formatted table of the results\nwilcox_table &lt;- data.frame(\n  Statistic = c(\"W-value\", \"p-value\"),\n  Value = c(\n    wilcox_result$statistic,\n    format.pval(wilcox_result$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(wilcox_table, \n             caption = \"Mann-Whitney U Test Results: Species Counts by Habitat\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nMann-Whitney U Test Results: Species Counts by Habitat\n\n\n\nStatistic\nValue\n\n\n\n\nW\nW-value\n88.5\n\n\n\np-value\n1.31e-05\n\n\n\n\n\n\n\nCode\n# Create a summary statistics table\nhabitat_summary &lt;- species_data %&gt;%\n  group_by(habitat) %&gt;%\n  summarize(\n    n = n(),\n    Median = median(count),\n    Mean = round(mean(count), 2),\n    SD = round(sd(count), 2),\n    Min = min(count),\n    Max = max(count)\n  )\n\n# Display the summary statistics table\nknitr::kable(habitat_summary, \n             caption = \"Summary Statistics: Species Counts by Habitat\",\n             align = c(\"l\", \"c\", \"c\", \"r\", \"r\", \"c\", \"c\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Species Counts by Habitat\n\n\nhabitat\nn\nMedian\nMean\nSD\nMin\nMax\n\n\n\n\nA\n25\n8\n8.28\n2.44\n3\n13\n\n\nB\n25\n13\n12.52\n3.18\n7\n20\n\n\n\n\n\n\n\n\n\n4.11.2 Wilcoxon Signed-Rank Test\nThis is a non-parametric alternative to the paired samples t-test.\n\n\nCode\n# Simulate non-normal paired data\nset.seed(202)\nbefore_restoration &lt;- rpois(20, lambda = 5)\nafter_restoration &lt;- before_restoration + rpois(20, lambda = 3)\n\n# Create a data frame\nrestoration_data &lt;- data.frame(\n  site_id = 1:20,\n  before = before_restoration,\n  after = after_restoration,\n  difference = after_restoration - before_restoration\n)\n\n# Visualize paired data\nrestoration_long &lt;- reshape2::melt(restoration_data[, c(\"site_id\", \"before\", \"after\")], \n                                  id.vars = \"site_id\", \n                                  variable.name = \"time\", \n                                  value.name = \"species_count\")\n\nggplot(restoration_long, aes(x = time, y = species_count, group = site_id)) +\n  geom_line(alpha = 0.3) +\n  geom_point(aes(color = time), size = 3) +\n  labs(title = \"Species Counts Before and After Restoration\",\n       x = \"Time\",\n       y = \"Species Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform Wilcoxon signed-rank test\nwilcox_paired_result &lt;- wilcox.test(restoration_data$after, restoration_data$before, paired = TRUE)\n\n# Create a formatted table of the results\nwilcox_paired_table &lt;- data.frame(\n  Statistic = c(\"V-value\", \"p-value\"),\n  Value = c(\n    wilcox_paired_result$statistic,\n    format.pval(wilcox_paired_result$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(wilcox_paired_table, \n             caption = \"Wilcoxon Signed-Rank Test Results: Species Counts Before and After Restoration\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nWilcoxon Signed-Rank Test Results: Species Counts Before and After Restoration\n\n\n\nStatistic\nValue\n\n\n\n\nV\nV-value\n210\n\n\n\np-value\n8.53e-05\n\n\n\n\n\n\n\nCode\n# Create a summary statistics table\nrestoration_summary &lt;- restoration_data %&gt;%\n  summarize(\n    `Median Before` = median(before),\n    `Mean Before` = round(mean(before), 2),\n    `Median After` = median(after),\n    `Mean After` = round(mean(after), 2),\n    `Median Difference` = median(difference),\n    `Mean Difference` = round(mean(difference), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(restoration_summary, \n             caption = \"Summary Statistics: Species Counts Before and After Restoration\",\n             align = rep(\"r\", 6),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Species Counts Before and After Restoration\n\n\nMedian Before\nMean Before\nMedian After\nMean After\nMedian Difference\nMean Difference\n\n\n\n\n3.5\n4.15\n7.5\n7.65\n3\n3.5",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#confidence-intervals",
    "href": "chapters/04-hypothesis-testing.html#confidence-intervals",
    "title": "4  Hypothesis Testing",
    "section": "4.12 Confidence Intervals",
    "text": "4.12 Confidence Intervals\nConfidence intervals provide a range of plausible values for a population parameter.\n\n\nCode\n# Calculate 95% confidence interval for mean tree height in Site A\nci_result &lt;- t.test(site_A)\n\n# Create a formatted table for the confidence interval\nci_table &lt;- data.frame(\n  Statistic = c(\"Sample Mean\", \"Standard Error\", \"95% CI Lower\", \"95% CI Upper\", \"Degrees of Freedom\"),\n  Value = c(\n    round(mean(site_A), 2),\n    round(sd(site_A)/sqrt(length(site_A)), 3),\n    round(ci_result$conf.int[1], 2),\n    round(ci_result$conf.int[2], 2),\n    ci_result$parameter\n  )\n)\n\n# Display the formatted table\nknitr::kable(ci_table, \n             caption = \"95% Confidence Interval for Mean Tree Height in Site A\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\n95% Confidence Interval for Mean Tree Height in Site A\n\n\nStatistic\nValue\n\n\n\n\nSample Mean\n24.760\n\n\nStandard Error\n0.896\n\n\n95% CI Lower\n22.930\n\n\n95% CI Upper\n26.600\n\n\nDegrees of Freedom\n29.000\n\n\n\n\n\n\n\nCode\n# Visualize confidence interval\nmean_height &lt;- mean(site_A)\nci_lower &lt;- ci_result$conf.int[1]\nci_upper &lt;- ci_result$conf.int[2]\n\nggplot(data.frame(height = site_A), aes(x = height)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = mean_height, color = \"red\", size = 1) +\n  geom_vline(xintercept = ci_lower, color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = ci_upper, color = \"blue\", linetype = \"dashed\") +\n  annotate(\"rect\", xmin = ci_lower, xmax = ci_upper, ymin = 0, ymax = Inf, \n           alpha = 0.2, fill = \"lightblue\") +\n  labs(title = \"Tree Heights in Site A with 95% Confidence Interval\",\n       x = \"Height (m)\",\n       y = \"Frequency\") +\n  annotate(\"text\", x = mean_height, y = 5, \n           label = paste(\"Mean =\", round(mean_height, 2)), \n           color = \"red\", vjust = -1) +\n  annotate(\"text\", x = mean(c(ci_lower, ci_upper)), y = 3, \n           label = paste(\"95% CI: [\", round(ci_lower, 2), \", \", round(ci_upper, 2), \"]\", sep = \"\"), \n           color = \"blue\") +\n  theme_minimal()",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#exercises",
    "href": "chapters/04-hypothesis-testing.html#exercises",
    "title": "4  Hypothesis Testing",
    "section": "4.13 Exercises",
    "text": "4.13 Exercises\n\nFormulate a hypothesis about a relationship between two variables in the forest inventory dataset.\nConduct an appropriate statistical test to evaluate your hypothesis.\nCalculate the effect size for your test.\nInterpret the results, including the p-value and effect size.\nCreate a visualization that effectively communicates your findings.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#summary",
    "href": "chapters/04-hypothesis-testing.html#summary",
    "title": "4  Hypothesis Testing",
    "section": "4.14 Summary",
    "text": "4.14 Summary\nIn this chapter, we’ve covered the fundamental concepts and techniques of hypothesis testing in ecological and forestry research:\n\nFormulating null and alternative hypotheses\nUnderstanding p-values and significance levels\nRecognizing Type I and Type II errors\nCalculating and interpreting statistical power\nConducting one-sample, two-sample, and paired tests\nUsing non-parametric alternatives when necessary\nCalculating and interpreting confidence intervals\n\nThese statistical tools allow researchers to make informed inferences about populations based on sample data, helping to advance knowledge in ecology and forestry.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#statistical-power-1",
    "href": "chapters/04-hypothesis-testing.html#statistical-power-1",
    "title": "4  Hypothesis Testing",
    "section": "4.15 Statistical Power",
    "text": "4.15 Statistical Power\nStatistical power is the probability of correctly rejecting the null hypothesis when it is false. It is influenced by:\n\nSample size\nEffect size\nSignificance level (α)\nVariability in the data\n\n\n\nCode\n# Demonstrate power calculation for a t-test\nlibrary(pwr)\n\n# Calculate power for our example\neffect_size &lt;- (28 - 25) / 5  # (mean difference) / standard deviation\npower_result &lt;- pwr.t.test(\n  n = 30,                    # Sample size per group\n  d = effect_size,           # Cohen's d effect size\n  sig.level = 0.05,          # Significance level\n  type = \"two.sample\",       # Two-sample t-test\n  alternative = \"two.sided\"  # Two-sided alternative\n)\n\nprint(power_result)\n\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.6\n      sig.level = 0.05\n          power = 0.6275046\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\n# Calculate required sample size for 80% power\nsample_size_result &lt;- pwr.t.test(\n  d = effect_size,           # Cohen's d effect size\n  sig.level = 0.05,          # Significance level\n  power = 0.8,               # Desired power\n  type = \"two.sample\",       # Two-sample t-test\n  alternative = \"two.sided\"  # Two-sided alternative\n)\n\nprint(sample_size_result)\n\n\n\n     Two-sample t test power calculation \n\n              n = 44.58577\n              d = 0.6\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html",
    "href": "chapters/05-statistical-tests.html",
    "title": "5  Common Statistical Tests",
    "section": "",
    "text": "5.1 Introduction\nThis chapter explores common statistical tests used in natural sciences research. Building on the hypothesis testing framework introduced in the previous chapter, we’ll examine specific tests for different research scenarios and data types.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#choosing-the-right-statistical-test",
    "href": "chapters/05-statistical-tests.html#choosing-the-right-statistical-test",
    "title": "5  Common Statistical Tests",
    "section": "5.2 Choosing the Right Statistical Test",
    "text": "5.2 Choosing the Right Statistical Test\nSelecting the appropriate statistical test depends on several factors:\n\nResearch Question: What you’re trying to determine\nData Type: Categorical, continuous, or ordinal\nNumber of Groups: One, two, or multiple groups\nData Distribution: Normal or non-normal\nIndependence: Whether observations are independent or related\n\n\n5.2.1 Decision Tree for Common Tests\nDecision Tree for Selecting Statistical Tests:\n\nFor One Variable:\n\nOne Sample:\n\nNormal, Continuous → One-Sample t-Test\nNon-normal, Continuous → Wilcoxon Signed-Rank Test\nCategorical → Binomial Test\n\nTwo Samples:\n\nNormal, Continuous, Independent → Independent t-Test\nNormal, Continuous, Related → Paired t-Test\nNon-normal, Continuous, Independent → Mann-Whitney U Test\nNon-normal, Continuous, Related → Wilcoxon Signed-Rank Test\nCategorical, Independent → Chi-Square Test\nCategorical, Related → McNemar Test\n\nMultiple Samples:\n\nNormal, Continuous, Independent → ANOVA\nNormal, Continuous, Related → Repeated Measures ANOVA\nNon-normal, Continuous, Independent → Kruskal-Wallis Test\nNon-normal, Continuous, Related → Friedman Test\nCategorical → Chi-Square Test\n\n\nFor Two Variables:\n\nNormal, Continuous → Pearson Correlation\nNon-normal or Ordinal → Spearman Correlation\nContinuous Predictor & Outcome → Linear Regression\nContinuous Predictor, Binary Outcome → Logistic Regression\n\nFor Multiple Variables:\n\nMultiple Continuous Outcomes → MANOVA\nDimension Reduction → Principal Component Analysis\nGrouping → Cluster Analysis\n\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Selecting and Reporting Statistical Tests\n\n\n\nWhen selecting and reporting statistical tests in your research:\n\nVerify assumptions: Always check if your data meets the assumptions of the test (normality, homogeneity of variance, independence)\nReport assumption tests: Include results of normality tests or variance tests when presenting your findings\nConsider transformations: If data violates assumptions, consider appropriate transformations (log, square root, etc.) before switching to non-parametric tests\nEffect sizes matter: Always report effect sizes (Cohen’s d, r, η²) alongside p-values\nUse consistent formatting: Present similar tests in the same format throughout your paper\nJustify your choices: Briefly explain why you selected a particular test, especially for complex analyses\nConsult statisticians early: For complex study designs, consult with a statistician during the planning phase, not after data collection",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#parametric-vs.-non-parametric-tests",
    "href": "chapters/05-statistical-tests.html#parametric-vs.-non-parametric-tests",
    "title": "5  Common Statistical Tests",
    "section": "5.3 Parametric vs. Non-Parametric Tests",
    "text": "5.3 Parametric vs. Non-Parametric Tests\n\n5.3.1 Parametric Tests\nParametric tests make assumptions about the underlying population distribution, typically that the data follows a normal distribution. Common parametric tests include:\n\nt-tests\nANOVA\nPearson correlation\nLinear regression\n\n\n\n5.3.2 Non-Parametric Tests\nNon-parametric tests make fewer assumptions about the population distribution and are useful when data doesn’t meet the assumptions of parametric tests. Common non-parametric tests include:\n\nMann-Whitney U test\nWilcoxon signed-rank test\nKruskal-Wallis test\nSpearman correlation\n\n\n\n5.3.3 Checking Assumptions\nBefore applying a parametric test, it’s essential to check if your data meets the necessary assumptions. Let’s use our crop yield dataset to demonstrate:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the crop yield dataset\ncrop_yields &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# View column names to see how R has formatted them\nnames(crop_yields)\n\n\n [1] \"Entity\"                           \"Code\"                            \n [3] \"Year\"                             \"Wheat (tonnes per hectare)\"      \n [5] \"Rice (tonnes per hectare)\"        \"Maize (tonnes per hectare)\"      \n [7] \"Soybeans (tonnes per hectare)\"    \"Potatoes (tonnes per hectare)\"   \n [9] \"Beans (tonnes per hectare)\"       \"Peas (tonnes per hectare)\"       \n[11] \"Cassava (tonnes per hectare)\"     \"Barley (tonnes per hectare)\"     \n[13] \"Cocoa beans (tonnes per hectare)\" \"Bananas (tonnes per hectare)\"    \n\n\nCode\n# Extract wheat yields for analysis\nwheat_yields &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`)\n\n# View the first few rows\nknitr::kable(head(wheat_yields), \n             caption = \"Sample of Wheat Yield Data\",\n             align = c(\"l\", \"c\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSample of Wheat Yield Data\n\n\nEntity\nYear\nWheat (tonnes per hectare)\n\n\n\n\nAfghanistan\n1961\n1.0220\n\n\nAfghanistan\n1962\n0.9735\n\n\nAfghanistan\n1963\n0.8317\n\n\nAfghanistan\n1964\n0.9510\n\n\nAfghanistan\n1965\n0.9723\n\n\nAfghanistan\n1966\n0.8666\n\n\n\n\n\n\n\nCode\n# Check for normality\n# Visual methods\npar(mfrow = c(1, 2))\nhist(wheat_yields$`Wheat (tonnes per hectare)`, main = \"Histogram of Wheat Yields\", xlab = \"Yield (tonnes/hectare)\")\nqqnorm(wheat_yields$`Wheat (tonnes per hectare)`); qqline(wheat_yields$`Wheat (tonnes per hectare)`, col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\n# Statistical test for normality\nshapiro_result &lt;- shapiro.test(sample(wheat_yields$`Wheat (tonnes per hectare)`, min(5000, length(wheat_yields$`Wheat (tonnes per hectare)`))))\n\n# Create a formatted table of the results\nshapiro_table &lt;- data.frame(\n  Statistic = c(\"W-value\", \"p-value\"),\n  Value = c(\n    round(shapiro_result$statistic, 2),\n    format.pval(shapiro_result$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(shapiro_table, \n             caption = \"Shapiro-Wilk Normality Test Results: Wheat Yields\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nShapiro-Wilk Normality Test Results: Wheat Yields\n\n\n\nStatistic\nValue\n\n\n\n\nW\nW-value\n0.87\n\n\n\np-value\n&lt;2e-16",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-comparing-groups",
    "href": "chapters/05-statistical-tests.html#tests-for-comparing-groups",
    "title": "5  Common Statistical Tests",
    "section": "5.4 Tests for Comparing Groups",
    "text": "5.4 Tests for Comparing Groups\n\n5.4.1 t-Tests\n\n5.4.1.1 Independent Samples t-Test\nUsed to compare means between two independent groups. Let’s compare wheat yields between two time periods:\n\n\nCode\n# Create two groups: early period (before 2000) and recent period (2000 onwards)\ncrop_yields_grouped &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & Year &gt;= 1960) %&gt;%\n  mutate(period = ifelse(Year &lt; 2000, \"Early Period (pre-2000)\", \"Recent Period (2000+)\"))\n\n# Visualize the data\nggplot(crop_yields_grouped, aes(x = period, y = `Wheat (tonnes per hectare)`, fill = period)) +\n  geom_boxplot() +\n  labs(title = \"Wheat Yields by Time Period\",\n       x = \"Period\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# Perform independent samples t-test using formula interface with backticks\nt_test_result &lt;- t.test(`Wheat (tonnes per hectare)` ~ period, data = crop_yields_grouped)\n\n# Create a formatted table of the results\nt_test_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(t_test_result$statistic, 3),\n    round(t_test_result$parameter, 1),\n    format.pval(t_test_result$p.value, digits = 3),\n    round(diff(t_test_result$estimate), 2),\n    round(t_test_result$conf.int[1], 2),\n    round(t_test_result$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(t_test_table, \n             caption = \"Independent Samples t-Test Results: Wheat Yields by Time Period\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nIndependent Samples t-Test Results: Wheat Yields by Time Period\n\n\nStatistic\nValue\n\n\n\n\nt-value\n-22.335\n\n\nDegrees of Freedom\n4970.2\n\n\np-value\n&lt;2e-16\n\n\nMean Difference\n0.9\n\n\n95% CI Lower\n-0.98\n\n\n95% CI Upper\n-0.82\n\n\n\n\n\n\n\nCode\n# Summary statistics by period\nperiod_summary &lt;- crop_yields_grouped %&gt;%\n  group_by(period) %&gt;%\n  summarize(\n    n = n(),\n    Mean = round(mean(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    SD = round(sd(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Min = round(min(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Max = round(max(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(period_summary, \n             caption = \"Summary Statistics: Wheat Yields by Time Period\",\n             align = c(\"l\", \"c\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Wheat Yields by Time Period\n\n\nperiod\nn\nMean\nSD\nMin\nMax\n\n\n\n\nEarly Period (pre-2000)\n5177\n2.11\n1.47\n0.05\n9.00\n\n\nRecent Period (2000+)\n2924\n3.01\n1.88\n0.00\n10.67\n\n\n\n\n\n\n\n\n\n5.4.1.2 Paired Samples t-Test\nUsed to compare means between two related groups. Let’s compare wheat and rice yields for the same countries and years:\n\n\nCode\n# Prepare data for paired t-test\npaired_data &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Rice (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`)\n\n# View the first few rows\nknitr::kable(head(paired_data), \n             caption = \"Sample of Paired Crop Yield Data\",\n             align = c(\"l\", \"c\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSample of Paired Crop Yield Data\n\n\nEntity\nYear\nWheat (tonnes per hectare)\nRice (tonnes per hectare)\n\n\n\n\nAfghanistan\n1961\n1.0220\n1.5190\n\n\nAfghanistan\n1962\n0.9735\n1.5190\n\n\nAfghanistan\n1963\n0.8317\n1.5190\n\n\nAfghanistan\n1964\n0.9510\n1.7273\n\n\nAfghanistan\n1965\n0.9723\n1.7273\n\n\nAfghanistan\n1966\n0.8666\n1.5180\n\n\n\n\n\n\n\nCode\n# Visualize the paired data\npaired_data_long &lt;- paired_data %&gt;%\n  pivot_longer(cols = c(`Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`), names_to = \"Crop\", values_to = \"Yield\")\n\nggplot(paired_data_long, aes(x = Crop, y = Yield, fill = Crop)) +\n  geom_boxplot() +\n  labs(title = \"Comparison of Wheat and Rice Yields\",\n       x = \"Crop Type\",\n       y = \"Yield (tonnes/hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform paired t-test using vectors directly\npaired_t_test &lt;- t.test(\n  paired_data$`Wheat (tonnes per hectare)`, \n  paired_data$`Rice (tonnes per hectare)`, \n  paired = TRUE\n)\n\n# Create a formatted table of the results\npaired_t_test_table &lt;- data.frame(\n  Statistic = c(\"t-value\", \"Degrees of Freedom\", \"p-value\", \"Mean Difference\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(paired_t_test$statistic, 3),\n    round(paired_t_test$parameter, 1),\n    format.pval(paired_t_test$p.value, digits = 3),\n    round(mean(paired_data$`Wheat (tonnes per hectare)` - paired_data$`Rice (tonnes per hectare)`, na.rm = TRUE), 2),\n    round(paired_t_test$conf.int[1], 2),\n    round(paired_t_test$conf.int[2], 2)\n  )\n)\n\n# Display the formatted table\nknitr::kable(paired_t_test_table, \n             caption = \"Paired Samples t-Test Results: Wheat vs. Rice Yields\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nPaired Samples t-Test Results: Wheat vs. Rice Yields\n\n\nStatistic\nValue\n\n\n\n\nt-value\n-61.854\n\n\nDegrees of Freedom\n5725\n\n\np-value\n&lt;2e-16\n\n\nMean Difference\n-1.52\n\n\n95% CI Lower\n-1.57\n\n\n95% CI Upper\n-1.47\n\n\n\n\n\n\n\nCode\n# Create a summary statistics table\npaired_summary &lt;- paired_data %&gt;%\n  summarize(\n    n = n(),\n    `Mean Wheat` = round(mean(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    `SD Wheat` = round(sd(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    `Mean Rice` = round(mean(`Rice (tonnes per hectare)`, na.rm = TRUE), 2),\n    `SD Rice` = round(sd(`Rice (tonnes per hectare)`, na.rm = TRUE), 2),\n    `Mean Difference` = round(mean(`Wheat (tonnes per hectare)` - `Rice (tonnes per hectare)`, na.rm = TRUE), 2),\n    `SD Difference` = round(sd(`Wheat (tonnes per hectare)` - `Rice (tonnes per hectare)`, na.rm = TRUE), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(paired_summary, \n             caption = \"Summary Statistics: Wheat vs. Rice Yields\",\n             align = rep(\"r\", 7),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Wheat vs. Rice Yields\n\n\nn\nMean Wheat\nSD Wheat\nMean Rice\nSD Rice\nMean Difference\nSD Difference\n\n\n\n\n5726\n2.04\n1.26\n3.55\n1.95\n-1.52\n1.86\n\n\n\n\n\n\n\n\n\n\n5.4.2 Analysis of Variance (ANOVA)\nANOVA is used to compare means among three or more independent groups. Let’s compare crop yields across different continents:\n\n\nCode\n# Create a mapping of countries to continents (simplified for demonstration)\ncontinent_mapping &lt;- tibble(\n  Entity = c(\"United States\", \"Canada\", \"Mexico\", \n             \"China\", \"India\", \"Japan\", \n             \"Germany\", \"France\", \"United Kingdom\", \n             \"Brazil\", \"Argentina\", \"Chile\",\n             \"Egypt\", \"Nigeria\", \"South Africa\",\n             \"Australia\", \"New Zealand\"),\n  Continent = c(rep(\"North America\", 3), \n                rep(\"Asia\", 3), \n                rep(\"Europe\", 3), \n                rep(\"South America\", 3),\n                rep(\"Africa\", 3),\n                rep(\"Oceania\", 2))\n)\n\n# Join with crop yields data\ncontinental_yields &lt;- crop_yields %&gt;%\n  inner_join(continent_mapping, by = \"Entity\") %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & Year &gt;= 2000)\n\n# Visualize wheat yields by continent\nggplot(continental_yields, aes(x = Continent, y = `Wheat (tonnes per hectare)`, fill = Continent)) +\n  geom_boxplot() +\n  labs(title = \"Wheat Yields by Continent (2000-present)\",\n       x = \"Continent\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Perform ANOVA\nanova_result &lt;- aov(`Wheat (tonnes per hectare)` ~ Continent, data = continental_yields)\nanova_summary &lt;- summary(anova_result)\n\n# Create a formatted ANOVA table\nanova_df &lt;- data.frame(\n  Source = c(\"Continent\", \"Residuals\"),\n  DF = c(anova_summary[[1]][[\"Df\"]][1], anova_summary[[1]][[\"Df\"]][2]),\n  `Sum Sq` = c(round(anova_summary[[1]][[\"Sum Sq\"]][1], 2), round(anova_summary[[1]][[\"Sum Sq\"]][2], 2)),\n  `Mean Sq` = c(round(anova_summary[[1]][[\"Mean Sq\"]][1], 2), round(anova_summary[[1]][[\"Mean Sq\"]][2], 2)),\n  `F value` = c(round(anova_summary[[1]][[\"F value\"]][1], 2), NA),\n  `Pr(&gt;F)` = c(format.pval(anova_summary[[1]][[\"Pr(&gt;F)\"]][1], digits = 3), NA)\n)\n\n# Display the ANOVA table\nknitr::kable(anova_df, \n             caption = \"ANOVA Results: Wheat Yields by Continent\",\n             align = c(\"l\", \"c\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nANOVA Results: Wheat Yields by Continent\n\n\nSource\nDF\nSum.Sq\nMean.Sq\nF.value\nPr..F.\n\n\n\n\nContinent\n5\n698.63\n139.73\n48.64\n&lt;2e-16\n\n\nResiduals\n317\n910.68\n2.87\nNA\nNA\n\n\n\n\n\n\n\nCode\n# Post-hoc test to identify which groups differ\ntukey_result &lt;- TukeyHSD(anova_result)\n\n# Convert Tukey HSD results to a data frame\ntukey_df &lt;- as.data.frame(tukey_result$Continent)\ntukey_df$comparison &lt;- rownames(tukey_df)\ntukey_long &lt;- pivot_longer(tukey_df, \n                           cols = -comparison, \n                           names_to = \"Continent2\", \n                           values_to = \"p_value\")\ntukey_long &lt;- tukey_long %&gt;%\n  filter(!is.na(p_value)) %&gt;%\n  mutate(\n    Comparison = paste(comparison, \"vs\", Continent2),\n    `p adj` = format.pval(p_value, digits = 3),\n    Significant = ifelse(p_value &lt; 0.05, \"Yes\", \"No\")\n  ) %&gt;%\n  select(Comparison, `p adj`, Significant)\n\n# Display the Tukey HSD results\nknitr::kable(tukey_long, \n             caption = \"Tukey HSD Post-hoc Test Results: Pairwise Comparisons of Continents\",\n             align = c(\"l\", \"c\", \"c\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nTukey HSD Post-hoc Test Results: Pairwise Comparisons of Continents\n\n\nComparison\np adj\nSignificant\n\n\n\n\nAsia-Africa vs diff\n0.277598\nNo\n\n\nAsia-Africa vs lwr\n&lt; 2e-16\nYes\n\n\nAsia-Africa vs upr\n1.187921\nNo\n\n\nAsia-Africa vs p adj\n0.952392\nNo\n\n\nEurope-Africa vs diff\n3.894058\nNo\n\n\nEurope-Africa vs lwr\n2.983736\nNo\n\n\nEurope-Africa vs upr\n4.804380\nNo\n\n\nEurope-Africa vs p adj\n1.08e-12\nYes\n\n\nNorth America-Africa vs diff\n0.060698\nNo\n\n\nNorth America-Africa vs lwr\n&lt; 2e-16\nYes\n\n\nNorth America-Africa vs upr\n0.971021\nNo\n\n\nNorth America-Africa vs p adj\n0.999965\nNo\n\n\nOceania-Africa vs diff\n1.368446\nNo\n\n\nOceania-Africa vs lwr\n0.350675\nNo\n\n\nOceania-Africa vs upr\n2.386218\nNo\n\n\nOceania-Africa vs p adj\n0.001931\nYes\n\n\nSouth America-Africa vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-Africa vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-Africa vs upr\n0.692550\nNo\n\n\nSouth America-Africa vs p adj\n0.983428\nNo\n\n\nEurope-Asia vs diff\n3.616460\nNo\n\n\nEurope-Asia vs lwr\n2.706137\nNo\n\n\nEurope-Asia vs upr\n4.526782\nNo\n\n\nEurope-Asia vs p adj\n1.09e-12\nYes\n\n\nNorth America-Asia vs diff\n&lt; 2e-16\nYes\n\n\nNorth America-Asia vs lwr\n&lt; 2e-16\nYes\n\n\nNorth America-Asia vs upr\n0.693422\nNo\n\n\nNorth America-Asia vs p adj\n0.983724\nNo\n\n\nOceania-Asia vs diff\n1.090848\nNo\n\n\nOceania-Asia vs lwr\n0.073077\nNo\n\n\nOceania-Asia vs upr\n2.108620\nNo\n\n\nOceania-Asia vs p adj\n0.027650\nYes\n\n\nSouth America-Asia vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-Asia vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-Asia vs upr\n0.414952\nNo\n\n\nSouth America-Asia vs p adj\n0.625365\nNo\n\n\nNorth America-Europe vs diff\n&lt; 2e-16\nYes\n\n\nNorth America-Europe vs lwr\n&lt; 2e-16\nYes\n\n\nNorth America-Europe vs upr\n&lt; 2e-16\nYes\n\n\nNorth America-Europe vs p adj\n1.08e-12\nYes\n\n\nOceania-Europe vs diff\n&lt; 2e-16\nYes\n\n\nOceania-Europe vs lwr\n&lt; 2e-16\nYes\n\n\nOceania-Europe vs upr\n&lt; 2e-16\nYes\n\n\nOceania-Europe vs p adj\n1.13e-10\nYes\n\n\nSouth America-Europe vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-Europe vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-Europe vs upr\n&lt; 2e-16\nYes\n\n\nSouth America-Europe vs p adj\n1.08e-12\nYes\n\n\nOceania-North America vs diff\n1.307748\nNo\n\n\nOceania-North America vs lwr\n0.289977\nNo\n\n\nOceania-North America vs upr\n2.325520\nNo\n\n\nOceania-North America vs p adj\n0.003642\nYes\n\n\nSouth America-North America vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-North America vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-North America vs upr\n0.631852\nNo\n\n\nSouth America-North America vs p adj\n0.951763\nNo\n\n\nSouth America-Oceania vs diff\n&lt; 2e-16\nYes\n\n\nSouth America-Oceania vs lwr\n&lt; 2e-16\nYes\n\n\nSouth America-Oceania vs upr\n&lt; 2e-16\nYes\n\n\nSouth America-Oceania vs p adj\n0.000159\nYes\n\n\n\n\n\n\n\nCode\n# Create a summary statistics table by continent\ncontinent_summary &lt;- continental_yields %&gt;%\n  group_by(Continent) %&gt;%\n  summarize(\n    n = n(),\n    Mean = round(mean(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    SD = round(sd(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Min = round(min(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2),\n    Max = round(max(`Wheat (tonnes per hectare)`, na.rm = TRUE), 2)\n  )\n\n# Display the summary statistics table\nknitr::kable(continent_summary, \n             caption = \"Summary Statistics: Wheat Yields by Continent\",\n             align = c(\"l\", \"c\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSummary Statistics: Wheat Yields by Continent\n\n\nContinent\nn\nMean\nSD\nMin\nMax\n\n\n\n\nAfrica\n57\n3.54\n2.24\n0.79\n6.86\n\n\nAsia\n57\n3.82\n0.86\n2.60\n5.48\n\n\nEurope\n57\n7.43\n0.66\n5.29\n8.98\n\n\nNorth America\n57\n3.60\n1.12\n1.83\n5.66\n\n\nOceania\n38\n4.91\n3.26\n0.91\n9.86\n\n\nSouth America\n57\n3.32\n1.34\n1.48\n6.21\n\n\n\n\n\n\n\n\n\n5.4.3 Non-Parametric Alternatives\n\n5.4.3.1 Mann-Whitney U Test\nThe Mann-Whitney U test (also called Wilcoxon rank-sum test) is a non-parametric alternative to the independent samples t-test:\n\n\nCode\n# Using the same time period groups as before\nwilcox_test &lt;- wilcox.test(`Wheat (tonnes per hectare)` ~ period, data = crop_yields_grouped)\n\n# Create a formatted table of the results\nwilcox_table &lt;- data.frame(\n  Statistic = c(\"W-value\", \"p-value\"),\n  Value = c(\n    wilcox_test$statistic,\n    format.pval(wilcox_test$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(wilcox_table, \n             caption = \"Mann-Whitney U Test Results: Wheat Yields by Time Period\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nMann-Whitney U Test Results: Wheat Yields by Time Period\n\n\n\nStatistic\nValue\n\n\n\n\nW\nW-value\n5031267.5\n\n\n\np-value\n&lt;2e-16\n\n\n\n\n\n\n\n\n\n5.4.3.2 Kruskal-Wallis Test\nThe Kruskal-Wallis test is a non-parametric alternative to ANOVA:\n\n\nCode\n# Using the same continental data as before\nkruskal_result &lt;- kruskal.test(`Wheat (tonnes per hectare)` ~ Continent, data = continental_yields)\n\n# Create a formatted table of the results\nkruskal_table &lt;- data.frame(\n  Statistic = c(\"Chi-squared\", \"Degrees of Freedom\", \"p-value\"),\n  Value = c(\n    round(kruskal_result$statistic, 2),\n    kruskal_result$parameter,\n    format.pval(kruskal_result$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(kruskal_table, \n             caption = \"Kruskal-Wallis Test Results: Wheat Yields by Continent\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nKruskal-Wallis Test Results: Wheat Yields by Continent\n\n\n\nStatistic\nValue\n\n\n\n\nKruskal-Wallis chi-squared\nChi-squared\n120.17\n\n\ndf\nDegrees of Freedom\n5\n\n\n\np-value\n&lt;2e-16\n\n\n\n\n\n\n\nCode\n# Post-hoc test for Kruskal-Wallis\nif(requireNamespace(\"dunn.test\", quietly = TRUE)) {\n  library(dunn.test)\n  dunn_result &lt;- dunn.test(continental_yields$`Wheat (tonnes per hectare)`, continental_yields$Continent, method = \"bonferroni\", kw = TRUE)\n  \n  # Create a data frame from the dunn test results\n  dunn_df &lt;- data.frame(\n    Comparison = dunn_result$comparisons,\n    `Z statistic` = round(dunn_result$Z, 2),\n    `P value` = format.pval(dunn_result$P, digits = 3),\n    `Adjusted P` = format.pval(dunn_result$P.adjusted, digits = 3),\n    Significant = ifelse(dunn_result$P.adjusted &lt; 0.05, \"Yes\", \"No\")\n  )\n  \n  # Display the formatted dunn test results\n  knitr::kable(dunn_df, \n               caption = \"Dunn's Post-hoc Test Results: Pairwise Comparisons of Continents\",\n               align = c(\"l\", \"r\", \"c\", \"c\", \"c\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                             full_width = FALSE,\n                             position = \"center\")\n} else {\n  # Alternative: pairwise Wilcoxon tests\n  pairwise_result &lt;- pairwise.wilcox.test(continental_yields$`Wheat (tonnes per hectare)`, continental_yields$Continent, \n                                         p.adjust.method = \"bonferroni\")\n  \n  # Convert matrix to data frame\n  pairwise_df &lt;- as.data.frame(pairwise_result$p.value)\n  pairwise_df$Continent1 &lt;- rownames(pairwise_df)\n  pairwise_long &lt;- pivot_longer(pairwise_df, \n                               cols = -Continent1, \n                               names_to = \"Continent2\", \n                               values_to = \"p_value\")\n  \n  # Filter out NA values and format\n  pairwise_long &lt;- pairwise_long %&gt;%\n    filter(!is.na(p_value)) %&gt;%\n    mutate(\n      Comparison = paste(Continent1, \"vs\", Continent2),\n      `P value` = format.pval(p_value, digits = 3),\n      Significant = ifelse(p_value &lt; 0.05, \"Yes\", \"No\")\n    ) %&gt;%\n    select(Comparison, `P value`, Significant)\n  \n  # Display the formatted pairwise Wilcoxon test results\n  knitr::kable(pairwise_long, \n               caption = \"Pairwise Wilcoxon Test Results with Bonferroni Correction\",\n               align = c(\"l\", \"c\", \"c\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                             full_width = FALSE,\n                             position = \"center\")\n}\n\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 120.1715, df = 5, p-value = 0\n\n                           Comparison of x by group                            \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |     Africa       Asia     Europe   North Am    Oceania\n---------+-------------------------------------------------------\n    Asia |  -1.814776\n         |     0.5217\n         |\n  Europe |  -9.079400  -7.264623\n         |    0.0000*    0.0000*\n         |\nNorth Am |  -0.948758   0.866018   8.130641\n         |     1.0000     1.0000    0.0000*\n         |\n Oceania |  -2.181366  -0.558180   5.939496  -1.332770\n         |     0.2187     1.0000    0.0000*     1.0000\n         |\nSouth Am |   0.300874   2.115651   9.380275   1.249633   2.450476\n         |     1.0000     0.2578    0.0000*     1.0000     0.1070\n\nalpha = 0.05\nReject Ho if p &lt;= alpha/2\n\n\n\nDunn's Post-hoc Test Results: Pairwise Comparisons of Continents\n\n\nComparison\nZ.statistic\nP.value\nAdjusted.P\nSignificant\n\n\n\n\nAfrica - Asia\n-1.81\n0.03478\n0.522\nNo\n\n\nAfrica - Europe\n-9.08\n&lt; 2e-16\n&lt; 2e-16\nYes\n\n\nAsia - Europe\n-7.26\n1.87e-13\n2.81e-12\nYes\n\n\nAfrica - North America\n-0.95\n0.17137\n1.000\nNo\n\n\nAsia - North America\n0.87\n0.19324\n1.000\nNo\n\n\nEurope - North America\n8.13\n&lt; 2e-16\n3.20e-15\nYes\n\n\nAfrica - Oceania\n-2.18\n0.01458\n0.219\nNo\n\n\nAsia - Oceania\n-0.56\n0.28836\n1.000\nNo\n\n\nEurope - Oceania\n5.94\n1.43e-09\n2.14e-08\nYes\n\n\nNorth America - Oceania\n-1.33\n0.09130\n1.000\nNo\n\n\nAfrica - South America\n0.30\n0.38175\n1.000\nNo\n\n\nAsia - South America\n2.12\n0.01719\n0.258\nNo\n\n\nEurope - South America\n9.38\n&lt; 2e-16\n&lt; 2e-16\nYes\n\n\nNorth America - South America\n1.25\n0.10572\n1.000\nNo\n\n\nOceania - South America\n2.45\n0.00713\n0.107\nNo",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-relationships",
    "href": "chapters/05-statistical-tests.html#tests-for-relationships",
    "title": "5  Common Statistical Tests",
    "section": "5.5 Tests for Relationships",
    "text": "5.5 Tests for Relationships\n\n5.5.1 Correlation Analysis\n\n5.5.1.1 Pearson Correlation\nPearson correlation measures the linear relationship between two continuous variables:\n\n\nCode\n# Examine correlation between wheat and maize yields\ncrop_correlation &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Maize (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Maize (tonnes per hectare)`)\n\n# Visualize the relationship\nggplot(crop_correlation, aes(x = `Wheat (tonnes per hectare)`, y = `Maize (tonnes per hectare)`)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Relationship Between Wheat and Maize Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Maize Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Calculate Pearson correlation\ncor_result &lt;- cor.test(crop_correlation$`Wheat (tonnes per hectare)`, crop_correlation$`Maize (tonnes per hectare)`, method = \"pearson\")\n\n# Create a formatted table of the results\ncor_table &lt;- data.frame(\n  Statistic = c(\"Correlation Coefficient (r)\", \"t-value\", \"Degrees of Freedom\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"),\n  Value = c(\n    round(cor_result$estimate, 3),\n    round(cor_result$statistic, 2),\n    cor_result$parameter,\n    format.pval(cor_result$p.value, digits = 3),\n    round(cor_result$conf.int[1], 3),\n    round(cor_result$conf.int[2], 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(cor_table, \n             caption = \"Pearson Correlation Results: Wheat and Maize Yields\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nPearson Correlation Results: Wheat and Maize Yields\n\n\nStatistic\nValue\n\n\n\n\nCorrelation Coefficient (r)\n0.501\n\n\nt-value\n49.75\n\n\nDegrees of Freedom\n7378\n\n\np-value\n&lt;2e-16\n\n\n95% CI Lower\n0.484\n\n\n95% CI Upper\n0.518\n\n\n\n\n\n\n\n\n\n5.5.1.2 Spearman Correlation\nSpearman correlation is a non-parametric measure of rank correlation:\n\n\nCode\n# Calculate Spearman correlation\nspearman_result &lt;- cor.test(crop_correlation$`Wheat (tonnes per hectare)`, crop_correlation$`Maize (tonnes per hectare)`, method = \"spearman\")\n\n# Create a formatted table of the results\nspearman_table &lt;- data.frame(\n  Statistic = c(\"Correlation Coefficient (rho)\", \"S-value\", \"p-value\"),\n  Value = c(\n    round(spearman_result$estimate, 3),\n    format(spearman_result$statistic, scientific = FALSE),\n    format.pval(spearman_result$p.value, digits = 3)\n  )\n)\n\n# Display the formatted table\nknitr::kable(spearman_table, \n             caption = \"Spearman Correlation Results: Wheat and Maize Yields\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nSpearman Correlation Results: Wheat and Maize Yields\n\n\n\nStatistic\nValue\n\n\n\n\nrho\nCorrelation Coefficient (rho)\n0.633\n\n\nS\nS-value\n24618249591\n\n\n\np-value\n&lt;2e-16\n\n\n\n\n\n\n\nCode\n# Create a comparison table of correlation methods\ncorrelation_comparison &lt;- data.frame(\n  `Correlation Method` = c(\"Pearson\", \"Spearman\"),\n  `Correlation Coefficient` = c(round(cor_result$estimate, 3), round(spearman_result$estimate, 3)),\n  `p-value` = c(format.pval(cor_result$p.value, digits = 3), format.pval(spearman_result$p.value, digits = 3)),\n  `Interpretation` = c(\n    ifelse(abs(cor_result$estimate) &gt; 0.7, \"Strong\", ifelse(abs(cor_result$estimate) &gt; 0.3, \"Moderate\", \"Weak\")),\n    ifelse(abs(spearman_result$estimate) &gt; 0.7, \"Strong\", ifelse(abs(spearman_result$estimate) &gt; 0.3, \"Moderate\", \"Weak\"))\n  )\n)\n\n# Display the comparison table\nknitr::kable(correlation_comparison, \n             caption = \"Comparison of Correlation Methods\",\n             align = c(\"l\", \"c\", \"c\", \"c\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nComparison of Correlation Methods\n\n\n\nCorrelation.Method\nCorrelation.Coefficient\np.value\nInterpretation\n\n\n\n\ncor\nPearson\n0.501\n&lt;2e-16\nModerate\n\n\nrho\nSpearman\n0.633\n&lt;2e-16\nModerate\n\n\n\n\n\n\n\n\n\n\n5.5.2 Regression Analysis\n\n5.5.2.1 Linear Regression\nLinear regression models the relationship between a dependent variable and one or more independent variables:\n\n\nCode\n# Create a dataset with year as predictor for wheat yields\ntime_series_data &lt;- crop_yields %&gt;%\n  filter(Entity == \"United States\" & !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  arrange(Year)\n\n# Visualize the trend\nggplot(time_series_data, aes(x = Year, y = `Wheat (tonnes per hectare)`)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Wheat Yield Trends in the United States\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform linear regression\nlm_model &lt;- lm(`Wheat (tonnes per hectare)` ~ Year, data = time_series_data)\nlm_summary &lt;- summary(lm_model)\n\n# Create a formatted table of the regression coefficients\ncoef_table &lt;- data.frame(\n  Term = c(\"(Intercept)\", \"Year\"),\n  Estimate = c(round(lm_summary$coefficients[1, 1], 3), round(lm_summary$coefficients[2, 1], 3)),\n  `Std. Error` = c(round(lm_summary$coefficients[1, 2], 3), round(lm_summary$coefficients[2, 2], 3)),\n  `t value` = c(round(lm_summary$coefficients[1, 3], 2), round(lm_summary$coefficients[2, 3], 2)),\n  `Pr(&gt;|t|)` = c(format.pval(lm_summary$coefficients[1, 4], digits = 3), format.pval(lm_summary$coefficients[2, 4], digits = 3))\n)\n\n# Display the coefficients table\nknitr::kable(coef_table, \n             caption = \"Linear Regression Coefficients: Wheat Yield by Year\",\n             align = c(\"l\", \"r\", \"r\", \"r\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nLinear Regression Coefficients: Wheat Yield by Year\n\n\nTerm\nEstimate\nStd..Error\nt.value\nPr...t..\n\n\n\n\n(Intercept)\n-48.466\n2.571\n-18.85\n&lt;2e-16\n\n\nYear\n0.026\n0.001\n19.81\n&lt;2e-16\n\n\n\n\n\n\n\nCode\n# Create a formatted table of the model summary statistics\nmodel_stats &lt;- data.frame(\n  Statistic = c(\"R-squared\", \"Adjusted R-squared\", \"F-statistic\", \"DF\", \"p-value\", \"Residual Standard Error\"),\n  Value = c(\n    round(lm_summary$r.squared, 3),\n    round(lm_summary$adj.r.squared, 3),\n    round(lm_summary$fstatistic[1], 2),\n    paste(lm_summary$fstatistic[2], \",\", lm_summary$fstatistic[3]),\n    format.pval(pf(lm_summary$fstatistic[1], lm_summary$fstatistic[2], lm_summary$fstatistic[3], lower.tail = FALSE), digits = 3),\n    round(lm_summary$sigma, 3)\n  )\n)\n\n# Display the model summary statistics table\nknitr::kable(model_stats, \n             caption = \"Linear Regression Model Summary Statistics\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nLinear Regression Model Summary Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.875\n\n\nAdjusted R-squared\n0.873\n\n\nF-statistic\n392.48\n\n\nDF\n1 , 56\n\n\np-value\n&lt;2e-16\n\n\nResidual Standard Error\n0.165\n\n\n\n\n\n\n\nCode\n# Check assumptions\npar(mfrow = c(2, 2))\nplot(lm_model)\n\n\n\n\n\n\n\n\n\n\n\n5.5.2.2 Multiple Regression\nMultiple regression includes more than one predictor variable:\n\n\nCode\n# Create a dataset with multiple predictors\nmulti_crop_data &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Rice (tonnes per hectare)`) & !is.na(`Maize (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`, `Maize (tonnes per hectare)`)\n\n# Perform multiple regression\nmulti_model &lt;- lm(`Wheat (tonnes per hectare)` ~ `Rice (tonnes per hectare)` + `Maize (tonnes per hectare)` + Year, data = multi_crop_data)\nmulti_summary &lt;- summary(multi_model)\n\n# Create a formatted table of the regression coefficients\nmulti_coef_table &lt;- data.frame(\n  Term = c(\"(Intercept)\", \"Rice (tonnes per hectare)\", \"Maize (tonnes per hectare)\", \"Year\"),\n  Estimate = round(multi_summary$coefficients[, 1], 3),\n  `Std. Error` = round(multi_summary$coefficients[, 2], 3),\n  `t value` = round(multi_summary$coefficients[, 3], 2),\n  `Pr(&gt;|t|)` = format.pval(multi_summary$coefficients[, 4], digits = 3),\n  Significance = ifelse(multi_summary$coefficients[, 4] &lt; 0.001, \"***\", \n                       ifelse(multi_summary$coefficients[, 4] &lt; 0.01, \"**\", \n                              ifelse(multi_summary$coefficients[, 4] &lt; 0.05, \"*\", \n                                     ifelse(multi_summary$coefficients[, 4] &lt; 0.1, \".\", \"\"))))\n)\n\n# Display the coefficients table\nknitr::kable(multi_coef_table, \n             caption = \"Multiple Regression Coefficients: Predicting Wheat Yield\",\n             align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"c\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\") %&gt;%\n  kableExtra::add_footnote(\"Significance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\", notation = \"none\")\n\n\n\nMultiple Regression Coefficients: Predicting Wheat Yield\n\n\n\nTerm\nEstimate\nStd..Error\nt.value\nPr...t..\nSignificance\n\n\n\n\n(Intercept)\n(Intercept)\n-21.612\n1.776\n-12.17\n&lt;2e-16\n***\n\n\n`Rice (tonnes per hectare)`\nRice (tonnes per hectare)\n-0.005\n0.010\n-0.56\n0.577\n\n\n\n`Maize (tonnes per hectare)`\nMaize (tonnes per hectare)\n0.279\n0.009\n32.78\n&lt;2e-16\n***\n\n\nYear\nYear\n0.011\n0.001\n12.81\n&lt;2e-16\n***\n\n\n\n Significance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a formatted table of the model summary statistics\nmulti_model_stats &lt;- data.frame(\n  Statistic = c(\"R-squared\", \"Adjusted R-squared\", \"F-statistic\", \"DF\", \"p-value\", \"Residual Standard Error\"),\n  Value = c(\n    round(multi_summary$r.squared, 3),\n    round(multi_summary$adj.r.squared, 3),\n    round(multi_summary$fstatistic[1], 2),\n    paste(multi_summary$fstatistic[2], \",\", multi_summary$fstatistic[3]),\n    format.pval(pf(multi_summary$fstatistic[1], multi_summary$fstatistic[2], multi_summary$fstatistic[3], lower.tail = FALSE), digits = 3),\n    round(multi_summary$sigma, 3)\n  )\n)\n\n# Display the model summary statistics table\nknitr::kable(multi_model_stats, \n             caption = \"Multiple Regression Model Summary Statistics\",\n             align = c(\"l\", \"r\"),\n             format = \"html\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                           full_width = FALSE,\n                           position = \"center\")\n\n\n\nMultiple Regression Model Summary Statistics\n\n\nStatistic\nValue\n\n\n\n\nR-squared\n0.341\n\n\nAdjusted R-squared\n0.341\n\n\nF-statistic\n987.24\n\n\nDF\n3 , 5722\n\n\np-value\n&lt;2e-16\n\n\nResidual Standard Error\n1.025",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-categorical-data",
    "href": "chapters/05-statistical-tests.html#tests-for-categorical-data",
    "title": "5  Common Statistical Tests",
    "section": "5.6 Tests for Categorical Data",
    "text": "5.6 Tests for Categorical Data\n\n5.6.1 Chi-Square Test\nThe Chi-Square test examines the association between categorical variables. Let’s use our biodiversity dataset:\n\n\nCode\n# Load the biodiversity dataset\nplants &lt;- read_csv(\"../data/ecology/biodiversity.csv\")\n\n# Create a contingency table of red list categories by plant group\nif(\"red_list_category\" %in% colnames(plants) & \"group\" %in% colnames(plants)) {\n  # Create a contingency table\n  contingency_table &lt;- table(plants$red_list_category, plants$group)\n  \n  # View the table\n  knitr::kable(contingency_table, \n               caption = \"Contingency Table: Red List Categories by Plant Group\",\n               align = rep(\"r\", ncol(contingency_table) + 1),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                             full_width = FALSE,\n                             position = \"center\")\n  \n  # Perform Chi-Square test\n  chi_sq_result &lt;- chisq.test(contingency_table)\n  chi_sq_table &lt;- data.frame(\n    Statistic = c(\"Chi-squared\", \"Degrees of Freedom\", \"p-value\"),\n    Value = c(\n      round(chi_sq_result$statistic, 2),\n      chi_sq_result$parameter,\n      format.pval(chi_sq_result$p.value, digits = 3)\n    )\n  )\n  \n  # Display the Chi-Square test results\n  knitr::kable(chi_sq_table, \n               caption = \"Chi-Square Test Results: Association Between Red List Categories and Plant Groups\",\n               align = c(\"l\", \"r\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                             full_width = FALSE,\n                             position = \"center\")\n  \n  # Examine residuals to understand the pattern of association\n  chi_sq_residuals &lt;- chi_sq_result$residuals\n  \n  # Create a data frame with the residuals in a more suitable format for display\n  residuals_matrix &lt;- as.matrix(chi_sq_residuals)\n  residuals_df &lt;- as.data.frame.table(residuals_matrix)\n  colnames(residuals_df) &lt;- c(\"Category\", \"Group\", \"Residual\")\n  residuals_df$Residual &lt;- round(residuals_df$Residual, 2)\n  \n  # Display the residuals table\n  knitr::kable(residuals_df,\n               caption = \"Standardized Residuals: Association Between Red List Categories and Plant Groups\",\n               align = c(\"l\", \"l\", \"r\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"),\n                             full_width = FALSE,\n                             position = \"center\")\n} else {\n  # If the expected columns don't exist, create a demonstration with available data\n  message(\"Required columns not found. Creating a demonstration with available columns.\")\n  \n  # Identify categorical columns\n  categorical_cols &lt;- sapply(plants, function(x) is.character(x) || is.factor(x))\n  cat_col_names &lt;- names(plants)[categorical_cols]\n  \n  if(length(cat_col_names) &gt;= 2) {\n    # Select the first two categorical columns\n    col1 &lt;- cat_col_names[1]\n    col2 &lt;- cat_col_names[2]\n    \n    # Create a contingency table\n    contingency_table &lt;- table(plants[[col1]], plants[[col2]])\n    \n    # View the table\n    knitr::kable(contingency_table, \n                 caption = paste(\"Contingency Table:\", col1, \"by\", col2),\n                 align = rep(\"r\", ncol(contingency_table) + 1),\n                 format = \"html\") %&gt;%\n      kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                               full_width = FALSE,\n                               position = \"center\")\n    \n    # Perform Chi-Square test if appropriate\n    if(min(dim(contingency_table)) &gt; 1 && sum(contingency_table) &gt; 0) {\n      chi_sq_result &lt;- chisq.test(contingency_table, simulate.p.value = TRUE)\n      chi_sq_table &lt;- data.frame(\n        Statistic = c(\"Chi-squared\", \"Degrees of Freedom\", \"p-value\"),\n        Value = c(\n          round(chi_sq_result$statistic, 2),\n          chi_sq_result$parameter,\n          format.pval(chi_sq_result$p.value, digits = 3)\n        )\n      )\n      \n      # Display the Chi-Square test results\n      knitr::kable(chi_sq_table, \n                   caption = paste(\"Chi-Square Test Results: Association Between\", col1, \"and\", col2),\n                   align = c(\"l\", \"r\"),\n                   format = \"html\") %&gt;%\n        kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                                 full_width = FALSE,\n                                 position = \"center\")\n    } else {\n      message(\"Contingency table not suitable for Chi-Square test.\")\n    }\n  } else {\n    message(\"Not enough categorical columns found for Chi-Square test demonstration.\")\n  }\n}\n\n\n\nStandardized Residuals: Association Between Red List Categories and Plant Groups\n\n\nCategory\nGroup\nResidual\n\n\n\n\nExtinct\nAlgae\n0.24\n\n\nExtinct in the Wild\nAlgae\n-0.62\n\n\nExtinct\nConifer\n0.14\n\n\nExtinct in the Wild\nConifer\n-0.36\n\n\nExtinct\nCycad\n-1.12\n\n\nExtinct in the Wild\nCycad\n2.90\n\n\nExtinct\nFerns and Allies\n0.21\n\n\nExtinct in the Wild\nFerns and Allies\n-0.53\n\n\nExtinct\nFlowering Plant\n0.06\n\n\nExtinct in the Wild\nFlowering Plant\n-0.16\n\n\nExtinct\nMosses\n0.28\n\n\nExtinct in the Wild\nMosses\n-0.72",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-trends-and-time-series",
    "href": "chapters/05-statistical-tests.html#tests-for-trends-and-time-series",
    "title": "5  Common Statistical Tests",
    "section": "5.7 Tests for Trends and Time Series",
    "text": "5.7 Tests for Trends and Time Series\n\n5.7.1 Time Series Analysis\nTime series analysis examines data collected over time to identify patterns, trends, and seasonal effects:\n\n\nCode\n# Create a time series of wheat yields for a specific country\nus_wheat &lt;- crop_yields %&gt;%\n  filter(Entity == \"United States\" & !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  arrange(Year)\n\n# Convert to time series object\nif(requireNamespace(\"zoo\", quietly = TRUE)) {\n  library(zoo)\n  wheat_ts &lt;- zoo(us_wheat$`Wheat (tonnes per hectare)`, us_wheat$Year)\n  \n  # Plot the time series\n  plot(wheat_ts, main = \"US Wheat Yields Over Time\",\n       xlab = \"Year\", ylab = \"Wheat Yield (tonnes/hectare)\")\n  \n  # Add trend line\n  lines(lowess(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`), col = \"red\", lwd = 2)\n} else {\n  # Basic plot if zoo package is not available\n  plot(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`, type = \"l\",\n       main = \"US Wheat Yields Over Time\",\n       xlab = \"Year\", ylab = \"Wheat Yield (tonnes per hectare)\")\n  \n  # Add trend line\n  lines(lowess(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`), col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Mann-Kendall Trend Test\nThe Mann-Kendall test is a non-parametric test for identifying trends in time series data:\n\n\nCode\n# Perform Mann-Kendall trend test\nif(requireNamespace(\"Kendall\", quietly = TRUE)) {\n  library(Kendall)\n  mk_test &lt;- Kendall::MannKendall(us_wheat$`Wheat (tonnes per hectare)`)\n  mk_table &lt;- data.frame(\n    Statistic = c(\"Tau\", \"p-value\"),\n    Value = c(\n      round(mk_test$tau, 3),\n      format.pval(mk_test$p.value, digits = 3)\n    )\n  )\n  \n  # Display the Mann-Kendall test results\n  knitr::kable(mk_table, \n               caption = \"Mann-Kendall Trend Test Results: US Wheat Yields\",\n               align = c(\"l\", \"r\"),\n               format = \"html\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"), \n                             full_width = FALSE,\n                             position = \"center\")\n} else {\n  message(\"The Kendall package is not installed. Install it with install.packages('Kendall') to run the Mann-Kendall trend test.\")\n}\n\n\n\nMann-Kendall Trend Test Results: US Wheat Yields\n\n\nStatistic\nValue\n\n\n\n\nTau\n0.798\n\n\np-value\n0.798",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#summary",
    "href": "chapters/05-statistical-tests.html#summary",
    "title": "5  Common Statistical Tests",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nThis chapter has demonstrated a variety of statistical tests using real agricultural and biodiversity datasets. We’ve covered:\n\nTests for comparing groups:\n\nt-tests for comparing two groups\nANOVA for comparing multiple groups\nNon-parametric alternatives when data doesn’t meet parametric assumptions\n\nTests for relationships:\n\nCorrelation analysis to measure the strength of relationships\nRegression analysis to model relationships between variables\n\nTests for categorical data:\n\nChi-Square test for examining associations between categorical variables\n\nTests for time series data:\n\nTime series analysis for identifying patterns over time\nMann-Kendall test for detecting trends\n\n\nWhen conducting statistical tests, remember to: - Clearly define your research question - Check if your data meets the assumptions of the test - Choose the appropriate test based on your data type and research question - Interpret results in the context of your research question - Consider the practical significance, not just statistical significance",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#exercises",
    "href": "chapters/05-statistical-tests.html#exercises",
    "title": "5  Common Statistical Tests",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\n\nUsing the crop yield dataset, compare maize yields between continents using both ANOVA and the Kruskal-Wallis test. Which is more appropriate and why?\nExamine the relationship between potato and rice yields using correlation analysis. Calculate both Pearson and Spearman correlations and explain which is more appropriate.\nUsing the biodiversity dataset, investigate whether there’s an association between conservation status and another categorical variable of your choice.\nPerform a time series analysis of wheat yields for China and compare the trend with that of the United States.\nUsing the animal dataset (../data/entomology/insects.csv), compare two groups using an appropriate statistical test.\nCreate a multiple regression model to predict coffee quality scores using the coffee economics dataset (../data/economics/economic.csv).",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#enhanced-statistical-tests-chapter",
    "href": "chapters/05-statistical-tests.html#enhanced-statistical-tests-chapter",
    "title": "5  Common Statistical Tests",
    "section": "5.10 Enhanced Statistical Tests Chapter",
    "text": "5.10 Enhanced Statistical Tests Chapter\nThe enhanced visualizations and tables for this chapter are available in a separate file to ensure compatibility with the book rendering process.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html",
    "href": "chapters/06-visualization.html",
    "title": "6  Data Visualization",
    "section": "",
    "text": "6.1 Introduction\nData visualization is a crucial skill for communicating scientific findings effectively. In this chapter, you will:",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#introduction",
    "href": "chapters/06-visualization.html#introduction",
    "title": "6  Data Visualization",
    "section": "",
    "text": "Learn various data visualization techniques\nGain expertise in creating informative graphs and plots\nUnderstand the role of visualization in conveying insights clearly in natural sciences",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#the-importance-of-data-visualization",
    "href": "chapters/06-visualization.html#the-importance-of-data-visualization",
    "title": "6  Data Visualization",
    "section": "6.2 The Importance of Data Visualization",
    "text": "6.2 The Importance of Data Visualization\n\n6.2.1 Why Data Visualization Matters\nData visualization plays a pivotal role in natural sciences research for several reasons:\n\nPattern Recognition: Visualizations make it easier to identify patterns, trends, and anomalies in data. This can reveal phenomena like population fluctuations, species distributions, or the impact of environmental factors.\nCommunication: Effective visualizations simplify complex scientific concepts, enabling researchers to convey findings to both expert and non-expert audiences. This is particularly valuable when sharing results with policymakers, stakeholders, or the general public.\nHypothesis Testing: Visualizations assist in formulating and testing scientific hypotheses. Researchers can visually explore data distributions, relationships, and spatial patterns, which informs the design of hypothesis tests.\nDecision-Making: Visualizations aid in making informed decisions about conservation and management strategies. For example, they can illustrate the effects of different interventions on ecosystem health or agricultural productivity.\n\n\n\n6.2.2 Types of Scientific Data\nData in natural sciences come in various forms, including:\n\nCategorical Data: These represent qualitative characteristics, such as species names, habitat types, or land-use categories. Suitable visualizations include bar charts, pie charts, and stacked bar plots.\nNumerical Data: Numerical data involve measurements or counts, such as temperature, population size, or crop yields. Histograms, scatter plots, and box plots are useful for visualizing numerical data.\nSpatial Data: Spatial data describe the geographical distribution of features. Maps, heatmaps, and spatial plots help visualize these data effectively, allowing researchers to observe spatial patterns and trends.\n\n\n\n\n\n\n\nPROFESSIONAL TIP: Principles of Effective Scientific Visualization\n\n\n\nWhen creating visualizations for scientific publications or presentations:\n\nChoose the right plot type: Match your visualization to your data type and research question\nPrioritize clarity over complexity: A simple, clear visualization is better than a complex, confusing one\nMaintain data integrity: Never distort data through misleading scales, truncated axes, or cherry-picked views\nDesign for accessibility: Use colorblind-friendly palettes (viridis, cividis, or ColorBrewer schemes)\nFollow journal standards: Check target journal guidelines for figure specifications before submission\nInclude uncertainty: Always visualize error bars, confidence intervals, or other measures of uncertainty\nLabel thoroughly: Every axis should have clear labels with units; legends should be comprehensive\nConsider the narrative: Ensure your visualization supports the scientific story you’re telling\nCreate self-contained figures: A good figure should be interpretable even when separated from the text",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#creating-basic-plots",
    "href": "chapters/06-visualization.html#creating-basic-plots",
    "title": "6  Data Visualization",
    "section": "6.3 Creating Basic Plots",
    "text": "6.3 Creating Basic Plots\n\n6.3.1 Introduction to Basic Plots\nHere’s an overview of common basic plots in natural sciences research and when to use them:\n\nBar Charts:\n\nUse: Bar charts are suitable for visualizing categorical data, such as the frequency of different species in a habitat.\nWhen to Use: Use bar charts when comparing the quantities or proportions of different categories. They’re great for showing discrete data.\n\nHistograms:\n\nUse: Histograms are ideal for visualizing the distribution of numerical data.\nWhen to Use: Use histograms when you want to understand the shape of data distributions, check for skewness, and identify potential outliers.\n\nScatter Plots:\n\nUse: Scatter plots are valuable for examining relationships between two numerical variables.\nWhen to Use: Use scatter plots when you want to see how one variable changes with respect to another. They’re helpful for identifying correlations or trends.\n\n\nThese basic plots serve as building blocks for more advanced visualizations and are foundational tools for exploring and communicating scientific data.\nVisualizations not only enhance the understanding of natural phenomena but also foster data-driven decision-making in research and conservation efforts. They allow researchers to uncover insights that might remain hidden in raw data and effectively communicate findings to a wide audience.\n\n\n6.3.2 Creating Bar Charts\nLet’s create a bar chart using the plant biodiversity dataset:\n\n\nCode\n# Load required packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(viridis)  # For colorblind-friendly palettes\n\n# Set a professional theme for all plots\ntheme_set(\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"gray50\"),\n    plot.caption = element_text(size = 10, color = \"gray50\"),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 12),\n    legend.title = element_text(face = \"bold\"),\n    legend.text = element_text(size = 12),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, size = 0.5)\n  )\n)\n\n# Read the biodiversity dataset\nbiodiversity &lt;- read.csv(\"../data/ecology/biodiversity.csv\")\n\n# Create a summary of conservation status by region\nstatus_summary &lt;- biodiversity %&gt;%\n  group_by(continent, red_list_category) %&gt;%\n  summarize(Count = n(), .groups = \"drop\") %&gt;%\n  filter(!is.na(red_list_category))\n\n# Create a professional bar chart\nggplot(status_summary, aes(x = continent, y = Count, fill = red_list_category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.7) +\n  scale_fill_viridis_d(option = \"plasma\", begin = 0.2, end = 0.8) +\n  labs(\n    title = \"Plant Conservation Status by Continent\",\n    subtitle = \"Distribution of species across different conservation categories\",\n    x = \"Continent\",\n    y = \"Number of Species\",\n    fill = \"Conservation Status\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  coord_flip() +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.major.y = element_line(color = \"gray90\", size = 0.3)\n  )\n\n\n\n\n\nBar chart showing the conservation status of plant species across different regions. This visualization highlights the varying levels of threatened species in different geographical areas.\n\n\n\n\n\n6.3.2.1 R Code Explanation\nThe provided R code creates a bar chart using the ggplot2 package (part of tidyverse). This code visualizes the number of plant species by their IUCN Red List conservation status category using our biodiversity dataset. Let’s break down the code step by step:\n\nLoad Required Libraries\n\nWe load the tidyverse package, which includes ggplot2 for visualization and dplyr for data manipulation.\n\nLoad the Dataset\n\nWe load the plant biodiversity dataset that we downloaded earlier.\n\nCreate a Summary\n\nWe use count() to count the number of species in each Red List category.\nWe arrange the categories in descending order by count.\nWe remove any NA values to ensure clean visualization.\n\nCreate a Bar Chart\n\nWe use ggplot() to initialize the plot with our summary data.\nWe map the Red List categories to the x-axis, the count to the y-axis, and use the categories for fill colors.\nWe use geom_bar(stat = \"identity\") to create bars with heights representing the counts.\nWe add appropriate labels and use a minimal theme for a clean appearance.\nWe angle the x-axis labels for better readability and remove the redundant legend.\n\n\n\n\n6.3.2.2 Practical Example\nIn biodiversity research, you might use bar charts to visualize:\n\nSpecies Richness: Show the number of species across different taxonomic groups.\nConservation Status: Compare the number of species in different threat categories.\nHabitat Distribution: Visualize the distribution of species across different habitat types.\nGeographic Distribution: Show species counts across different countries or regions.\nTemporal Changes: Track changes in species numbers over different time periods.\n\n\n\n\n6.3.3 Constructing Histograms\nNow, let’s create a histogram to visualize the distribution of a numerical variable in our plant dataset:\n\n\nCode\n# Create a threat score by summing all threat columns\nbiodiversity_with_scores &lt;- biodiversity %&gt;%\n  mutate(\n    threat_score = threat_AA + threat_BRU + threat_RCD + \n                  threat_ISGD + threat_EPM + threat_CC + \n                  threat_HID + threat_P + threat_TS + \n                  threat_NSM + threat_GE\n  )\n\n# Create a histogram of threat scores by continent\nggplot(biodiversity_with_scores, aes(x = threat_score, fill = continent)) +\n  geom_histogram(bins = 15, alpha = 0.8, position = \"identity\", color = \"white\", size = 0.2) +\n  scale_fill_viridis_d(option = \"turbo\", begin = 0.2, end = 0.8) +\n  labs(\n    title = \"Distribution of Threat Scores by Continent\",\n    subtitle = \"Frequency of threat levels across geographical regions\",\n    x = \"Threat Score\",\n    y = \"Frequency\",\n    fill = \"Continent\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  facet_wrap(~continent, scales = \"free_y\") +\n  theme(\n    strip.background = element_rect(fill = \"gray95\"),\n    strip.text = element_text(face = \"bold\"),\n    legend.position = \"none\"\n  )\n\n\n\n\n\nHistogram showing the distribution of threat scores across different continents. The visualization reveals distinct patterns in conservation threats related to geographical regions.\n\n\n\n\n\n6.3.3.1 R Code Explanation\nThe code above attempts to create a histogram of plant heights. Since we’re working with a real dataset, we first check if the column exists before creating the visualization. This is good practice when working with external datasets where column names might vary.\n\nWe use conditional logic to check if “Height_cm” exists in the dataset.\nIf it exists, we create a histogram with appropriate binning and styling.\nIf not, we examine the structure of the dataset to identify other numeric variables that could be visualized.\n\nThis approach demonstrates how to handle real-world data that might not always conform to our expectations.\n\n\n\n6.3.4 Designing Scatter Plots\nLet’s create a scatter plot to examine relationships between variables in our biodiversity dataset:\n\n\nCode\n# Create year numeric variable from year_last_seen\nbiodiversity_for_scatter &lt;- biodiversity_with_scores %&gt;%\n  # Create a numeric year value from the year_last_seen categories\n  mutate(\n    year_numeric = case_when(\n      year_last_seen == \"Before 1900\" ~ 1890,\n      year_last_seen == \"1900-1919\" ~ 1910,\n      year_last_seen == \"1920-1939\" ~ 1930,\n      year_last_seen == \"1940-1959\" ~ 1950,\n      year_last_seen == \"1960-1979\" ~ 1970,\n      year_last_seen == \"1980-1999\" ~ 1990,\n      year_last_seen == \"2000-2020\" ~ 2010,\n      TRUE ~ NA_real_\n    )\n  ) %&gt;%\n  filter(!is.na(year_numeric), !is.na(threat_score))\n\n# Create a publication-quality scatter plot\nggplot(biodiversity_for_scatter, aes(x = year_numeric, y = threat_score, color = continent)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = TRUE, alpha = 0.2) +\n  scale_color_viridis_d(option = \"cividis\") +\n  labs(\n    title = \"Relationship Between Last Sighting Year and Threat Score\",\n    subtitle = \"Analysis of extinction patterns across time and geography\",\n    x = \"Approximate Year Last Seen\",\n    y = \"Threat Score\",\n    color = \"Continent\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  theme(\n    legend.position = \"right\",\n    panel.grid.major = element_line(color = \"gray90\", size = 0.3)\n  )\n\n\n\n\n\nScatter plot showing the relationship between threat scores and year last seen for plant species. Points are colored by continent to reveal geographical patterns in extinction threats.\n\n\n\n\n\n6.3.4.1 R Code Explanation\nThis code creates a scatter plot using our biodiversity dataset:\n\nLoad and Explore Data\n\nWe load the biodiversity dataset and examine its structure.\nWe identify numeric columns that could be used for a scatter plot.\n\nDynamic Column Selection\n\nRather than hardcoding column names, we dynamically select numeric columns.\nThis makes the code more robust when working with unfamiliar datasets.\n\nCreate Scatter Plot\n\nWe use ggplot() with aes_string() to dynamically map variables to axes.\nWe add points with some transparency for better visualization of overlapping data.\nWe include a linear regression line with confidence interval to show the trend.\nWe use appropriate labels and a minimal theme.\n\n\nThis approach demonstrates how to create visualizations when working with new datasets where you might not know the column names in advance.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#advanced-visualization-techniques",
    "href": "chapters/06-visualization.html#advanced-visualization-techniques",
    "title": "6  Data Visualization",
    "section": "6.4 Advanced Visualization Techniques",
    "text": "6.4 Advanced Visualization Techniques\n\n6.4.1 Creating Box Plots\nBox plots are excellent for comparing distributions across groups:\n\n\nCode\n# Create a publication-quality box plot\nggplot(biodiversity_with_scores, aes(x = reorder(continent, threat_score, FUN = median, na.rm = TRUE), \n                          y = threat_score, \n                          fill = continent)) +\n  geom_boxplot(alpha = 0.8, outlier.shape = 21, outlier.size = 2) +\n  scale_fill_viridis_d(option = \"mako\", begin = 0.2, end = 0.9) +\n  labs(\n    title = \"Threat Score Comparison Across Continents\",\n    subtitle = \"Distribution of conservation threats by geographical region\",\n    x = NULL,\n    y = \"Threat Score\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  coord_flip() +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major.y = element_line(color = \"gray90\", size = 0.3)\n  )\n\n\n\n\n\nBox plot comparing threat scores across different continents. The visualization highlights the median, quartiles, and outliers in conservation threat data.\n\n\n\n\n\n6.4.1.1 Box Plot Interpretation\nBox plots provide a comprehensive view of data distributions:\n\nThe box represents the interquartile range (IQR), from the 25th to 75th percentile.\nThe line inside the box shows the median (50th percentile).\nThe whiskers typically extend to the smallest and largest values within 1.5 times the IQR.\nPoints beyond the whiskers represent potential outliers.\n\nIn our threat score example, the box plot allows us to compare: - The typical threat score (median) for different continents - The variability in threat scores (box width and whisker length) - The presence of unusually high or low threat scores (outliers) - Differences between continents in terms of both threat score levels and consistency\n\n\n\n6.4.2 Designing Heatmaps\nHeatmaps are powerful for visualizing complex relationships in multivariate data:\n\n\nCode\n# Create a correlation heatmap of threat types\n# First, prepare the data\nthreat_columns &lt;- biodiversity %&gt;%\n  select(starts_with(\"threat_\"), -threat_NA) %&gt;%\n  names()\n\n# Calculate correlation matrix\nthreat_cor &lt;- biodiversity %&gt;%\n  select(all_of(threat_columns)) %&gt;%\n  cor(use = \"pairwise.complete.obs\")\n\n# Convert to long format for ggplot\nthreat_cor_long &lt;- as.data.frame(as.table(threat_cor))\nnames(threat_cor_long) &lt;- c(\"Threat1\", \"Threat2\", \"Correlation\")\n\n# Create readable threat labels\nthreat_labels &lt;- c(\n  \"threat_AA\" = \"Agriculture\",\n  \"threat_BRU\" = \"Biological Resource Use\",\n  \"threat_RCD\" = \"Residential Development\",\n  \"threat_ISGD\" = \"Invasive Species\",\n  \"threat_EPM\" = \"Energy Production\",\n  \"threat_CC\" = \"Climate Change\",\n  \"threat_HID\" = \"Human Intrusion\",\n  \"threat_P\" = \"Pollution\",\n  \"threat_TS\" = \"Transportation\",\n  \"threat_NSM\" = \"Natural System Modification\",\n  \"threat_GE\" = \"Geological Events\"\n)\n\n# Replace the threat codes with readable labels\nthreat_cor_long$Threat1 &lt;- factor(threat_cor_long$Threat1, \n                                 levels = names(threat_labels),\n                                 labels = threat_labels)\nthreat_cor_long$Threat2 &lt;- factor(threat_cor_long$Threat2, \n                                 levels = names(threat_labels),\n                                 labels = threat_labels)\n\n# Create a publication-quality heatmap\nggplot(threat_cor_long, aes(x = Threat1, y = Threat2, fill = Correlation)) +\n  geom_tile(color = \"white\", size = 0.5) +\n  scale_fill_gradient2(\n    low = \"#4575b4\", \n    mid = \"white\", \n    high = \"#d73027\",\n    midpoint = 0,\n    limits = c(-1, 1)\n  ) +\n  geom_text(aes(label = sprintf(\"%.2f\", Correlation)), \n            color = ifelse(abs(threat_cor_long$Correlation) &gt; 0.7, \"white\", \"black\"),\n            size = 3) +\n  labs(\n    title = \"Correlation Between Different Threat Types\",\n    subtitle = \"Strength of relationship between conservation threats\",\n    x = NULL, y = NULL,\n    fill = \"Correlation\\nCoefficient\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank(),\n    panel.background = element_rect(fill = \"white\", color = NA),\n    legend.position = \"right\",\n    legend.key.height = unit(1, \"cm\")\n  ) +\n  coord_fixed()\n\n\n\n\n\nHeatmap visualizing the correlation matrix between different threat types. The color intensity represents the strength and direction of relationships between conservation threats.\n\n\n\n\n\n6.4.2.1 Heatmap Interpretation\nThe heatmap visualizes the correlation between different threat types:\n\nColor intensity represents the strength of correlation (red for positive, blue for negative).\nThe diagonal shows perfect correlation of each variable with itself (always 1).\nClusters of similar colors indicate groups of variables that are highly correlated.\n\nThis visualization helps researchers identify: - Which threats tend to have similar patterns - Potential underlying factors affecting multiple threats simultaneously - Opportunities for threat mitigation based on low correlations - Geographical patterns in threat correlations, which could inform regional conservation strategies\n\n\n\n6.4.3 Creating Time Series Plots\nTime series plots are essential for visualizing trends over time:\n\n\nCode\n# Create a time series plot using the crop yields data\n# First, read the dataset\ncrop_yields &lt;- read.csv(\"../data/agriculture/crop_yields.csv\")\n\n# Check column names to ensure we're using the correct ones\nwheat_col &lt;- names(crop_yields)[grep(\"Wheat\", names(crop_yields))]\n\n# Create a simplified dataset for time series analysis\n# Select top countries based on data availability\ntop_countries &lt;- crop_yields %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(count = n()) %&gt;%\n  filter(count &gt; 30) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(6) %&gt;%\n  pull(Entity)\n\n# Create the time series data\ntime_series_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% top_countries) %&gt;%\n  filter(Year &gt;= 1970)\n\n# Create a publication-quality time series plot\n# Use a column that exists in the dataset\nif(length(wheat_col) &gt; 0) {\n  # If we have a wheat column, use it\n  ggplot(time_series_data, aes(x = Year, y = .data[[wheat_col[1]]], color = Entity)) +\n    geom_line(size = 1, na.rm = TRUE) +\n    geom_point(size = 2, alpha = 0.7, na.rm = TRUE) +\n    scale_color_viridis_d(option = \"turbo\", begin = 0.1, end = 0.9) +\n    scale_x_continuous(breaks = seq(1970, 2020, by = 10)) +\n    labs(\n      title = \"Agricultural Yield Trends Over Time (1970-Present)\",\n      subtitle = \"Productivity changes for major agricultural producers\",\n      x = \"Year\",\n      y = paste(\"Yield\", wheat_col[1]),\n      color = \"Country\",\n      caption = \"Data source: Our World in Data\"\n    ) +\n    theme(\n      legend.position = \"right\",\n      panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n      axis.text.x = element_text(angle = 0)\n    )\n} else {\n  # If no wheat column, use another numeric column\n  numeric_cols &lt;- sapply(time_series_data, is.numeric)\n  numeric_col_names &lt;- names(time_series_data)[numeric_cols]\n  numeric_col_names &lt;- numeric_col_names[numeric_col_names != \"Year\"]\n  \n  if(length(numeric_col_names) &gt; 0) {\n    selected_col &lt;- numeric_col_names[1]\n    \n    ggplot(time_series_data, aes(x = Year, y = .data[[selected_col]], color = Entity)) +\n      geom_line(size = 1, na.rm = TRUE) +\n      geom_point(size = 2, alpha = 0.7, na.rm = TRUE) +\n      scale_color_viridis_d(option = \"turbo\", begin = 0.1, end = 0.9) +\n      scale_x_continuous(breaks = seq(1970, 2020, by = 10)) +\n      labs(\n        title = \"Agricultural Trends Over Time (1970-Present)\",\n        subtitle = \"Changes for major agricultural producers\",\n        x = \"Year\",\n        y = selected_col,\n        color = \"Country\",\n        caption = \"Data source: Our World in Data\"\n      ) +\n      theme(\n        legend.position = \"right\",\n        panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n        axis.text.x = element_text(angle = 0)\n      )\n  } else {\n    # If no suitable numeric columns, create a message plot\n    plot_data &lt;- data.frame(x = 1:10, y = 1:10)\n    ggplot(plot_data, aes(x, y)) +\n      geom_blank() +\n      annotate(\"text\", x = 5, y = 5, label = \"No suitable data available for time series\") +\n      theme_minimal() +\n      labs(title = \"Time Series Plot\", subtitle = \"Data not available\")\n  }\n}\n\n\n\n\n\nTime series plot tracking agricultural trends over time for major producers. The visualization illustrates long-term productivity changes and allows comparison between countries.\n\n\n\n\n\n6.4.3.1 Time Series Interpretation\nTime series plots reveal important temporal patterns:\n\nTrends: Long-term increases or decreases over time\nSeasonality: Regular patterns that repeat at fixed intervals\nCycles: Patterns that occur but not at fixed intervals\nIrregular fluctuations: Random variations that don’t follow a pattern\n\nIn our agricultural yield example, we can observe: - The overall trend in yields for different countries - Relative performance of countries over time - Rate of improvement in agricultural productivity - Stability or volatility in yields year-to-year - Convergence or divergence between countries",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#best-practices-for-data-visualization",
    "href": "chapters/06-visualization.html#best-practices-for-data-visualization",
    "title": "6  Data Visualization",
    "section": "6.5 Best Practices for Data Visualization",
    "text": "6.5 Best Practices for Data Visualization\n\n6.5.1 Choosing the Right Visualization\nSelecting the appropriate visualization depends on your data and the story you want to tell:\n\nFor Comparing Categories:\n\nBar charts for comparing values across categories\nGrouped or stacked bar charts for comparing multiple variables across categories\n\nFor Showing Distributions:\n\nHistograms for showing the distribution of a single variable\nBox plots for comparing distributions across groups\nViolin plots for showing distribution shape along with summary statistics\n\nFor Showing Relationships:\n\nScatter plots for examining relationships between two variables\nBubble charts for examining relationships among three variables\nHeatmaps for visualizing complex relationships in multivariate data\n\nFor Showing Compositions:\n\nPie charts for showing parts of a whole (use sparingly)\nStacked bar charts for showing composition across categories\nArea charts for showing composition over time\n\nFor Showing Trends:\n\nLine charts for showing changes over time\nArea charts for showing cumulative totals over time\n\n\n\n\n6.5.2 Design Principles for Effective Visualization\nFollow these principles to create clear, informative visualizations:\n\nSimplicity: Keep visualizations simple and focused on the main message. Avoid unnecessary elements that can distract from the data.\nClarity: Ensure that your visualization clearly communicates the intended message. Use appropriate labels, titles, and annotations.\nAccuracy: Represent data accurately. Avoid distorting the data through inappropriate scales or misleading visual elements.\nConsistency: Use consistent colors, shapes, and styles throughout your visualizations for better comprehension.\nColor Use: Choose colors thoughtfully. Use color to highlight important aspects of your data, but be mindful of color blindness and cultural associations.\nAnnotation: Add context through appropriate annotations, explaining unusual patterns or important events.\nAudience Consideration: Tailor your visualizations to your audience’s knowledge level and needs.\n\n\n\n6.5.3 Common Pitfalls to Avoid\nBe aware of these common visualization mistakes:\n\nMisleading Scales: Starting y-axes at values other than zero can exaggerate differences.\nOvercomplication: Adding too many variables or visual elements can confuse rather than clarify.\nPoor Color Choices: Using colors that are difficult to distinguish or that carry unintended connotations.\nIgnoring Accessibility: Not considering color blindness or other accessibility issues.\nInappropriate Chart Types: Using chart types that don’t match the data or the story you want to tell.\nMissing Context: Failing to provide necessary context for interpreting the visualization.\nNeglecting Uncertainty: Not showing confidence intervals, error bars, or other indicators of uncertainty.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#summary",
    "href": "chapters/06-visualization.html#summary",
    "title": "6  Data Visualization",
    "section": "6.6 Summary",
    "text": "6.6 Summary\nEffective data visualization is a powerful tool for both exploring data and communicating findings. By choosing the right visualization techniques and following best practices, you can gain deeper insights from your data and share those insights with others in a compelling way.\nIn this chapter, we’ve explored: - The importance of data visualization in natural sciences - Basic visualization techniques including bar charts, histograms, and scatter plots - Advanced visualization methods like box plots, heatmaps, and time series plots - Best practices and principles for creating effective visualizations\nBy applying these techniques to real datasets from agriculture, ecology, and geography, we’ve demonstrated how visualization can reveal patterns and relationships that might otherwise remain hidden in the raw data.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#exercises",
    "href": "chapters/06-visualization.html#exercises",
    "title": "6  Data Visualization",
    "section": "6.7 Exercises",
    "text": "6.7 Exercises\n\nUsing the plant biodiversity dataset (../data/ecology/biodiversity.csv), create a visualization showing the distribution of plant species across different taxonomic groups.\nCreate a time series plot using the crop yield dataset (../data/agriculture/crop_yields.csv) that shows the trends in rice yields for the top 5 producing countries.\nUsing the spatial dataset (../data/geography/spatial.csv), create a scatter plot matrix (pairs plot) to explore relationships between multiple numeric variables.\nDesign a visualization that compares the conservation status of plant species across different habitat types using the biodiversity dataset.\nCreate a heatmap visualization using the coffee economics dataset (../data/economics/economic.csv) to explore correlations between quality scores and other variables.\nDesign an animated visualization (using gganimate package) that shows how crop yields have changed over time for a specific country.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html",
    "href": "chapters/07-advanced-visualization.html",
    "title": "7  Advanced Data Visualization",
    "section": "",
    "text": "7.1 Introduction\nBuilding on the visualization techniques covered in Chapter 6, this chapter explores advanced data visualization methods that can help you communicate complex ecological data more effectively. We’ll focus on creating publication-quality graphics, interactive visualizations, and specialized plots for ecological data.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#creating-publication-quality-graphics",
    "href": "chapters/07-advanced-visualization.html#creating-publication-quality-graphics",
    "title": "7  Advanced Data Visualization",
    "section": "7.2 Creating Publication-Quality Graphics",
    "text": "7.2 Creating Publication-Quality Graphics\n\n7.2.1 Customizing ggplot2 Themes\nThe ggplot2 package allows extensive customization of plot appearance:\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load data\ndata(iris)\n\n# Create a basic scatter plot\nbase_plot &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Relationship between Sepal Length and Petal Length\",\n       subtitle = \"Iris Dataset\",\n       x = \"Sepal Length (cm)\",\n       y = \"Petal Length (cm)\",\n       caption = \"Source: Anderson's Iris dataset\")\n\n# Display the base plot\nbase_plot\n\n\n\n\n\n\n\n\n\nCode\n# Create a customized theme\ncustom_theme &lt;- theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"gray50\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\"),\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA)\n  )\n\n# Apply the custom theme\nbase_plot + custom_theme\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Color Palettes for Ecological Data\nChoosing appropriate color palettes is crucial for effective visualization:\n\n\nCode\n# Load packages for color palettes\nlibrary(RColorBrewer)\nlibrary(viridis)\n\n# Display color palettes suitable for ecological data\npar(mfrow = c(4, 1), mar = c(2, 6, 2, 1))\ndisplay.brewer.pal(8, \"YlGn\")\ndisplay.brewer.pal(8, \"BrBG\")\ndisplay.brewer.pal(11, \"RdYlBu\")\nscales::show_col(viridis(8))\n\n\n\n\n\n\n\n\n\nCode\n# Apply different color palettes to our plot\nplot1 &lt;- base_plot + \n  scale_color_brewer(palette = \"Set1\") +\n  custom_theme +\n  ggtitle(\"Color Brewer 'Set1' Palette\")\n\nplot2 &lt;- base_plot + \n  scale_color_viridis_d() +\n  custom_theme +\n  ggtitle(\"Viridis Discrete Palette\")\n\n# Display the plots\nplot1\n\n\n\n\n\n\n\n\n\nCode\nplot2\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Arranging Multiple Plots\nCombining multiple plots can help compare different aspects of your data:\n\n\nCode\nlibrary(patchwork)\n\n# Create individual plots\np1 &lt;- ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_boxplot() +\n  labs(title = \"Sepal Length by Species\",\n       x = NULL,\n       y = \"Sepal Length (cm)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(iris, aes(x = Species, y = Petal.Length, fill = Species)) +\n  geom_boxplot() +\n  labs(title = \"Petal Length by Species\",\n       x = NULL,\n       y = \"Petal Length (cm)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np3 &lt;- ggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  labs(title = \"Sepal Length Distribution\",\n       x = \"Sepal Length (cm)\",\n       y = \"Density\") +\n  theme_minimal()\n\np4 &lt;- ggplot(iris, aes(x = Petal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  labs(title = \"Petal Length Distribution\",\n       x = \"Petal Length (cm)\",\n       y = \"Density\") +\n  theme_minimal()\n\n# Arrange the plots\n(p1 + p2) / (p3 + p4) + \n  plot_annotation(\n    title = \"Iris Morphology by Species\",\n    caption = \"Source: Anderson's Iris dataset\"\n  )",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#interactive-visualizations",
    "href": "chapters/07-advanced-visualization.html#interactive-visualizations",
    "title": "7  Advanced Data Visualization",
    "section": "7.3 Interactive Visualizations",
    "text": "7.3 Interactive Visualizations\n\n7.3.1 Creating Interactive Plots with plotly\nInteractive plots allow users to explore data more deeply:\n\n\nCode\nlibrary(plotly)\nlibrary(knitr)\n\n# Create a ggplot visualization\np &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Relationship between Sepal Length and Petal Length\",\n       x = \"Sepal Length (cm)\",\n       y = \"Petal Length (cm)\") +\n  theme_minimal() +\n  scale_color_viridis_d()\n\n# Check if we're in HTML output mode\nif (knitr::is_html_output()) {\n  # For HTML output, use the interactive plotly version\n  ggplotly(p)\n} else {\n  # For PDF output, use the static ggplot version\n  p + annotate(\"text\", x = 6, y = 6, \n               label = \"Note: Interactive version available in HTML output\", \n               fontface = \"italic\", size = 3)\n}\n\n\n\n\nRelationship between Sepal Length and Petal Length across different Iris species. In the HTML version, this plot is interactive and allows zooming, panning, and hovering for details.\n\n\n\n\n7.3.2 Interactive Maps with leaflet\nFor spatial ecological data, interactive maps can be particularly useful:\n\n\nCode\nlibrary(leaflet)\nlibrary(ggplot2)\nlibrary(knitr)\n\n# Create sample ecological site data\nsites &lt;- data.frame(\n  name = c(\"Forest Reserve\", \"Wetland Study Area\", \"Grassland Transect\", \n           \"Mountain Research Station\", \"Coastal Monitoring Site\"),\n  lat = c(37.7749, 37.8, 37.75, 37.85, 37.7),\n  lng = c(-122.4194, -122.45, -122.5, -122.4, -122.3),\n  habitat = c(\"Forest\", \"Wetland\", \"Grassland\", \"Alpine\", \"Coastal\"),\n  species_count = c(120, 85, 65, 95, 110)\n)\n\n# Create a color palette based on habitat type\nhabitat_colors &lt;- c(\"darkgreen\", \"blue\", \"gold\", \"purple\", \"lightblue\")\nnames(habitat_colors) &lt;- c(\"Forest\", \"Wetland\", \"Grassland\", \"Alpine\", \"Coastal\")\n\nif (knitr::is_html_output()) {\n  # For HTML output, create an interactive leaflet map\n  habitat_pal &lt;- colorFactor(\n    palette = habitat_colors,\n    domain = sites$habitat\n  )\n  \n  # Create an interactive map\n  leaflet(sites) %&gt;%\n    addTiles() %&gt;%  # Add default OpenStreetMap tiles\n    addCircleMarkers(\n      ~lng, ~lat,\n      color = ~habitat_pal(habitat),\n      radius = ~sqrt(species_count) * 1.5,\n      popup = ~paste(\"&lt;b&gt;\", name, \"&lt;/b&gt;&lt;br&gt;\",\n                     \"Habitat: \", habitat, \"&lt;br&gt;\",\n                     \"Species Count: \", species_count),\n      label = ~name,\n      fillOpacity = 0.7\n    ) %&gt;%\n    addLegend(\n      position = \"bottomright\",\n      pal = habitat_pal,\n      values = ~habitat,\n      title = \"Habitat Type\",\n      opacity = 0.7\n    )\n} else {\n  # For PDF output, create a static ggplot map\n  world &lt;- map_data(\"world\")\n  \n  ggplot() +\n    geom_polygon(data = world, aes(x = long, y = lat, group = group), \n                 fill = \"lightgray\", color = \"darkgray\", size = 0.2) +\n    geom_point(data = sites, aes(x = lng, y = lat, color = habitat, size = species_count),\n               alpha = 0.7) +\n    scale_color_manual(values = habitat_colors) +\n    scale_size_continuous(range = c(3, 8), name = \"Species Count\") +\n    coord_fixed(xlim = c(-123, -122), ylim = c(37.6, 37.9)) +\n    labs(title = \"Ecological Study Sites\",\n         subtitle = \"Note: Interactive version available in HTML output\",\n         x = \"Longitude\", y = \"Latitude\", color = \"Habitat Type\") +\n    theme_minimal() +\n    theme(legend.position = \"right\")\n}\n\n\n\n\nEcological study sites across different habitat types. In the HTML version, this map is interactive and allows zooming, panning, and clicking on markers for details.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#specialized-ecological-visualizations",
    "href": "chapters/07-advanced-visualization.html#specialized-ecological-visualizations",
    "title": "7  Advanced Data Visualization",
    "section": "7.4 Specialized Ecological Visualizations",
    "text": "7.4 Specialized Ecological Visualizations\n\n7.4.1 Ordination Plots\nOrdination techniques like PCA and NMDS are common in ecological studies:\n\n\nCode\n# Perform PCA on iris dataset\npca_result &lt;- prcomp(iris[, 1:4], scale. = TRUE)\npca_data &lt;- as.data.frame(pca_result$x)\npca_data$Species &lt;- iris$Species\n\n# Create a PCA biplot\nggplot(pca_data, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  stat_ellipse(level = 0.95) +\n  labs(title = \"PCA of Iris Dataset\",\n       x = paste0(\"PC1 (\", round(summary(pca_result)$importance[2, 1] * 100, 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(summary(pca_result)$importance[2, 2] * 100, 1), \"%)\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Create a loadings plot\nloadings &lt;- as.data.frame(pca_result$rotation)\nloadings$variable &lt;- rownames(loadings)\n\nggplot(loadings, aes(x = PC1, y = PC2)) +\n  geom_segment(aes(x = 0, y = 0, xend = PC1 * 5, yend = PC2 * 5),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"gray50\") +\n  geom_text(aes(label = variable), nudge_x = sign(loadings$PC1) * 0.05,\n            nudge_y = sign(loadings$PC2) * 0.05) +\n  labs(title = \"PCA Loadings\",\n       x = \"PC1\",\n       y = \"PC2\") +\n  theme_minimal() +\n  xlim(-0.7, 0.7) +\n  ylim(-0.7, 0.7)\n\n\n\n\n\n\n\n\n\n\n\n7.4.2 Heatmaps for Community Data\nHeatmaps are useful for visualizing species-by-site matrices:\n\n\nCode\n# Create a simulated species-by-site matrix\nset.seed(123)\nn_sites &lt;- 10\nn_species &lt;- 15\ncommunity_matrix &lt;- matrix(rpois(n_sites * n_species, lambda = 2), \n                          nrow = n_sites, ncol = n_species)\nrownames(community_matrix) &lt;- paste0(\"Site\", 1:n_sites)\ncolnames(community_matrix) &lt;- paste0(\"Sp\", 1:n_species)\n\n# Convert to long format for ggplot\ncommunity_data &lt;- as.data.frame(as.table(community_matrix))\nnames(community_data) &lt;- c(\"Site\", \"Species\", \"Abundance\")\n\n# Create a heatmap\nggplot(community_data, aes(x = Species, y = Site, fill = Abundance)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Species Abundance by Site\",\n       x = \"Species\",\n       y = \"Site\",\n       fill = \"Abundance\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n7.4.3 Network Diagrams for Ecological Interactions\nNetwork diagrams can visualize species interactions:\n\n\nCode\nlibrary(igraph)\nlibrary(ggraph)\n\n# Create a simulated interaction network\nset.seed(456)\nn_species &lt;- 10\ninteraction_matrix &lt;- matrix(rbinom(n_species^2, 1, 0.2), \n                            nrow = n_species, ncol = n_species)\ndiag(interaction_matrix) &lt;- 0  # No self-interactions\nspecies_names &lt;- paste0(\"Species\", 1:n_species)\nrownames(interaction_matrix) &lt;- species_names\ncolnames(interaction_matrix) &lt;- species_names\n\n# Convert to igraph object\ng &lt;- graph_from_adjacency_matrix(interaction_matrix, mode = \"directed\")\nV(g)$type &lt;- sample(c(\"Plant\", \"Pollinator\", \"Herbivore\"), n_species, replace = TRUE)\n\n# Create a network diagram\nggraph(g, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(2, \"mm\")), \n                end_cap = circle(2, \"mm\"),\n                color = \"gray50\") +\n  geom_node_point(aes(color = type), size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Ecological Interaction Network\",\n       color = \"Species Type\") +\n  theme_void()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#visualizing-spatial-data",
    "href": "chapters/07-advanced-visualization.html#visualizing-spatial-data",
    "title": "7  Advanced Data Visualization",
    "section": "7.5 Visualizing Spatial Data",
    "text": "7.5 Visualizing Spatial Data\n\n7.5.1 Creating Maps with ggplot2\nSpatial visualization is crucial for ecological data:\n\n\nCode\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(knitr)\n\n# Get world map data\nworld &lt;- map_data(\"world\")\n\n# Create sample species occurrence data\nset.seed(789)\nn_points &lt;- 100\noccurrences &lt;- data.frame(\n  species = sample(c(\"Species A\", \"Species B\", \"Species C\"), n_points, replace = TRUE),\n  longitude = runif(n_points, -10, 40),\n  latitude = runif(n_points, 35, 60)\n)\n\n# Create a map\nggplot() +\n  geom_polygon(data = world, aes(x = long, y = lat, group = group), \n               fill = \"white\", color = \"gray70\", size = 0.2) +\n  geom_point(data = occurrences, \n             aes(x = longitude, y = latitude, color = species),\n             alpha = 0.7, size = 2) +\n  coord_fixed(xlim = c(-10, 40), ylim = c(35, 60)) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8) +\n  labs(title = \"Species Distribution Map\",\n       subtitle = \"Sample occurrences across Europe\",\n       x = \"Longitude\", y = \"Latitude\", color = \"Species\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_line(color = \"gray90\", size = 0.2))\n\n\n\n\n\nDistribution of sample species occurrences across Europe. The map shows the spatial patterns of three different species.\n\n\n\n\n\n\n7.5.2 Visualizing Raster Data\nEnvironmental raster data is common in ecological studies:\n\n\nCode\nlibrary(raster)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(maps)\n\n# Create a sample raster\nr &lt;- raster(ncol = 100, nrow = 100)\nextent(r) &lt;- c(-10, 40, 35, 60)  # Same extent as our map\nvalues(r) &lt;- runif(ncell(r)) * 10  # Random values\n\n# Convert to data frame for ggplot\nr_df &lt;- as.data.frame(r, xy = TRUE)\ncolnames(r_df) &lt;- c(\"longitude\", \"latitude\", \"value\")\n\n# Get world map data\nworld &lt;- map_data(\"world\")\n\n# Create a raster map\nggplot() +\n  geom_raster(data = r_df, aes(x = longitude, y = latitude, fill = value)) +\n  geom_polygon(data = world, aes(x = long, y = lat, group = group), \n               fill = NA, color = \"gray30\", size = 0.2) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Value\") +\n  coord_fixed(xlim = c(-10, 40), ylim = c(35, 60)) +\n  labs(title = \"Environmental Variable Distribution\",\n       subtitle = \"Simulated environmental gradient across Europe\",\n       x = \"Longitude\", y = \"Latitude\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nEnvironmental variable visualization across Europe. The raster data shows a simulated environmental gradient overlaid with country boundaries.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#summary",
    "href": "chapters/07-advanced-visualization.html#summary",
    "title": "7  Advanced Data Visualization",
    "section": "7.6 Summary",
    "text": "7.6 Summary\nIn this chapter, we’ve explored advanced visualization techniques in R that go beyond basic plots. These techniques allow researchers to create more informative, interactive, and publication-quality visualizations for ecological and forestry data.\nKey points covered include:\n\nCreating complex multi-panel visualizations\nDeveloping interactive plots for exploration\nDesigning effective spatial visualizations\nImplementing animation for temporal data\nCustomizing visualizations for publication\n\nAs you continue to develop your data visualization skills, remember that effective visualization is both an art and a science. The goal is not just to make visually appealing graphics, but to create visualizations that accurately and clearly communicate your findings to your audience.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#exercises",
    "href": "chapters/07-advanced-visualization.html#exercises",
    "title": "7  Advanced Data Visualization",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\n\nCreate a faceted plot showing the relationship between two variables across different categories in one of the datasets.\nDevelop an interactive plot that allows users to explore relationships in ecological data.\nCreate a custom theme for ggplot2 that matches the style guidelines of a scientific journal in your field.\nDesign a spatial visualization showing the distribution of a species or environmental variable.\nCreate an animated plot showing changes in an ecological variable over time.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html",
    "href": "chapters/09-conservation.html",
    "title": "8  Conservation Applications",
    "section": "",
    "text": "8.1 Introduction\nThis chapter explores how data analysis techniques can be applied to conservation science and management. We’ll examine how the statistical methods covered in previous chapters can help address real-world conservation challenges, from monitoring endangered species to evaluating the effectiveness of protected areas.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#introduction",
    "href": "chapters/09-conservation.html#introduction",
    "title": "8  Conservation Applications",
    "section": "",
    "text": "PROFESSIONAL TIP: Data-Driven Decision Making in Conservation\n\n\n\nWhen applying statistical methods to conservation problems:\n\nDocument analytical decisions: Clearly explain why you chose specific statistical approaches (e.g., Type II ANOVA for unbalanced ecological data)\nConsider scale mismatches: Ensure your analysis scale matches both ecological processes and management decisions\nAcknowledge uncertainty: Always communicate confidence intervals and limitations of your models to decision-makers\nUse multiple lines of evidence: Combine different analytical approaches to strengthen conservation recommendations\nIncorporate local knowledge: Integrate traditional ecological knowledge with statistical analyses\nApply adaptive management: Design analyses to evaluate interventions and inform iterative improvements\nConsider statistical power: Ensure monitoring programs have sufficient sample sizes to detect biologically meaningful changes\nReport effect sizes: Focus on magnitude of effects, not just statistical significance\nCreate accessible visualizations: Develop clear graphics that communicate results to diverse stakeholders\nArchive data and code: Maintain reproducible workflows that allow others to build on your conservation research",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#conservation-data-types-and-sources",
    "href": "chapters/09-conservation.html#conservation-data-types-and-sources",
    "title": "8  Conservation Applications",
    "section": "8.2 Conservation Data Types and Sources",
    "text": "8.2 Conservation Data Types and Sources\n\n8.2.1 Types of Conservation Data\nConservation science relies on various types of data:\n\nSpecies Occurrence Data: Presence/absence or abundance of species\nHabitat Data: Vegetation structure, land cover, habitat quality\nThreat Data: Pollution levels, invasive species, human disturbance\nProtected Area Data: Boundaries, management activities, effectiveness\nSocioeconomic Data: Human population, land use, resource extraction\n\n\n\n8.2.2 Data Sources\n\n\n\nCommon Data Sources in Conservation Science\n\n\n\n\n\n\n\n\nSource\nDescription\nAdvantages\nLimitations\n\n\n\n\nField Surveys\nDirect collection of data through field observations and measurements\nHigh accuracy, detailed information\nTime-consuming, expensive, limited spatial coverage\n\n\nRemote Sensing\nSatellite imagery, aerial photography, LiDAR, and other remote sensing techniques\nLarge spatial coverage, temporal consistency\nLower resolution for some applications, cloud cover issues\n\n\nCitizen Science\nData collected by volunteers and non-specialists\nCost-effective, large-scale data collection\nVariable data quality, sampling bias\n\n\nExisting Databases\nGBIF, IUCN Red List, World Database on Protected Areas (WDPA)\nComprehensive, standardized data\nMay have gaps, outdated information\n\n\nEnvironmental Monitoring\nContinuous monitoring of environmental variables (e.g., weather stations, water quality sensors)\nContinuous temporal data, real-time information\nEquipment failures, limited spatial coverage",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#species-distribution-modeling",
    "href": "chapters/09-conservation.html#species-distribution-modeling",
    "title": "8  Conservation Applications",
    "section": "8.3 Species Distribution Modeling",
    "text": "8.3 Species Distribution Modeling\nSpecies distribution models (SDMs) predict where species are likely to occur based on environmental variables (Elith et al., 2009).\n\n8.3.1 Example: Simple Species Distribution Model\n\n\nCode\n# Load required packages\nlibrary(ggplot2)\n\n# Create a simulated environmental dataset\nset.seed(123)\nn &lt;- 200\ntemperature &lt;- runif(n, 5, 30)\nprecipitation &lt;- runif(n, 200, 2000)\nelevation &lt;- runif(n, 0, 3000)\n\n# Calculate species probability based on environmental preferences\n# This species prefers moderate temperatures, high precipitation, and lower elevations\nprobability &lt;- dnorm(temperature, mean = 18, sd = 5) * \n               dnorm(precipitation, mean = 1500, sd = 400) * \n               (1 - elevation/3000)\nprobability &lt;- probability / max(probability)  # Scale to 0-1\n\n# Generate presence/absence based on probability\npresence &lt;- rbinom(n, 1, probability)\n\n# Create a data frame\nspecies_data &lt;- data.frame(\n  temperature = temperature,\n  precipitation = precipitation,\n  elevation = elevation,\n  probability = probability,\n  presence = factor(presence, labels = c(\"Absent\", \"Present\"))\n)\n\n# Visualize the relationship between environmental variables and species presence\nggplot(species_data, aes(x = temperature, y = precipitation, color = presence)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"Species Presence in Environmental Space\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a logistic regression model (simple SDM)\nsdm &lt;- glm(presence ~ temperature + precipitation + elevation, \n           family = binomial, data = species_data)\n\n# Summary of the model\nsummary(sdm)\n\n\n\nCall:\nglm(formula = presence ~ temperature + precipitation + elevation, \n    family = binomial, data = species_data)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -2.8652062  0.9176119  -3.122 0.001793 ** \ntemperature   -0.0073130  0.0317987  -0.230 0.818108    \nprecipitation  0.0022744  0.0004514   5.039 4.69e-07 ***\nelevation     -0.0009398  0.0002641  -3.558 0.000374 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 200.16  on 199  degrees of freedom\nResidual deviance: 150.01  on 196  degrees of freedom\nAIC: 158.01\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\n# Calculate predicted probabilities\nspecies_data$predicted &lt;- predict(sdm, type = \"response\")\n\n# Create a prediction surface for visualization\ntemp_seq &lt;- seq(min(temperature), max(temperature), length.out = 50)\nprecip_seq &lt;- seq(min(precipitation), max(precipitation), length.out = 50)\nelev_mean &lt;- mean(elevation)\n\nprediction_grid &lt;- expand.grid(\n  temperature = temp_seq,\n  precipitation = precip_seq,\n  elevation = elev_mean\n)\n\nprediction_grid$probability &lt;- predict(sdm, newdata = prediction_grid, type = \"response\")\n\n# Plot the prediction surface\nggplot(prediction_grid, aes(x = temperature, y = precipitation, fill = probability)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"Predicted Species Distribution\",\n       subtitle = \"Based on temperature and precipitation (at mean elevation)\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\",\n       fill = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Add actual presence points to the prediction map\nggplot(prediction_grid, aes(x = temperature, y = precipitation, fill = probability)) +\n  geom_tile() +\n  geom_point(data = species_data[species_data$presence == \"Present\", ], \n             aes(x = temperature, y = precipitation), \n             color = \"white\", size = 2, alpha = 0.7) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"Predicted Species Distribution with Actual Presence\",\n       subtitle = \"White points show actual presence records\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\",\n       fill = \"Probability\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#population-trend-analysis",
    "href": "chapters/09-conservation.html#population-trend-analysis",
    "title": "8  Conservation Applications",
    "section": "8.4 Population Trend Analysis",
    "text": "8.4 Population Trend Analysis\nAnalyzing population trends is crucial for conservation planning and evaluating management effectiveness.\n\n8.4.1 Example: Linear Mixed Models for Population Trends\n\n\nCode\n# Simulate population monitoring data\nset.seed(456)\nn_sites &lt;- 10\nn_years &lt;- 15\n\n# Create site and year variables\nsite &lt;- rep(paste0(\"Site\", 1:n_sites), each = n_years)\nyear &lt;- rep(2008:(2008 + n_years - 1), times = n_sites)\n\n# Create random site effects and declining trend\nsite_effect &lt;- rep(rnorm(n_sites, 0, 0.5), each = n_years)\ntime_effect &lt;- -0.05 * (year - 2008)  # Declining trend\nnoise &lt;- rnorm(n_sites * n_years, 0, 0.2)\n\n# Calculate log population size\nlog_pop_size &lt;- 2 + site_effect + time_effect + noise\n\n# Convert to actual counts\npopulation &lt;- round(exp(log_pop_size))\n\n# Create a data frame\npop_data &lt;- data.frame(\n  site = factor(site),\n  year = year,\n  population = population\n)\n\n# Visualize the data\nlibrary(ggplot2)\nggplot(pop_data, aes(x = year, y = population, color = site, group = site)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Population Trends Across Multiple Sites\",\n       x = \"Year\",\n       y = \"Population Size\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a linear mixed model\nlibrary(lme4)\ntrend_model &lt;- lmer(log(population) ~ year + (1|site), data = pop_data)\n\n# Display model summary\nsummary(trend_model)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(population) ~ year + (1 | site)\n   Data: pop_data\n\nREML criterion at convergence: 2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.64610 -0.69998 -0.02039  0.62219  1.92852 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n site     (Intercept) 0.17634  0.4199  \n Residual             0.04223  0.2055  \nNumber of obs: 150, groups:  site, 10\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept) 100.003639   7.826672   12.78\nyear         -0.048800   0.003884  -12.57\n\nCorrelation of Fixed Effects:\n     (Intr)\nyear -1.000\n\n\nCode\n# Calculate overall trend\ntrend_coef &lt;- fixef(trend_model)[\"year\"]\nannual_change &lt;- (exp(trend_coef) - 1) * 100\ncat(\"Annual population change:\", round(annual_change, 2), \"%\\n\")\n\n\nAnnual population change: -4.76 %\n\n\nCode\n# Predict values for visualization\npop_data$predicted &lt;- exp(predict(trend_model))\n\n# Plot observed vs. predicted values\nggplot(pop_data, aes(x = year)) +\n  geom_point(aes(y = population, color = site), alpha = 0.5) +\n  geom_line(aes(y = predicted, group = site), color = \"black\") +\n  labs(title = \"Observed and Predicted Population Sizes\",\n       x = \"Year\",\n       y = \"Population Size\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#habitat-fragmentation-analysis",
    "href": "chapters/09-conservation.html#habitat-fragmentation-analysis",
    "title": "8  Conservation Applications",
    "section": "8.5 Habitat Fragmentation Analysis",
    "text": "8.5 Habitat Fragmentation Analysis\nHabitat fragmentation is a major threat to biodiversity. Landscape metrics help quantify fragmentation patterns.\n\n8.5.1 Example: Calculating Landscape Metrics\n\n\nCode\n# Load required packages\nlibrary(terra)\nlibrary(ggplot2)\n\n# Create a simple landscape raster\nr &lt;- rast(ncol=30, nrow=30)\nvalues(r) &lt;- sample(c(1, 2, 3, 4), ncell(r), replace=TRUE, \n                   prob=c(0.4, 0.3, 0.2, 0.1))\nnames(r) &lt;- \"landcover\"\n\n# Plot the landscape\nplot(r, main=\"Simulated Landscape\", col=c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"))\n\n\n\n\n\n\n\n\n\nCode\n# Create a data frame with class-level metrics manually\nclass_metrics &lt;- data.frame(\n  class = c(1, 2, 3, 4),\n  class_name = c(\"Forest\", \"Agriculture\", \"Water\", \"Urban\"),\n  percentage = c(40, 30, 20, 10),\n  edge_density = c(0.12, 0.09, 0.06, 0.03),\n  num_patches = c(15, 12, 8, 5)\n)\n\n# Visualize class-level metrics\nggplot(class_metrics, aes(x = factor(class), y = percentage, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Percentage of Landscape by Class\",\n       x = \"Land Cover Class\",\n       y = \"Percentage (%)\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# Visualize number of patches\nggplot(class_metrics, aes(x = factor(class), y = num_patches, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Patches by Class\",\n       x = \"Land Cover Class\",\n       y = \"Number of Patches\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# Visualize edge density\nggplot(class_metrics, aes(x = factor(class), y = edge_density, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Edge Density by Class\",\n       x = \"Land Cover Class\",\n       y = \"Edge Density\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#protected-area-effectiveness",
    "href": "chapters/09-conservation.html#protected-area-effectiveness",
    "title": "8  Conservation Applications",
    "section": "8.6 Protected Area Effectiveness",
    "text": "8.6 Protected Area Effectiveness\nEvaluating the effectiveness of protected areas is essential for conservation planning and management.\n\n8.6.1 Example: Before-After-Control-Impact (BACI) Analysis\n\n\nCode\n# Simulate protected area effectiveness data\nset.seed(789)\nn_sites &lt;- 20\nn_years &lt;- 10\n\n# Create site, protection status, and year variables\nsite &lt;- rep(paste0(\"Site\", 1:n_sites), each = n_years)\nprotected &lt;- rep(rep(c(\"Protected\", \"Unprotected\"), each = n_sites/2), each = n_years)\nyear &lt;- rep(2013:(2013 + n_years - 1), times = n_sites)\nperiod &lt;- ifelse(year &lt; 2018, \"Before\", \"After\")  # Protection started in 2018\n\n# Create random site effects and impact of protection\nsite_effect &lt;- rep(rnorm(n_sites, 0, 0.5), each = n_years)\nprotection_effect &lt;- ifelse(protected == \"Protected\" & period == \"After\", 0.3, 0)\ntime_effect &lt;- -0.05 * (year - 2013)  # General declining trend\nnoise &lt;- rnorm(n_sites * n_years, 0, 0.2)\n\n# Calculate biodiversity index\nbiodiversity &lt;- 5 + site_effect + time_effect + protection_effect + noise\n\n# Create a data frame\npa_data &lt;- data.frame(\n  site = factor(site),\n  protected = factor(protected),\n  year = year,\n  period = factor(period),\n  biodiversity = biodiversity\n)\n\n# Visualize the data\nggplot(pa_data, aes(x = year, y = biodiversity, color = protected, group = interaction(site, protected))) +\n  geom_line(alpha = 0.3) +\n  stat_summary(aes(group = protected), fun = mean, geom = \"line\", size = 1.5) +\n  geom_vline(xintercept = 2018, linetype = \"dashed\") +\n  labs(title = \"Biodiversity Trends in Protected and Unprotected Sites\",\n       subtitle = \"Vertical line indicates when protection was implemented\",\n       x = \"Year\",\n       y = \"Biodiversity Index\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a BACI model\nbaci_model &lt;- lm(biodiversity ~ protected * period, data = pa_data)\n\n# Display model summary\nsummary(baci_model)\n\n\n\nCall:\nlm(formula = biodiversity ~ protected * period, data = pa_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18762 -0.25169  0.00786  0.29460  0.93568 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        4.79029    0.05943  80.604  &lt; 2e-16 ***\nprotectedUnprotected              -0.29698    0.08405  -3.534 0.000511 ***\nperiodBefore                      -0.08027    0.08405  -0.955 0.340742    \nprotectedUnprotected:periodBefore  0.33219    0.11886   2.795 0.005709 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4202 on 196 degrees of freedom\nMultiple R-squared:  0.06998,   Adjusted R-squared:  0.05574 \nF-statistic: 4.916 on 3 and 196 DF,  p-value: 0.002578\n\n\nCode\n# Visualize the interaction effect\npa_summary &lt;- aggregate(biodiversity ~ protected + period, data = pa_data, FUN = mean)\n\nggplot(pa_summary, aes(x = period, y = biodiversity, color = protected, group = protected)) +\n  geom_point(size = 3) +\n  geom_line() +\n  labs(title = \"BACI Design: Interaction between Protection Status and Time Period\",\n       x = \"Period\",\n       y = \"Mean Biodiversity Index\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#threat-assessment-and-prioritization",
    "href": "chapters/09-conservation.html#threat-assessment-and-prioritization",
    "title": "8  Conservation Applications",
    "section": "8.7 Threat Assessment and Prioritization",
    "text": "8.7 Threat Assessment and Prioritization\nConservation resources are limited, so prioritizing threats and actions is essential.\n\n8.7.1 Example: Multi-Criteria Decision Analysis\n\n\nCode\n# Create a threat assessment dataset\nthreats &lt;- c(\"Habitat Loss\", \"Invasive Species\", \"Climate Change\", \"Pollution\", \"Overexploitation\")\nseverity &lt;- c(0.9, 0.7, 0.8, 0.6, 0.7)\nscope &lt;- c(0.8, 0.6, 0.9, 0.5, 0.6)\nirreversibility &lt;- c(0.9, 0.7, 0.9, 0.4, 0.5)\n\n# Create a data frame\nthreat_data &lt;- data.frame(\n  threat = threats,\n  severity = severity,\n  scope = scope,\n  irreversibility = irreversibility\n)\n\n# Calculate overall threat magnitude\nthreat_data$magnitude &lt;- with(threat_data, severity * scope * irreversibility)\n\n# Sort by magnitude\nthreat_data &lt;- threat_data[order(threat_data$magnitude, decreasing = TRUE), ]\n\n# Visualize the threat assessment\nggplot(threat_data, aes(x = reorder(threat, magnitude), y = magnitude)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Threat Prioritization Based on Magnitude\",\n       x = \"Threat\",\n       y = \"Magnitude (Severity × Scope × Irreversibility)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Visualize the components\nthreat_data_long &lt;- reshape2::melt(threat_data[, c(\"threat\", \"severity\", \"scope\", \"irreversibility\")],\n                                 id.vars = \"threat\")\n\nggplot(threat_data_long, aes(x = reorder(threat, -value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Components of Threat Assessment\",\n       x = \"Threat\",\n       y = \"Score\",\n       fill = \"Component\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#conservation-planning",
    "href": "chapters/09-conservation.html#conservation-planning",
    "title": "8  Conservation Applications",
    "section": "8.8 Conservation Planning",
    "text": "8.8 Conservation Planning\nSystematic conservation planning helps identify priority areas for conservation.\n\n8.8.1 Example: Complementarity Analysis\n\n\nCode\n# Create a species-by-site matrix\nset.seed(101)\nn_sites &lt;- 10\nn_species &lt;- 15\nspecies_names &lt;- paste0(\"Species\", 1:n_species)\nsite_names &lt;- paste0(\"Site\", 1:n_sites)\n\n# Generate presence/absence data\npresence_prob &lt;- matrix(runif(n_sites * n_species, 0, 1), nrow = n_sites, ncol = n_species)\npresence &lt;- ifelse(presence_prob &gt; 0.7, 0, 1)  # 30% chance of presence\nrownames(presence) &lt;- site_names\ncolnames(presence) &lt;- species_names\n\n# Calculate species richness per site\nrichness &lt;- rowSums(presence)\n\n# Calculate site complementarity\ncomplementarity &lt;- function(selected, candidates, presence_matrix) {\n  if (length(selected) == 0) {\n    # If no sites selected yet, return site richness\n    return(rowSums(presence_matrix[candidates, , drop = FALSE]))\n  } else {\n    # Calculate new species added by each candidate site\n    species_in_selected &lt;- colSums(presence_matrix[selected, , drop = FALSE]) &gt; 0\n    new_species &lt;- function(site) {\n      sum(presence_matrix[site, ] & !species_in_selected)\n    }\n    return(sapply(candidates, new_species))\n  }\n}\n\n# Greedy algorithm for site selection\nselect_sites &lt;- function(presence_matrix, n_to_select) {\n  n_sites &lt;- nrow(presence_matrix)\n  available_sites &lt;- 1:n_sites\n  selected_sites &lt;- integer(0)\n  \n  for (i in 1:n_to_select) {\n    if (length(available_sites) == 0) break\n    \n    # Calculate complementarity scores\n    scores &lt;- complementarity(selected_sites, available_sites, presence_matrix)\n    \n    # Select site with highest score\n    best &lt;- available_sites[which.max(scores)]\n    selected_sites &lt;- c(selected_sites, best)\n    available_sites &lt;- setdiff(available_sites, best)\n  }\n  \n  return(selected_sites)\n}\n\n# Select 3 priority sites\npriority_sites &lt;- select_sites(presence, 3)\ncat(\"Priority sites:\", site_names[priority_sites], \"\\n\")\n\n\nPriority sites: Site7 Site8 Site1 \n\n\nCode\n# Calculate species coverage\nspecies_covered &lt;- colSums(presence[priority_sites, , drop = FALSE]) &gt; 0\ncat(\"Species covered:\", sum(species_covered), \"out of\", n_species, \n    \"(\", round(100 * sum(species_covered) / n_species, 1), \"%)\\n\")\n\n\nSpecies covered: 15 out of 15 ( 100 %)\n\n\nCode\n# Visualize the species-site matrix\nlibrary(pheatmap)\npheatmap(presence, \n        cluster_rows = FALSE, \n        cluster_cols = FALSE,\n        main = \"Species Presence by Site\",\n        color = c(\"white\", \"steelblue\"),\n        labels_row = site_names,\n        labels_col = species_names,\n        display_numbers = TRUE,\n        number_color = \"black\",\n        fontsize = 10,\n        fontsize_number = 8)\n\n\n\n\n\n\n\n\n\nCode\n# Highlight priority sites\npriority_data &lt;- data.frame(\n  Priority = factor(ifelse(1:n_sites %in% priority_sites, \"Selected\", \"Not Selected\"))\n)\nrownames(priority_data) &lt;- site_names\n\npheatmap(presence, \n        cluster_rows = FALSE, \n        cluster_cols = FALSE,\n        main = \"Priority Sites for Conservation\",\n        color = c(\"white\", \"steelblue\"),\n        labels_row = site_names,\n        labels_col = species_names,\n        display_numbers = TRUE,\n        number_color = \"black\",\n        annotation_row = priority_data,\n        fontsize = 10,\n        fontsize_number = 8)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#climate-change-vulnerability-assessment",
    "href": "chapters/09-conservation.html#climate-change-vulnerability-assessment",
    "title": "8  Conservation Applications",
    "section": "8.9 Climate Change Vulnerability Assessment",
    "text": "8.9 Climate Change Vulnerability Assessment\nClimate change poses significant threats to biodiversity. Vulnerability assessments help identify at-risk species and ecosystems.\n\n8.9.1 Example: Trait-Based Vulnerability Analysis\n\n\nCode\n# Create a species trait dataset\nspecies &lt;- paste0(\"Species\", 1:12)\ndispersal_ability &lt;- c(1, 3, 2, 1, 3, 2, 1, 2, 3, 1, 2, 3)  # 1=low, 2=medium, 3=high\nthermal_tolerance &lt;- c(1, 2, 3, 1, 2, 3, 2, 3, 1, 3, 1, 2)  # 1=low, 2=medium, 3=high\nhabitat_specificity &lt;- c(3, 2, 1, 3, 1, 2, 3, 2, 1, 2, 3, 1)  # 1=low, 2=medium, 3=high\npopulation_size &lt;- c(1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 2, 3)  # 1=small, 2=medium, 3=large\n\n# Create a data frame\nvulnerability_data &lt;- data.frame(\n  species = species,\n  dispersal_ability = dispersal_ability,\n  thermal_tolerance = thermal_tolerance,\n  habitat_specificity = habitat_specificity,\n  population_size = population_size\n)\n\n# Calculate vulnerability scores (higher = more vulnerable)\nvulnerability_data$sensitivity &lt;- 4 - thermal_tolerance\nvulnerability_data$adaptive_capacity &lt;- 4 - (dispersal_ability + population_size) / 2\nvulnerability_data$exposure &lt;- habitat_specificity\nvulnerability_data$vulnerability &lt;- with(vulnerability_data, \n                                       (sensitivity + adaptive_capacity + exposure) / 3)\n\n# Sort by vulnerability\nvulnerability_data &lt;- vulnerability_data[order(vulnerability_data$vulnerability, decreasing = TRUE), ]\n\n# Visualize vulnerability scores\nggplot(vulnerability_data, aes(x = reorder(species, -vulnerability), y = vulnerability)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Climate Change Vulnerability by Species\",\n       x = \"Species\",\n       y = \"Vulnerability Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Visualize components\nvulnerability_components &lt;- vulnerability_data[, c(\"species\", \"sensitivity\", \"adaptive_capacity\", \"exposure\")]\nvulnerability_long &lt;- reshape2::melt(vulnerability_components, id.vars = \"species\")\n\nggplot(vulnerability_long, aes(x = reorder(species, -value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Components of Climate Change Vulnerability\",\n       x = \"Species\",\n       y = \"Score\",\n       fill = \"Component\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#community-based-conservation-monitoring",
    "href": "chapters/09-conservation.html#community-based-conservation-monitoring",
    "title": "8  Conservation Applications",
    "section": "8.10 Community-Based Conservation Monitoring",
    "text": "8.10 Community-Based Conservation Monitoring\nInvolving local communities in conservation monitoring can improve data collection and conservation outcomes.\n\n8.10.1 Example: Analyzing Community Monitoring Data\n\n\nCode\n# Simulate community monitoring data\nset.seed(202)\nn_villages &lt;- 5\nn_months &lt;- 24\n\n# Create variables\nvillage &lt;- rep(paste0(\"Village\", 1:n_villages), each = n_months)\nmonth &lt;- rep(1:n_months, times = n_villages)\nyear &lt;- rep(rep(c(1, 2), each = 12), times = n_villages)\n\n# Generate poaching incidents with seasonal pattern and declining trend\nseason &lt;- sin(month * pi / 6) + 1  # Seasonal pattern\ntrend &lt;- -0.03 * (month - 1)  # Declining trend\nvillage_effect &lt;- rep(rnorm(n_villages, 0, 0.5), each = n_months)\nlambda &lt;- exp(1 + 0.5 * season + trend + village_effect)\npoaching &lt;- rpois(n_villages * n_months, lambda)\n\n# Create a data frame\nmonitoring_data &lt;- data.frame(\n  village = factor(village),\n  month = month,\n  year = factor(year),\n  poaching = poaching\n)\n\n# Visualize the data\nggplot(monitoring_data, aes(x = month, y = poaching, color = village, group = village)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~year, scales = \"free_x\", labeller = labeller(year = c(\"1\" = \"Year 1\", \"2\" = \"Year 2\"))) +\n  labs(title = \"Poaching Incidents Reported by Community Monitors\",\n       x = \"Month\",\n       y = \"Number of Incidents\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Analyze trends\nlibrary(MASS)\ntrend_model &lt;- glm.nb(poaching ~ month + village, data = monitoring_data)\nsummary(trend_model)\n\n\n\nCall:\nglm.nb(formula = poaching ~ month + village, data = monitoring_data, \n    init.theta = 21.97524464, link = log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      1.229650   0.181336   6.781 1.19e-11 ***\nmonth           -0.057015   0.008742  -6.522 6.94e-11 ***\nvillageVillage2  0.545243   0.201645   2.704 0.006852 ** \nvillageVillage3  0.558968   0.201193   2.778 0.005465 ** \nvillageVillage4  0.270966   0.211843   1.279 0.200866    \nvillageVillage5  0.716424   0.196360   3.649 0.000264 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(21.9752) family taken to be 1)\n\n    Null deviance: 199.13  on 119  degrees of freedom\nResidual deviance: 136.01  on 114  degrees of freedom\nAIC: 468.75\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  22.0 \n          Std. Err.:  23.9 \n\n 2 x log-likelihood:  -454.752 \n\n\nCode\n# Calculate overall trend\ntrend_coef &lt;- coef(trend_model)[\"month\"]\nmonthly_change &lt;- (exp(trend_coef) - 1) * 100\ncat(\"Monthly change in poaching incidents:\", round(monthly_change, 2), \"%\\n\")\n\n\nMonthly change in poaching incidents: -5.54 %\n\n\nCode\n# Analyze seasonal patterns\nseason_model &lt;- glm.nb(poaching ~ sin(2 * pi * month / 12) + cos(2 * pi * month / 12) + village, \n                      data = monitoring_data)\nsummary(season_model)\n\n\n\nCall:\nglm.nb(formula = poaching ~ sin(2 * pi * month/12) + cos(2 * \n    pi * month/12) + village, data = monitoring_data, init.theta = 40.9900692, \n    link = log)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.48468    0.15815   3.065 0.002180 ** \nsin(2 * pi * month/12)  0.64312    0.08539   7.531 5.03e-14 ***\ncos(2 * pi * month/12)  0.08170    0.08155   1.002 0.316459    \nvillageVillage2         0.55310    0.19722   2.805 0.005039 ** \nvillageVillage3         0.57202    0.19658   2.910 0.003617 ** \nvillageVillage4         0.28057    0.20754   1.352 0.176412    \nvillageVillage5         0.72313    0.19186   3.769 0.000164 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(40.9901) family taken to be 1)\n\n    Null deviance: 209.78  on 119  degrees of freedom\nResidual deviance: 129.01  on 113  degrees of freedom\nAIC: 457.46\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  41.0 \n          Std. Err.:  70.8 \n\n 2 x log-likelihood:  -441.462 \n\n\nCode\n# Compare models\nanova(trend_model, season_model)\n\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: poaching\n                                                      Model    theta Resid. df\n1                                           month + village 21.97524       114\n2 sin(2 * pi * month/12) + cos(2 * pi * month/12) + village 40.99007       113\n     2 x log-lik.   Test    df LR stat.      Pr(Chi)\n1       -454.7522                                   \n2       -441.4616 1 vs 2     1  13.2906 0.0002667407",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#summary",
    "href": "chapters/09-conservation.html#summary",
    "title": "8  Conservation Applications",
    "section": "8.11 Summary",
    "text": "8.11 Summary\nIn this chapter, we’ve explored how data analysis techniques can be applied to conservation challenges:\n\nSpecies distribution modeling to predict habitat suitability\nPopulation trend analysis to monitor species status\nHabitat fragmentation analysis to assess landscape connectivity\nProtected area effectiveness evaluation using BACI designs\nThreat assessment and prioritization for conservation planning\nSystematic conservation planning using complementarity analysis\nClimate change vulnerability assessment based on species traits\nCommunity-based conservation monitoring to track threats\n\nThese applications demonstrate how the statistical methods covered throughout this book can help address real-world conservation problems, inform management decisions, and ultimately contribute to biodiversity conservation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#exercises",
    "href": "chapters/09-conservation.html#exercises",
    "title": "8  Conservation Applications",
    "section": "8.12 Exercises",
    "text": "8.12 Exercises\n\nImport a dataset on species occurrences and environmental variables, then build a simple species distribution model.\nAnalyze population monitoring data to detect trends and assess conservation status.\nCalculate basic landscape metrics for a land cover map to quantify habitat fragmentation.\nDesign and analyze a BACI study to evaluate the effectiveness of a conservation intervention.\nConduct a threat assessment for a species or ecosystem of your choice.\nUse complementarity analysis to identify priority sites for conservation.\nPerform a climate change vulnerability assessment for a group of species.\nAnalyze community monitoring data to detect trends in threats or biodiversity.\n\n\n\n\n\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species distribution models: Ecological explanation and prediction across space and time. Annual Review of Ecology, Evolution, and Systematics, 40, 677–697.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bolker, B. et al. (2009). Generalized linear mixed models: A\npractical guide. Trends in Ecology & Evolution.\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species\ndistribution models: Ecological explanation and prediction across space\nand time. Annual Review of Ecology, Evolution, and Systematics,\n40, 677–697.\n\n\nGotelli, N. J., & Ellison, A. M. (2004). Null model analysis of\nspecies co-occurrence patterns. Sinauer Associates.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import,\ntidy, transform, visualize, and model data. O’Reilly Media, Inc.\n\n\nZuur, A., Ieno, E. N., & Smith, G. M. (2007). Analyzing\necological data. Springer.\n\n\nZuur, A., Ieno, E. N., Walker, N., Saveliev, A. A., & Smith, G. M.\n(2009). Mixed effects models and extensions in ecology with r.\nSpringer Science & Business Media.",
    "crumbs": [
      "References"
    ]
  }
]