[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "",
    "text": "Preface\nWelcome to Data Analysis in Natural Sciences: An R-Based Approach, a comprehensive guide designed for students, professionals, and researchers across the natural sciences. This book provides practical methods for analyzing and visualizing data using R, with applications spanning forestry, agriculture, ecology, marine biology, environmental science, geology, atmospheric science, hydrology, and more.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "About the Author",
    "text": "About the Author\nThis book has been developed by Dr. Jimmy Moses (PhD) from the School of Forestry, Faculty of Natural Resources, Papua New Guinea University of Technology. With extensive experience in ecological research and data analysis, Dr. Moses has created this resource to support students and researchers in developing essential analytical skills for natural science disciplines.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Target Audience",
    "text": "Target Audience\nThis book is designed for:\n\nUndergraduate and postgraduate students in natural science disciplines\nResearchers seeking to enhance their data analysis capabilities\nTechnicians working in laboratories and field settings\nProfessionals in government agencies, NGOs, and private sector\nHobbyists with an interest in analyzing scientific data\n\nThe content is relevant to those working in:\n\nForestry and agroforestry\nAgriculture and agronomy\nEcology and conservation\nEnvironmental science\nGeography and GIS/remote sensing\nMarine biology and fisheries\nBotany and plant sciences\nEntomology and zoology\nEpidemiology and veterinary sciences\nGeology and earth sciences\nAtmospheric and climate sciences\nHydrology and water resources\nNatural resource management\nConservation biology",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nThis book will guide you through:\n\nThe fundamentals of data analysis with R\nData preparation and management techniques\nExploratory data analysis approaches\nStatistical hypothesis testing\nAdvanced visualization methods\nSpecialized analyses for environmental and scientific data\nReproducible research practices",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nThis book is designed to be both a learning resource and a reference guide. You can read it from start to finish to build your skills progressively, or use specific chapters as needed for particular tasks.\nCode examples are provided throughout, and you can run them directly in R or RStudio. Each chapter includes practical examples using real datasets from various natural science disciplines.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo get the most out of this book, you should have:\n\nBasic computer skills\nR and RStudio installed (instructions provided in Chapter 1)\nA basic understanding of statistics (helpful but not required)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Data Analysis in Natural Sciences: An R-Based Approach",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to thank all those who contributed to the development of this book, including colleagues, students, and the open-source community that makes tools like R and RStudio possible.\nLet’s begin our journey into the world of data analysis for natural sciences!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "About This Book",
    "section": "",
    "text": "About the Author\nDr. Jimmy Moses is a Papua New Guinean entomologist and lecturer at the Papua New Guinea University of Technology’s School of Forestry, specializing in ant ecology, biostatistics, and geospatial analysis. He holds a Ph.D. in Entomology from the University of South Bohemia (2021) and has extensive experience in tropical ecology research, particularly focusing on ant communities along elevational gradients.\nHe currently supervises four master’s students and co-supervises a Ph.D. student, Dr. Moses brings significant expertise in both research and education. He has published several peer-reviewed papers, including work in prestigious journals like Global Ecology and Biogeography and Proceedings of the Royal Society B.\nHis technical skills span multiple areas:\nDr. Moses has maintained strong international collaborations, having worked with institutions in the Czech Republic, Germany, and Belgium. He has been actively involved with the New Guinea Binatang Research Center, contributing to both research and education initiatives in Papua New Guinea.\nHis research interests combine ecological field studies with modern analytical approaches, particularly in ant ecology, spatial ecology, macroecology, and crop protection. He spends his free time reading technical, historical, psychological and ecological books and more time tinkering with codes.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#about-the-author",
    "href": "preface.html#about-the-author",
    "title": "About This Book",
    "section": "",
    "text": "Advanced proficiency in R and Python for statistical computing and data science\nExpertise in GIS and Satellite Remote Sensing\nStrong background in biostatistics and experimental design\nEmerging skills in full-stack app development",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#purpose-and-scope",
    "href": "preface.html#purpose-and-scope",
    "title": "About This Book",
    "section": "Purpose and Scope",
    "text": "Purpose and Scope\nThis book is designed to serve as both a learning resource and a reference guide for data analysis in the natural sciences, with applications spanning forestry, agriculture, ecology, environmental science, marine biology, and related disciplines. Whether you’re a student, researcher, technician, professional, or hobbyist in these fields, this book will help you develop the skills needed to analyze and visualize data effectively using R.\nThe focus is on practical applications rather than theoretical statistics, with an emphasis on techniques commonly used across natural science disciplines. By working through this book, you will:\n\nMaster the fundamentals of data analysis in R\nLearn to import, clean, and organize various types of scientific data\nDevelop skills in exploratory data analysis and visualization\nApply appropriate statistical tests for different research questions\nCreate publication-quality visualizations\nImplement reproducible research workflows\nInterpret and communicate results effectively",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#features-of-this-book",
    "href": "preface.html#features-of-this-book",
    "title": "About This Book",
    "section": "Features of This Book",
    "text": "Features of This Book\nThis book includes:\n\nStep-by-step instructions for R with complete code examples\nPractical examples using real datasets from various natural science disciplines\nExercises to reinforce learning and build skills\nTips and best practices from experienced researchers\nReproducible code that can be adapted for your own research",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#how-to-use-the-code-examples",
    "href": "preface.html#how-to-use-the-code-examples",
    "title": "About This Book",
    "section": "How to Use the Code Examples",
    "text": "How to Use the Code Examples\nAll code examples in this book are written in R and can be executed in RStudio. To use the examples:\n\nMake sure you have R and RStudio installed (see Chapter 1 for installation instructions)\nInstall the required packages mentioned at the beginning of each chapter\nCopy and paste the code into your R console or script editor\nModify the code as needed for your own data\n\nThe datasets used in the examples are available in the docs/data directory of the book’s repository and are properly cited throughout the text.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#software-requirements",
    "href": "preface.html#software-requirements",
    "title": "About This Book",
    "section": "Software Requirements",
    "text": "Software Requirements\nThis book uses:\n\nR (version 4.0.0 or higher)\nRStudio (latest version recommended)\nVarious R packages (installation instructions provided in each chapter)",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#feedback-and-contributions",
    "href": "preface.html#feedback-and-contributions",
    "title": "About This Book",
    "section": "Feedback and Contributions",
    "text": "Feedback and Contributions\nYour feedback is valuable for improving future editions of this book. If you find errors, have suggestions, or want to contribute examples, please submit them through the book’s repository or contact the author directly.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html#acknowledgments",
    "href": "preface.html#acknowledgments",
    "title": "About This Book",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to express my gratitude to colleagues, students, and the broader R community whose insights and feedback have contributed to the development of this book. Special thanks to the creators and maintainers of the R packages used throughout this book, as well as the data providers whose datasets make the examples both practical and relevant.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "1  Introduction to Data Analysis",
    "section": "",
    "text": "1.1 Overview\nData analysis is a critical skill in modern natural sciences research (Wickham & Grolemund, 2016; Zuur et al., 2009). This chapter introduces the fundamental concepts, tools, and approaches that form the foundation of effective data analysis across various scientific disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-data-analysis-matters-in-natural-sciences",
    "href": "chapters/01-introduction.html#why-data-analysis-matters-in-natural-sciences",
    "title": "1  Introduction to Data Analysis",
    "section": "1.2 Why Data Analysis Matters in Natural Sciences",
    "text": "1.2 Why Data Analysis Matters in Natural Sciences\nData analysis plays a pivotal role in natural sciences research for several reasons:\n\nEvidence-Based Decision Making: Data analysis transforms raw observations into actionable insights, enabling researchers and practitioners to make informed decisions about conservation strategies, resource management practices, agricultural planning, environmental interventions, and more (Bolker et al., 2009).\nPattern Recognition: Through statistical analysis, researchers can identify patterns, trends, and relationships within natural systems that might not be apparent from casual observation alone (Zuur et al., 2007). This applies to diverse fields including ecology, geology, marine biology, atmospheric science, and agriculture.\nHypothesis Testing: Data analysis provides rigorous methods to test hypotheses about natural phenomena, allowing researchers to build and refine scientific theories about how natural systems function (Gotelli & Ellison, 2004). This is fundamental across all scientific disciplines.\nPrediction and Modeling: Advanced analytical techniques enable the development of predictive models that can forecast changes in natural systems, such as species distribution shifts under climate change, crop yield predictions, geological processes, weather patterns, and more (Elith et al., 2009).",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#tools-for-data-analysis",
    "href": "chapters/01-introduction.html#tools-for-data-analysis",
    "title": "1  Introduction to Data Analysis",
    "section": "1.3 Tools for Data Analysis",
    "text": "1.3 Tools for Data Analysis\nThis book focuses on R and RStudio as the primary tools for data analysis:\n\n1.3.1 R and RStudio\nR is a powerful programming language and environment specifically designed for statistical computing and graphics. RStudio is an integrated development environment (IDE) that makes working with R more accessible and efficient.\nKey advantages of R include:\n\nOpen-source and free: Available to anyone without cost\nExtensive package ecosystem: Thousands of specialized packages for various types of analyses across all scientific disciplines\nReproducibility: Code-based approach ensures analyses can be repeated and verified\nFlexibility: Can be adapted to virtually any analytical need in the natural sciences\nActive community: Large user base provides support and continuous development\n\n\n\nCode\n# A simple example of R code using real-world data\n# Load the Palmer penguins dataset (a subset of climate_data.csv)\npenguins &lt;- read.csv(\"../data/environmental/climate_data.csv\")\n\n# View the first few rows\nhead(penguins)\n\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n6  Adelie Torgersen           39.3          20.6               190        3650\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n6   male 2007\n\n\nCode\n# Get a summary of bill length measurements\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#setting-up-your-environment",
    "href": "chapters/01-introduction.html#setting-up-your-environment",
    "title": "1  Introduction to Data Analysis",
    "section": "1.4 Setting Up Your Environment",
    "text": "1.4 Setting Up Your Environment\n\n1.4.1 Installing R and RStudio\nTo install R and RStudio:\n\nDownload and install R from CRAN\nDownload and install RStudio from RStudio’s website\n\n\n\n1.4.2 Essential R Packages\nFor the analyses in this book, you’ll need several R packages. You can install them with the following code:\n\n\nCode\ninstall.packages(c(\n  \"tidyverse\",  # Data manipulation and visualization\n  \"rstatix\",    # Statistical tests\n  \"ggplot2\",    # Advanced plotting\n  \"knitr\",      # Document generation\n  \"rmarkdown\"   # Document formatting\n))",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#the-data-analysis-workflow",
    "href": "chapters/01-introduction.html#the-data-analysis-workflow",
    "title": "1  Introduction to Data Analysis",
    "section": "1.5 The Data Analysis Workflow",
    "text": "1.5 The Data Analysis Workflow\nEffective data analysis typically follows a structured workflow:\n\nDefine the Question: Clearly articulate what you want to learn from your data\nCollect Data: Gather the necessary data through fieldwork, experiments, laboratory measurements, or existing datasets\nClean and Prepare Data: Handle missing values, correct errors, and format data appropriately\nExplore Data: Conduct exploratory data analysis to understand patterns and distributions\nAnalyze Data: Apply appropriate statistical methods to address your research questions\nInterpret Results: Draw conclusions based on your analysis\nCommunicate Findings: Present your results through visualizations, reports, or publications\n\nThroughout this book, we’ll follow this workflow as we explore various datasets from across the natural sciences.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#types-of-data-in-natural-sciences-research",
    "href": "chapters/01-introduction.html#types-of-data-in-natural-sciences-research",
    "title": "1  Introduction to Data Analysis",
    "section": "1.6 Types of Data in Natural Sciences Research",
    "text": "1.6 Types of Data in Natural Sciences Research\nResearch across the natural sciences involves several types of data:\n\n1.6.1 Categorical Data\nCategorical data represent qualitative characteristics, such as: - Species names or taxonomic classifications - Habitat or ecosystem types - Rock or soil classifications - Land-use categories - Treatment groups in experiments - Genetic markers\n\n\n1.6.2 Numerical Data\nNumerical data involve measurements or counts: - Continuous measurements (e.g., temperature, pH, concentration, biomass, wavelength) - Discrete counts (e.g., number of individuals, species richness, occurrence frequency) - Rates (e.g., growth rates, reaction rates, decomposition rates) - Ratios and indices (e.g., diversity indices, chemical ratios)\n\n\n1.6.3 Spatial Data\nSpatial data describe geographical distributions: - Coordinates (latitude/longitude) - Elevation or depth - Topographic features - Land cover maps - Remote sensing data - Geological formations\n\n\n1.6.4 Temporal Data\nTemporal data track changes over time: - Time series of measurements - Seasonal patterns - Long-term monitoring data - Growth curves - Decay rates - Historical records\nUnderstanding the type of data you’re working with is crucial for selecting appropriate analytical methods across all natural science disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#summary",
    "href": "chapters/01-introduction.html#summary",
    "title": "1  Introduction to Data Analysis",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nIn this chapter, we’ve introduced the importance of data analysis in natural sciences research and the tools we’ll be using throughout this book. We’ve also outlined the typical data analysis workflow and the types of data commonly encountered across scientific disciplines.\nIn the next chapter, we’ll dive deeper into data basics, learning how to import, clean, and prepare data for analysis.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#exercises",
    "href": "chapters/01-introduction.html#exercises",
    "title": "1  Introduction to Data Analysis",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\n\nInstall R and RStudio on your computer.\nInstall the required R packages listed in this chapter.\nOpen RStudio and create a new R script. Try running a simple command like summary(iris).\nThink about a research question in your field of natural science that interests you. What type of data would you need to address this question?\nExplore one of R’s built-in datasets (e.g., mtcars, iris, or trees) using functions like head(), summary(), and plot().\n\n\n\n\n\n\n\nBolker, B. et al. (2009). Generalized linear mixed models: A practical guide. Trends in Ecology & Evolution.\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species distribution models: Ecological explanation and prediction across space and time. Annual Review of Ecology, Evolution, and Systematics, 40, 677–697.\n\n\nGotelli, N. J., & Ellison, A. M. (2004). Null model analysis of species co-occurrence patterns. Sinauer Associates.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.\n\n\nZuur, A., Ieno, E. N., & Smith, G. M. (2007). Analyzing ecological data. Springer.\n\n\nZuur, A., Ieno, E. N., Walker, N., Saveliev, A. A., & Smith, G. M. (2009). Mixed effects models and extensions in ecology with r. Springer Science & Business Media.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html",
    "href": "chapters/02-data-basics.html",
    "title": "2  Data Basics",
    "section": "",
    "text": "2.1 Introduction\nThis chapter covers the fundamental concepts of working with data in R. You’ll learn how to import, clean, and prepare data for analysis, which are essential skills for any data analysis project across all natural science disciplines.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#understanding-data-structures",
    "href": "chapters/02-data-basics.html#understanding-data-structures",
    "title": "2  Data Basics",
    "section": "2.2 Understanding Data Structures",
    "text": "2.2 Understanding Data Structures\nBefore diving into data analysis, it’s important to understand the basic data structures in R:\n\n2.2.1 Data Types\nR has several basic data types:\n\nNumeric: Decimal values (e.g., measurements of temperature, pH, concentration, or distance)\nInteger: Whole numbers (e.g., counts of organisms, samples, or observations)\nCharacter: Text strings (e.g., species names, site descriptions, or treatment labels)\nLogical: TRUE/FALSE values (e.g., presence/absence data or condition met/not met)\nFactor: Categorical variables with levels (e.g., experimental treatments, taxonomic classifications, or soil types)\nDate/Time: Temporal data (e.g., sampling dates, observation times, or seasonal markers)\n\n\n\nCode\n# Examples of different data types\nnumeric_example &lt;- 25.4  # Temperature in Celsius\ncharacter_example &lt;- \"Adelie\"  # Penguin species\nlogical_example &lt;- TRUE  # Presence/absence data\nfactor_example &lt;- factor(c(\"Control\", \"Treatment\", \"Control\"), \n                         levels = c(\"Control\", \"Treatment\"))\ndate_example &lt;- as.Date(\"2020-07-15\")  # Sampling date\n\n# Print examples\nprint(numeric_example)\n\n\n[1] 25.4\n\n\nCode\nprint(character_example)\n\n\n[1] \"Adelie\"\n\n\nCode\nprint(logical_example)\n\n\n[1] TRUE\n\n\nCode\nprint(factor_example)\n\n\n[1] Control   Treatment Control  \nLevels: Control Treatment\n\n\nCode\nprint(date_example)\n\n\n[1] \"2020-07-15\"\n\n\n\n\n2.2.2 Data Structures in R\nR has several data structures for organizing information:\n\n\nCode\n# Load real datasets\nlibrary(readr)\npenguins &lt;- read_csv(\"../data/environmental/climate_data.csv\")\ncrops &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# Vector example - penguin bill lengths\nbill_lengths &lt;- na.omit(penguins$bill_length_mm[1:10])\nprint(bill_lengths)\n\n\n[1] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 34.1 42.0\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n\nCode\n# Matrix example - create a matrix from penguin measurements\npenguin_matrix &lt;- as.matrix(penguins[1:5, 3:6])\nprint(penguin_matrix)\n\n\n     bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n[1,]           39.1          18.7               181        3750\n[2,]           39.5          17.4               186        3800\n[3,]           40.3          18.0               195        3250\n[4,]             NA            NA                NA          NA\n[5,]           36.7          19.3               193        3450\n\n\nCode\n# Data frame example - first few rows of penguin data\npenguin_data &lt;- penguins[1:5, ]\nprint(penguin_data)\n\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# List example - store different aspects of the dataset\npenguin_summary &lt;- list(\n  species = unique(penguins$species),\n  avg_bill_length = mean(penguins$bill_length_mm, na.rm = TRUE),\n  sample_size = nrow(penguins),\n  years = unique(penguins$year)\n)\nprint(penguin_summary)\n\n\n$species\n[1] \"Adelie\"    \"Gentoo\"    \"Chinstrap\"\n\n$avg_bill_length\n[1] 43.92193\n\n$sample_size\n[1] 344\n\n$years\n[1] 2007 2008 2009",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#importing-data",
    "href": "chapters/02-data-basics.html#importing-data",
    "title": "2  Data Basics",
    "section": "2.3 Importing Data",
    "text": "2.3 Importing Data\n\n2.3.1 Reading Data Files\nR provides several functions for importing data from different file formats:\n\n\nCode\n# CSV files - Palmer Penguins dataset\npenguins_csv &lt;- read.csv(\"../data/environmental/climate_data.csv\")\nhead(penguins_csv, 3)\n\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n\n\nCode\n# Using the tidyverse approach for better handling\nlibrary(tidyverse)\npenguins_tidy &lt;- readr::read_csv(\"../data/environmental/climate_data.csv\")\nhead(penguins_tidy, 3)\n\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# Crop yields dataset\ncrops_csv &lt;- read.csv(\"../data/agriculture/crop_yields.csv\")\nhead(crops_csv, 3)\n\n\n       Entity Code Year Wheat..tonnes.per.hectare. Rice..tonnes.per.hectare.\n1 Afghanistan  AFG 1961                     1.0220                     1.519\n2 Afghanistan  AFG 1962                     0.9735                     1.519\n3 Afghanistan  AFG 1963                     0.8317                     1.519\n  Maize..tonnes.per.hectare. Soybeans..tonnes.per.hectare.\n1                      1.400                            NA\n2                      1.400                            NA\n3                      1.426                            NA\n  Potatoes..tonnes.per.hectare. Beans..tonnes.per.hectare.\n1                        8.6667                         NA\n2                        7.6667                         NA\n3                        8.1333                         NA\n  Peas..tonnes.per.hectare. Cassava..tonnes.per.hectare.\n1                        NA                           NA\n2                        NA                           NA\n3                        NA                           NA\n  Barley..tonnes.per.hectare. Cocoa.beans..tonnes.per.hectare.\n1                        1.08                               NA\n2                        1.08                               NA\n3                        1.08                               NA\n  Bananas..tonnes.per.hectare.\n1                           NA\n2                           NA\n3                           NA\n\n\n\n\n2.3.2 Exploring Real-World Datasets\nLet’s explore some of the real-world datasets we have available:\n\n\nCode\n# Palmer Penguins dataset\npenguins &lt;- read_csv(\"../data/environmental/climate_data.csv\")\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nCode\n# Basic summary statistics\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCode\nsummary(penguins$flipper_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nCode\n# Crop yields dataset\ncrops &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\nglimpse(crops)\n\n\nRows: 13,075\nColumns: 14\n$ Entity                             &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afgh…\n$ Code                               &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", …\n$ Year                               &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966,…\n$ `Wheat (tonnes per hectare)`       &lt;dbl&gt; 1.0220, 0.9735, 0.8317, 0.9510, 0.9…\n$ `Rice (tonnes per hectare)`        &lt;dbl&gt; 1.5190, 1.5190, 1.5190, 1.7273, 1.7…\n$ `Maize (tonnes per hectare)`       &lt;dbl&gt; 1.4000, 1.4000, 1.4260, 1.4257, 1.4…\n$ `Soybeans (tonnes per hectare)`    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Potatoes (tonnes per hectare)`    &lt;dbl&gt; 8.6667, 7.6667, 8.1333, 8.6000, 8.8…\n$ `Beans (tonnes per hectare)`       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Peas (tonnes per hectare)`        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Cassava (tonnes per hectare)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Barley (tonnes per hectare)`      &lt;dbl&gt; 1.0800, 1.0800, 1.0800, 1.0857, 1.0…\n$ `Cocoa beans (tonnes per hectare)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `Bananas (tonnes per hectare)`     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#data-cleaning-and-preparation",
    "href": "chapters/02-data-basics.html#data-cleaning-and-preparation",
    "title": "2  Data Basics",
    "section": "2.4 Data Cleaning and Preparation",
    "text": "2.4 Data Cleaning and Preparation\n\n2.4.1 Handling Missing Values\nMissing values are common in scientific datasets and need to be addressed before analysis:\n\n\nCode\n# Check for missing values in the penguins dataset\nsum(is.na(penguins))\n\n\n[1] 19\n\n\nCode\ncolSums(is.na(penguins))\n\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0 \n\n\nCode\n# Create a complete cases dataset\npenguins_complete &lt;- na.omit(penguins)\nprint(paste(\"Original dataset rows:\", nrow(penguins)))\n\n\n[1] \"Original dataset rows: 344\"\n\n\nCode\nprint(paste(\"Complete cases rows:\", nrow(penguins_complete)))\n\n\n[1] \"Complete cases rows: 333\"\n\n\nCode\n# Replace missing values with the mean for numeric columns\npenguins_imputed &lt;- penguins\npenguins_imputed$bill_length_mm[is.na(penguins_imputed$bill_length_mm)] &lt;- \n  mean(penguins_imputed$bill_length_mm, na.rm = TRUE)\npenguins_imputed$bill_depth_mm[is.na(penguins_imputed$bill_depth_mm)] &lt;- \n  mean(penguins_imputed$bill_depth_mm, na.rm = TRUE)\n\n# Check if missing values were replaced\nsum(is.na(penguins_imputed$bill_length_mm))\n\n\n[1] 0\n\n\n\n\n2.4.2 Data Transformation\nOften, you’ll need to transform variables to meet statistical assumptions or for better visualization:\n\n\nCode\n# Load the biodiversity dataset\nbiodiversity &lt;- read_csv(\"../data/ecology/biodiversity.csv\")\nglimpse(biodiversity)\n\n\nRows: 500\nColumns: 24\n$ binomial_name     &lt;chr&gt; \"Abutilon pitcairnense\", \"Acaena exigua\", \"Acalypha …\n$ country           &lt;chr&gt; \"Pitcairn\", \"United States\", \"Congo\", \"Saint Helena,…\n$ continent         &lt;chr&gt; \"Oceania\", \"North America\", \"Africa\", \"Africa\", \"Oce…\n$ group             &lt;chr&gt; \"Flowering Plant\", \"Flowering Plant\", \"Flowering Pla…\n$ year_last_seen    &lt;chr&gt; \"2000-2020\", \"1980-1999\", \"1940-1959\", \"Before 1900\"…\n$ threat_AA         &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1…\n$ threat_BRU        &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0…\n$ threat_RCD        &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…\n$ threat_ISGD       &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_EPM        &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_CC         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_HID        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_P          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_TS         &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_NSM        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_GE         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ threat_NA         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0…\n$ action_LWP        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ action_SM         &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_LP         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_RM         &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_EA         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ action_NA         &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ red_list_category &lt;chr&gt; \"Extinct in the Wild\", \"Extinct\", \"Extinct\", \"Extinc…\n\n\nCode\n# Log transformation of a skewed variable (if available)\nif(\"n\" %in% colnames(biodiversity)) {\n  biodiversity$log_n &lt;- log(biodiversity$n + 1)  # Add 1 to handle zeros\n  \n  # Compare original and transformed\n  summary(biodiversity$n)\n  summary(biodiversity$log_n)\n}\n\n# Standardization (z-score) of penguin measurements\npenguins_std &lt;- penguins %&gt;%\n  mutate(\n    bill_length_std = scale(bill_length_mm),\n    flipper_length_std = scale(flipper_length_mm),\n    body_mass_std = scale(body_mass_g)\n  )\n\n# View the first few rows of the transformed data\nhead(select(penguins_std, species, bill_length_mm, bill_length_std, \n             flipper_length_mm, flipper_length_std), 5)\n\n\n# A tibble: 5 × 5\n  species bill_length_mm bill_length_std[,1] flipper_length_mm\n  &lt;chr&gt;            &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1 Adelie            39.1              -0.883               181\n2 Adelie            39.5              -0.810               186\n3 Adelie            40.3              -0.663               195\n4 Adelie            NA                NA                    NA\n5 Adelie            36.7              -1.32                193\n# ℹ 1 more variable: flipper_length_std &lt;dbl[,1]&gt;\n\n\n\n\n2.4.3 Creating New Variables\nCreating new variables from existing ones is a common data preparation task:\n\n\nCode\n# Create new variables in the penguins dataset\npenguins_derived &lt;- penguins %&gt;%\n  filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm)) %&gt;%\n  mutate(\n    bill_ratio = bill_length_mm / bill_depth_mm,\n    size_category = case_when(\n      body_mass_g &lt; 3500 ~ \"Small\",\n      body_mass_g &lt; 4500 ~ \"Medium\",\n      TRUE ~ \"Large\"\n    )\n  )\n\n# View the new variables\nhead(select(penguins_derived, species, bill_length_mm, bill_depth_mm, \n             bill_ratio, body_mass_g, size_category), 5)\n\n\n# A tibble: 5 × 6\n  species bill_length_mm bill_depth_mm bill_ratio body_mass_g size_category\n  &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;        \n1 Adelie            39.1          18.7       2.09        3750 Medium       \n2 Adelie            39.5          17.4       2.27        3800 Medium       \n3 Adelie            40.3          18         2.24        3250 Small        \n4 Adelie            36.7          19.3       1.90        3450 Small        \n5 Adelie            39.3          20.6       1.91        3650 Medium",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#data-manipulation-with-dplyr",
    "href": "chapters/02-data-basics.html#data-manipulation-with-dplyr",
    "title": "2  Data Basics",
    "section": "2.5 Data Manipulation with dplyr",
    "text": "2.5 Data Manipulation with dplyr\nThe dplyr package provides a powerful grammar for data manipulation:\n\n\nCode\nlibrary(dplyr)\n\n# Filter rows - only Adelie penguins\nadelie_penguins &lt;- penguins %&gt;%\n  filter(species == \"Adelie\")\nhead(adelie_penguins, 3)\n\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nCode\n# Select columns - focus on measurements\npenguin_measurements &lt;- penguins %&gt;%\n  select(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)\nhead(penguin_measurements, 3)\n\n\n# A tibble: 3 × 6\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n\n\nCode\n# Create new variables\npenguins_analyzed &lt;- penguins %&gt;%\n  mutate(\n    bill_ratio = bill_length_mm / bill_depth_mm,\n    body_mass_kg = body_mass_g / 1000\n  )\nhead(select(penguins_analyzed, species, bill_ratio, body_mass_kg), 3)\n\n\n# A tibble: 3 × 3\n  species bill_ratio body_mass_kg\n  &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Adelie        2.09         3.75\n2 Adelie        2.27         3.8 \n3 Adelie        2.24         3.25\n\n\nCode\n# Summarize data by species\npenguin_summary &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(\n    count = n(),\n    avg_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    avg_bill_depth = mean(bill_depth_mm, na.rm = TRUE),\n    avg_body_mass = mean(body_mass_g, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(avg_body_mass))\nprint(penguin_summary)\n\n\n# A tibble: 3 × 5\n  species   count avg_bill_length avg_bill_depth avg_body_mass\n  &lt;chr&gt;     &lt;int&gt;           &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1 Gentoo      124            47.5           15.0         5076.\n2 Chinstrap    68            48.8           18.4         3733.\n3 Adelie      152            38.8           18.3         3701.\n\n\nCode\n# Analyze crop yields data\ncrop_summary &lt;- crops %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(\n    years_recorded = n(),\n    avg_wheat_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    max_wheat_yield = max(`Wheat (tonnes per hectare)`, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(avg_wheat_yield)) %&gt;%\n  head(10)  # Top 10 countries by average wheat yield\n\nprint(crop_summary)\n\n\n# A tibble: 10 × 4\n   Entity          years_recorded avg_wheat_yield max_wheat_yield\n   &lt;chr&gt;                    &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 Belgium                     19            8.54           10.0 \n 2 Netherlands                 58            7.03            9.29\n 3 Ireland                     58            6.83           10.7 \n 4 United Kingdom              58            6.37            8.98\n 5 Denmark                     58            6.18            8.24\n 6 Luxembourg                  19            5.98            6.82\n 7 Germany                     58            5.89            8.63\n 8 Europe, Western             58            5.72            7.88\n 9 France                      58            5.65            7.80\n10 Northern Europe             58            5.59            7.21",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#exploratory-data-analysis",
    "href": "chapters/02-data-basics.html#exploratory-data-analysis",
    "title": "2  Data Basics",
    "section": "2.6 Exploratory Data Analysis",
    "text": "2.6 Exploratory Data Analysis\nBefore diving into formal statistical tests, it’s essential to explore your data:\n\n\nCode\n# Basic summary statistics\nsummary(penguins$bill_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCode\nsummary(penguins$flipper_length_mm)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nCode\nsummary(penguins$body_mass_g)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\n\nCode\n# Correlation between variables\ncor_matrix &lt;- cor(\n  penguins %&gt;% \n    select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g),\n  use = \"complete.obs\"\n)\nprint(cor_matrix)\n\n\n                  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\nbill_length_mm         1.0000000    -0.2350529         0.6561813   0.5951098\nbill_depth_mm         -0.2350529     1.0000000        -0.5838512  -0.4719156\nflipper_length_mm      0.6561813    -0.5838512         1.0000000   0.8712018\nbody_mass_g            0.5951098    -0.4719156         0.8712018   1.0000000\n\n\nCode\n# Basic visualization - histogram of bill lengths\nlibrary(ggplot2)\nggplot(penguins, aes(x = bill_length_mm)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Penguin Bill Lengths\",\n       x = \"Bill Length (mm)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Boxplot of body mass by species\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Body Mass by Penguin Species\",\n       x = \"Species\",\n       y = \"Body Mass (g)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# Scatterplot of bill length vs. flipper length\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.7) +\n  labs(title = \"Bill Length vs. Flipper Length\",\n       x = \"Flipper Length (mm)\",\n       y = \"Bill Length (mm)\") +\n  theme_minimal()",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#summary",
    "href": "chapters/02-data-basics.html#summary",
    "title": "2  Data Basics",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nIn this chapter, we’ve covered the basics of working with data in R:\n\nUnderstanding different data types and structures\nImporting data from various file formats\nCleaning and preparing data for analysis\nCreating new variables\nUsing dplyr for powerful data manipulation\nConducting initial exploratory data analysis\n\nThese skills form the foundation for all the analyses we’ll perform in the subsequent chapters. By mastering these basics, you’ll be well-prepared to tackle more complex analytical challenges in various scientific fields.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02-data-basics.html#exercises",
    "href": "chapters/02-data-basics.html#exercises",
    "title": "2  Data Basics",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\n\nLoad the Palmer Penguins dataset (../data/environmental/climate_data.csv) and create a summary of the number of penguins by species and island.\nCalculate the mean and standard deviation of bill length, bill depth, and body mass for each penguin species.\nCreate a new variable that represents the ratio of flipper length to body mass. Interpret what this ratio might represent biologically.\nCreate a visualization that shows the relationship between bill length and bill depth, colored by species.\nLoad the crop yields dataset (../data/agriculture/crop_yields.csv) and analyze trends in wheat yields over time for a country of your choice.\nCompare the distributions of body mass between male and female penguins using appropriate visualizations.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Basics</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html",
    "href": "chapters/03-exploratory-analysis.html",
    "title": "3  Exploratory Data Analysis",
    "section": "",
    "text": "3.1 Introduction\nExploratory Data Analysis (EDA) is a critical first step in any data analysis project. In this chapter, you’ll learn how to systematically explore your data to understand its structure, identify patterns, detect anomalies, and generate hypotheses for further investigation.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#the-purpose-of-exploratory-data-analysis",
    "href": "chapters/03-exploratory-analysis.html#the-purpose-of-exploratory-data-analysis",
    "title": "3  Exploratory Data Analysis",
    "section": "3.2 The Purpose of Exploratory Data Analysis",
    "text": "3.2 The Purpose of Exploratory Data Analysis\nExploratory Data Analysis serves several important purposes in natural sciences research:\n\nUnderstanding Data Structure: Gain insights into the basic properties of your dataset\nChecking Data Quality: Identify missing values, outliers, and potential errors\nDiscovering Patterns: Detect relationships, trends, and distributions\nGenerating Hypotheses: Develop questions and hypotheses for formal testing\nInforming Analysis Choices: Guide decisions about appropriate statistical methods",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#summarizing-data",
    "href": "chapters/03-exploratory-analysis.html#summarizing-data",
    "title": "3  Exploratory Data Analysis",
    "section": "3.3 Summarizing Data",
    "text": "3.3 Summarizing Data\n\n3.3.1 Descriptive Statistics\nDescriptive statistics provide a concise summary of your data’s central tendency, dispersion, and shape:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the crop yield dataset\ncrop_yields &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# View the first few rows\nhead(crop_yields)\n\n\n# A tibble: 6 × 14\n  Entity      Code   Year `Wheat (tonnes per hectare)` Rice (tonnes per hectar…¹\n  &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;                        &lt;dbl&gt;                     &lt;dbl&gt;\n1 Afghanistan AFG    1961                        1.02                       1.52\n2 Afghanistan AFG    1962                        0.974                      1.52\n3 Afghanistan AFG    1963                        0.832                      1.52\n4 Afghanistan AFG    1964                        0.951                      1.73\n5 Afghanistan AFG    1965                        0.972                      1.73\n6 Afghanistan AFG    1966                        0.867                      1.52\n# ℹ abbreviated name: ¹​`Rice (tonnes per hectare)`\n# ℹ 9 more variables: `Maize (tonnes per hectare)` &lt;dbl&gt;,\n#   `Soybeans (tonnes per hectare)` &lt;dbl&gt;,\n#   `Potatoes (tonnes per hectare)` &lt;dbl&gt;, `Beans (tonnes per hectare)` &lt;dbl&gt;,\n#   `Peas (tonnes per hectare)` &lt;dbl&gt;, `Cassava (tonnes per hectare)` &lt;dbl&gt;,\n#   `Barley (tonnes per hectare)` &lt;dbl&gt;,\n#   `Cocoa beans (tonnes per hectare)` &lt;dbl&gt;, …\n\n\nCode\n# Get summary statistics for wheat yields\nwheat_summary &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  summarize(\n    Mean = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Median = median(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    StdDev = sd(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Min = min(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Max = max(`Wheat (tonnes per hectare)`, na.rm = TRUE),\n    Q1 = quantile(`Wheat (tonnes per hectare)`, 0.25, na.rm = TRUE),\n    Q3 = quantile(`Wheat (tonnes per hectare)`, 0.75, na.rm = TRUE)\n  )\n\n# Display the summary statistics\nknitr::kable(wheat_summary, caption = \"Summary Statistics for Global Wheat Yields\")\n\n\n\nSummary Statistics for Global Wheat Yields\n\n\nMean\nMedian\nStdDev\nMin\nMax\nQ1\nQ3\n\n\n\n\n2.434914\n1.99\n1.687949\n0\n10.6677\n1.228\n3.1245\n\n\n\n\n\nCode\n# Visualize the distribution of wheat yields\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(bins = 30, fill = \"forestgreen\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Global Wheat Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Identify top wheat-producing countries (by average yield)\ntop_wheat_countries &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(Avg_Yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE)) %&gt;%\n  arrange(desc(Avg_Yield)) %&gt;%\n  head(10)\n\n# Display the top countries\nknitr::kable(top_wheat_countries, caption = \"Top 10 Countries by Average Wheat Yield\")\n\n\n\nTop 10 Countries by Average Wheat Yield\n\n\nEntity\nAvg_Yield\n\n\n\n\nBelgium\n8.544200\n\n\nNetherlands\n7.030172\n\n\nIreland\n6.829840\n\n\nUnited Kingdom\n6.366400\n\n\nDenmark\n6.175285\n\n\nLuxembourg\n5.977411\n\n\nGermany\n5.893978\n\n\nEurope, Western\n5.723267\n\n\nFrance\n5.645341\n\n\nNorthern Europe\n5.589988\n\n\n\n\n\n\n\n3.3.2 Frequency Tables\nFrequency tables are useful for understanding the distribution of categorical variables:\n\n\nCode\n# Let's create a categorical variable based on wheat yield levels\ncrop_yields_with_categories &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(yield_category = case_when(\n    `Wheat (tonnes per hectare)` &lt; 2 ~ \"Low\",\n    `Wheat (tonnes per hectare)` &gt;= 2 & `Wheat (tonnes per hectare)` &lt; 4 ~ \"Medium\",\n    `Wheat (tonnes per hectare)` &gt;= 4 ~ \"High\"\n  ))\n\n# Frequency table for yield categories\ntable(crop_yields_with_categories$yield_category)\n\n\n\n  High    Low Medium \n  1279   4081   2741 \n\n\nCode\n# Proportions\nprop.table(table(crop_yields_with_categories$yield_category))\n\n\n\n     High       Low    Medium \n0.1578817 0.5037650 0.3383533 \n\n\nCode\n# Create a decade variable for temporal analysis\ncrop_yields_with_categories &lt;- crop_yields_with_categories %&gt;%\n  mutate(decade = floor(Year / 10) * 10)\n\n# Two-way frequency table: yield category by decade\nyield_decade_table &lt;- table(crop_yields_with_categories$yield_category, \n                            crop_yields_with_categories$decade)\nyield_decade_table\n\n\n        \n         1960 1970 1980 1990 2000 2010\n  High     34  102  200  261  326  356\n  Low     838  833  760  681  563  406\n  Medium  239  335  344  550  656  617\n\n\nCode\n# Convert to proportions (by row)\nprop.table(yield_decade_table, margin = 1)\n\n\n        \n               1960       1970       1980       1990       2000       2010\n  High   0.02658327 0.07974980 0.15637217 0.20406568 0.25488663 0.27834246\n  Low    0.20534183 0.20411664 0.18622887 0.16687086 0.13795638 0.09948542\n  Medium 0.08719445 0.12221817 0.12550164 0.20065669 0.23932871 0.22510033",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#visualizing-distributions",
    "href": "chapters/03-exploratory-analysis.html#visualizing-distributions",
    "title": "3  Exploratory Data Analysis",
    "section": "3.4 Visualizing Distributions",
    "text": "3.4 Visualizing Distributions\n\n3.4.1 Histograms and Density Plots\nHistograms and density plots help visualize the distribution of continuous variables:\n\n\nCode\n# Histogram of wheat yields\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", color = \"white\", na.rm = TRUE) +\n  labs(title = \"Histogram of Wheat Yields\", \n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Density plot\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_density(fill = \"darkgreen\", alpha = 0.5, na.rm = TRUE) +\n  labs(title = \"Density Plot of Wheat Yields\", \n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Histogram with density overlay\nggplot(crop_yields, aes(x = `Wheat (tonnes per hectare)`)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"darkgreen\", color = \"white\", na.rm = TRUE) +\n  geom_density(color = \"darkgreen\", linewidth = 1, na.rm = TRUE) +\n  labs(title = \"Distribution of Wheat Yields\", \n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Box Plots\nBox plots are excellent for comparing distributions across groups:\n\n\nCode\n# Select a few major countries for comparison\nmajor_wheat_producers &lt;- c(\"United States\", \"China\", \"India\", \"Russia\", \"France\", \"Australia\")\n\n# Filter data for these countries and recent years\nrecent_wheat_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% major_wheat_producers, \n         Year &gt;= 2000,\n         !is.na(`Wheat (tonnes per hectare)`))\n\n# Box plot of wheat yields by country\nggplot(recent_wheat_data, aes(x = Entity, y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Wheat Yields by Country (2000-present)\", \n       x = \"Country\", \n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Enhanced box plot with jittered points\nggplot(recent_wheat_data, aes(x = Entity, y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.5) +\n  geom_jitter(width = 0.2, alpha = 0.5, color = \"darkgreen\") +\n  labs(title = \"Wheat Yields by Country (2000-present)\", \n       x = \"Country\", \n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n3.4.3 Bar Charts\nBar charts are useful for visualizing categorical data:\n\n\nCode\n# Calculate average wheat yield by country for the last decade\nrecent_avg_yields &lt;- crop_yields %&gt;%\n  filter(Year &gt;= 2010, !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(avg_wheat_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_wheat_yield)) %&gt;%\n  head(10)  # Top 10 countries\n\n# Bar chart of average wheat yields\nggplot(recent_avg_yields, aes(x = reorder(Entity, avg_wheat_yield), y = avg_wheat_yield)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  labs(title = \"Top 10 Countries by Average Wheat Yield (2010-present)\", \n       x = \"Country\", \n       y = \"Average Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#exploring-relationships",
    "href": "chapters/03-exploratory-analysis.html#exploring-relationships",
    "title": "3  Exploratory Data Analysis",
    "section": "3.5 Exploring Relationships",
    "text": "3.5 Exploring Relationships\n\n3.5.1 Scatter Plots\nScatter plots help visualize relationships between two continuous variables:\n\n\nCode\n# Let's compare wheat and rice yields\ncrop_yields_filtered &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`), !is.na(`Rice (tonnes per hectare)`)) %&gt;%\n  filter(Year &gt;= 2000)\n\n# Basic scatter plot\nggplot(crop_yields_filtered, aes(x = `Wheat (tonnes per hectare)`, y = `Rice (tonnes per hectare)`)) +\n  geom_point(alpha = 0.5, color = \"darkgreen\") +\n  labs(title = \"Relationship between Wheat and Rice Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Rice Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Scatter plot with color by continent (we'll need to add continent information)\n# For demonstration, let's create a simple mapping for a few countries\ncontinent_mapping &lt;- tibble(\n  Entity = c(\"United States\", \"Canada\", \"Mexico\", \n             \"China\", \"India\", \"Japan\", \n             \"Germany\", \"France\", \"United Kingdom\", \n             \"Brazil\", \"Argentina\", \"Chile\",\n             \"Egypt\", \"Nigeria\", \"South Africa\",\n             \"Australia\", \"New Zealand\"),\n  Continent = c(rep(\"North America\", 3), \n                rep(\"Asia\", 3), \n                rep(\"Europe\", 3), \n                rep(\"South America\", 3),\n                rep(\"Africa\", 3),\n                rep(\"Oceania\", 2))\n)\n\n# Join with our dataset\ncrop_yields_with_continent &lt;- crop_yields_filtered %&gt;%\n  inner_join(continent_mapping, by = \"Entity\")\n\n# Scatter plot with color by continent\nggplot(crop_yields_with_continent, aes(x = `Wheat (tonnes per hectare)`, y = `Rice (tonnes per hectare)`, color = Continent)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Relationship between Wheat and Rice Yields by Continent\",\n       x = \"Wheat Yield (tonnes per hectare)\", \n       y = \"Rice Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Correlation Analysis\nCorrelation analysis quantifies the strength and direction of relationships between variables:\n\n\nCode\n# Select numeric columns for correlation analysis\ncrop_numeric &lt;- crop_yields %&gt;%\n  select(`Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`, `Maize (tonnes per hectare)`, `Soybeans (tonnes per hectare)`, `Potatoes (tonnes per hectare)`, `Beans (tonnes per hectare)`) %&gt;%\n  na.omit()\n\n# Correlation matrix\ncor_matrix &lt;- cor(crop_numeric)\nround(cor_matrix, 2)\n\n\n                              Wheat (tonnes per hectare)\nWheat (tonnes per hectare)                          1.00\nRice (tonnes per hectare)                           0.43\nMaize (tonnes per hectare)                          0.57\nSoybeans (tonnes per hectare)                       0.47\nPotatoes (tonnes per hectare)                       0.57\nBeans (tonnes per hectare)                          0.44\n                              Rice (tonnes per hectare)\nWheat (tonnes per hectare)                         0.43\nRice (tonnes per hectare)                          1.00\nMaize (tonnes per hectare)                         0.73\nSoybeans (tonnes per hectare)                      0.58\nPotatoes (tonnes per hectare)                      0.67\nBeans (tonnes per hectare)                         0.46\n                              Maize (tonnes per hectare)\nWheat (tonnes per hectare)                          0.57\nRice (tonnes per hectare)                           0.73\nMaize (tonnes per hectare)                          1.00\nSoybeans (tonnes per hectare)                       0.65\nPotatoes (tonnes per hectare)                       0.74\nBeans (tonnes per hectare)                          0.63\n                              Soybeans (tonnes per hectare)\nWheat (tonnes per hectare)                             0.47\nRice (tonnes per hectare)                              0.58\nMaize (tonnes per hectare)                             0.65\nSoybeans (tonnes per hectare)                          1.00\nPotatoes (tonnes per hectare)                          0.59\nBeans (tonnes per hectare)                             0.41\n                              Potatoes (tonnes per hectare)\nWheat (tonnes per hectare)                             0.57\nRice (tonnes per hectare)                              0.67\nMaize (tonnes per hectare)                             0.74\nSoybeans (tonnes per hectare)                          0.59\nPotatoes (tonnes per hectare)                          1.00\nBeans (tonnes per hectare)                             0.46\n                              Beans (tonnes per hectare)\nWheat (tonnes per hectare)                          0.44\nRice (tonnes per hectare)                           0.46\nMaize (tonnes per hectare)                          0.63\nSoybeans (tonnes per hectare)                       0.41\nPotatoes (tonnes per hectare)                       0.46\nBeans (tonnes per hectare)                          1.00\n\n\nCode\n# Visualize correlation matrix\nlibrary(corrplot)\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45,\n         title = \"Correlation Matrix of Crop Yields\")\n\n\n\n\n\n\n\n\n\n\n\n3.5.3 Pair Plots\nPair plots provide a comprehensive view of relationships between multiple variables:\n\n\nCode\n# Basic pair plot\npairs(crop_numeric, pch = 19, col = \"darkgreen\")\n\n\n\n\n\n\n\n\n\nCode\n# Enhanced pair plot with GGally\nlibrary(GGally)\nggpairs(crop_numeric) +\n  theme_minimal() +\n  labs(title = \"Relationships Between Different Crop Yields\")",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#identifying-outliers-and-anomalies",
    "href": "chapters/03-exploratory-analysis.html#identifying-outliers-and-anomalies",
    "title": "3  Exploratory Data Analysis",
    "section": "3.6 Identifying Outliers and Anomalies",
    "text": "3.6 Identifying Outliers and Anomalies\n\n3.6.1 Box Plots for Outlier Detection\nBox plots can help identify potential outliers:\n\n\nCode\n# Box plot to identify outliers in wheat yield\nggplot(crop_yields, aes(y = `Wheat (tonnes per hectare)`)) +\n  geom_boxplot(fill = \"darkgreen\", alpha = 0.7, na.rm = TRUE) +\n  labs(title = \"Box Plot of Wheat Yields with Potential Outliers\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Identify potential outliers\nwheat_outliers &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(\n    q1 = quantile(`Wheat (tonnes per hectare)`, 0.25),\n    q3 = quantile(`Wheat (tonnes per hectare)`, 0.75),\n    iqr = q3 - q1,\n    lower_bound = q1 - 1.5 * iqr,\n    upper_bound = q3 + 1.5 * iqr,\n    is_outlier = `Wheat (tonnes per hectare)` &lt; lower_bound | `Wheat (tonnes per hectare)` &gt; upper_bound\n  ) %&gt;%\n  filter(is_outlier) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`)\n\n# Display the outliers\nhead(wheat_outliers, 10)\n\n\n# A tibble: 10 × 3\n   Entity   Year `Wheat (tonnes per hectare)`\n   &lt;chr&gt;   &lt;dbl&gt;                        &lt;dbl&gt;\n 1 Austria  2016                         6.25\n 2 Belgium  2000                         7.92\n 3 Belgium  2001                         8.05\n 4 Belgium  2002                         8.28\n 5 Belgium  2003                         8.58\n 6 Belgium  2004                         8.98\n 7 Belgium  2005                         8.27\n 8 Belgium  2006                         8.25\n 9 Belgium  2007                         7.89\n10 Belgium  2008                         8.76\n\n\n\n\n3.6.2 Z-Scores for Outlier Detection\nZ-scores can also help identify outliers:\n\n\nCode\n# Calculate z-scores for wheat yields\nwheat_z_scores &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  mutate(\n    wheat_mean = mean(`Wheat (tonnes per hectare)`),\n    wheat_sd = sd(`Wheat (tonnes per hectare)`),\n    z_score = (`Wheat (tonnes per hectare)` - wheat_mean) / wheat_sd,\n    is_extreme = abs(z_score) &gt; 3\n  )\n\n# Display extreme values (z-score &gt; 3 or &lt; -3)\nwheat_extremes &lt;- wheat_z_scores %&gt;%\n  filter(is_extreme) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, z_score) %&gt;%\n  arrange(desc(abs(z_score)))\n\nhead(wheat_extremes, 10)\n\n\n# A tibble: 10 × 4\n   Entity       Year `Wheat (tonnes per hectare)` z_score\n   &lt;chr&gt;       &lt;dbl&gt;                        &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ireland      2015                        10.7     4.88\n 2 Ireland      2017                        10.2     4.58\n 3 Belgium      2015                        10.0     4.49\n 4 Ireland      2014                        10.0     4.49\n 5 Zambia       2008                         9.94    4.45\n 6 Ireland      2004                         9.92    4.44\n 7 New Zealand  2017                         9.86    4.40\n 8 Ireland      2011                         9.86    4.40\n 9 Ireland      2016                         9.54    4.21\n10 Belgium      2009                         9.47    4.16",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#time-series-exploration",
    "href": "chapters/03-exploratory-analysis.html#time-series-exploration",
    "title": "3  Exploratory Data Analysis",
    "section": "3.7 Time Series Exploration",
    "text": "3.7 Time Series Exploration\nAgricultural data often contains important temporal patterns:\n\n\nCode\n# Select a few countries for time series analysis\ncountries_for_ts &lt;- c(\"United States\", \"China\", \"India\", \"France\")\n\n# Filter data for these countries\nwheat_ts_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% countries_for_ts, !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  filter(Year &gt;= 1960)\n\n# Time series plot\nggplot(wheat_ts_data, aes(x = Year, y = `Wheat (tonnes per hectare)`, color = Entity)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  labs(title = \"Wheat Yield Trends Over Time\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\")",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#missing-data-analysis",
    "href": "chapters/03-exploratory-analysis.html#missing-data-analysis",
    "title": "3  Exploratory Data Analysis",
    "section": "3.8 Missing Data Analysis",
    "text": "3.8 Missing Data Analysis\nUnderstanding patterns of missing data is crucial:\n\n\nCode\n# Check for missing values in each column\ncolSums(is.na(crop_yields))\n\n\n                          Entity                             Code \n                               0                             1919 \n                            Year       Wheat (tonnes per hectare) \n                               0                             4974 \n       Rice (tonnes per hectare)       Maize (tonnes per hectare) \n                            4604                             2301 \n   Soybeans (tonnes per hectare)    Potatoes (tonnes per hectare) \n                            7114                             3059 \n      Beans (tonnes per hectare)        Peas (tonnes per hectare) \n                            5066                             6840 \n    Cassava (tonnes per hectare)      Barley (tonnes per hectare) \n                            5887                             6342 \nCocoa beans (tonnes per hectare)     Bananas (tonnes per hectare) \n                            8466                             4166 \n\n\nCode\n# Visualize missing data patterns\nif(requireNamespace(\"naniar\", quietly = TRUE)) {\n  library(naniar)\n  \n  # Create a visualization of missing data\n  gg_miss_var(crop_yields)\n  \n  # Create a matrix showing missing data patterns\n  vis_miss(crop_yields[, c(\"Entity\", \"Year\", \"Wheat (tonnes per hectare)\", \"Rice (tonnes per hectare)\", \"Maize (tonnes per hectare)\")])\n} else {\n  message(\"The 'naniar' package is not installed. Install it with install.packages('naniar') to visualize missing data patterns.\")\n  \n  # Alternative: simple summary of missing data\n  missing_summary &lt;- sapply(crop_yields, function(x) sum(is.na(x)))\n  missing_df &lt;- data.frame(\n    Variable = names(missing_summary),\n    Missing_Count = missing_summary,\n    Missing_Percent = round(missing_summary / nrow(crop_yields) * 100, 2)\n  )\n  \n  # Display the summary\n  missing_df &lt;- missing_df[order(-missing_df$Missing_Count), ]\n  head(missing_df, 10)\n}\n\n\n                                                         Variable Missing_Count\nCocoa beans (tonnes per hectare) Cocoa beans (tonnes per hectare)          8466\nSoybeans (tonnes per hectare)       Soybeans (tonnes per hectare)          7114\nPeas (tonnes per hectare)               Peas (tonnes per hectare)          6840\nBarley (tonnes per hectare)           Barley (tonnes per hectare)          6342\nCassava (tonnes per hectare)         Cassava (tonnes per hectare)          5887\nBeans (tonnes per hectare)             Beans (tonnes per hectare)          5066\nWheat (tonnes per hectare)             Wheat (tonnes per hectare)          4974\nRice (tonnes per hectare)               Rice (tonnes per hectare)          4604\nBananas (tonnes per hectare)         Bananas (tonnes per hectare)          4166\nPotatoes (tonnes per hectare)       Potatoes (tonnes per hectare)          3059\n                                 Missing_Percent\nCocoa beans (tonnes per hectare)           64.75\nSoybeans (tonnes per hectare)              54.41\nPeas (tonnes per hectare)                  52.31\nBarley (tonnes per hectare)                48.50\nCassava (tonnes per hectare)               45.02\nBeans (tonnes per hectare)                 38.75\nWheat (tonnes per hectare)                 38.04\nRice (tonnes per hectare)                  35.21\nBananas (tonnes per hectare)               31.86\nPotatoes (tonnes per hectare)              23.40",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#summary",
    "href": "chapters/03-exploratory-analysis.html#summary",
    "title": "3  Exploratory Data Analysis",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nThis chapter has demonstrated various techniques for exploratory data analysis using a real agricultural dataset. We’ve covered:\n\nComputing and interpreting descriptive statistics\nCreating and analyzing frequency tables\nVisualizing distributions with histograms, density plots, and box plots\nExploring relationships with scatter plots and correlation analysis\nIdentifying outliers and anomalies\nAnalyzing time series patterns\nExamining missing data\n\nThese techniques provide a foundation for understanding your data before proceeding to more advanced analyses. By thoroughly exploring your data, you can make informed decisions about appropriate statistical methods and generate meaningful hypotheses for testing.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/03-exploratory-analysis.html#exercises",
    "href": "chapters/03-exploratory-analysis.html#exercises",
    "title": "3  Exploratory Data Analysis",
    "section": "3.10 Exercises",
    "text": "3.10 Exercises\n\nLoad the plant biodiversity dataset from docs/data/ecology/biodiversity.csv and perform a comprehensive exploratory analysis.\nCreate a histogram and density plot for another crop in the dataset. How does its distribution compare to wheat?\nInvestigate the relationship between potato yields and latitude (you’ll need to find or create a dataset with latitude information).\nIdentify countries with the most significant improvement in crop yields over time.\nCreate a time series plot showing the ratio of wheat to rice yields over time for major producing countries.\nPerform the same exploratory analyses in R for the spatial dataset in docs/data/geography/spatial.csv.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html",
    "href": "chapters/04-hypothesis-testing.html",
    "title": "4  Hypothesis Testing",
    "section": "",
    "text": "4.1 Introduction\nHypothesis testing is a fundamental statistical approach used to make inferences about populations based on sample data. In ecological and forestry research, hypothesis testing helps researchers determine whether observed patterns or differences are statistically significant or merely due to random chance.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "href": "chapters/04-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "title": "4  Hypothesis Testing",
    "section": "4.2 The Logic of Hypothesis Testing",
    "text": "4.2 The Logic of Hypothesis Testing\n\n4.2.1 Null and Alternative Hypotheses\nThe foundation of hypothesis testing involves two competing hypotheses:\n\nNull Hypothesis (H₀): This is the default position that assumes no effect, no difference, or no relationship exists. For example, “There is no difference in tree height between two forest types.”\nAlternative Hypothesis (H₁ or Hₐ): This is the hypothesis that the researcher typically wants to provide evidence for. For example, “There is a significant difference in tree height between two forest types.”\n\n\n\n4.2.2 Example in Ecological Research\nLet’s consider a specific example from forestry research:\n\nResearch Question: Is there a difference in the average height of oak trees between Site A and Site B?\nNull Hypothesis (H₀): There is no difference in the average height of oak trees between Site A and Site B.\nAlternative Hypothesis (H₁): There is a significant difference in the average height of oak trees between Site A and Site B.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#understanding-p-values-and-significance-levels",
    "href": "chapters/04-hypothesis-testing.html#understanding-p-values-and-significance-levels",
    "title": "4  Hypothesis Testing",
    "section": "4.3 Understanding P-values and Significance Levels",
    "text": "4.3 Understanding P-values and Significance Levels\n\n4.3.1 The P-value\nThe p-value is the probability of obtaining results at least as extreme as those observed, assuming the null hypothesis is true. A smaller p-value indicates stronger evidence against the null hypothesis.\n\n\n4.3.2 Significance Level (Alpha)\nThe significance level, often denoted as α (alpha), represents the threshold for statistical significance. In most research, it is set at 0.05 (5%). This value signifies the maximum acceptable probability of making a Type I error — wrongly rejecting the null hypothesis when it is true.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#example-two-sample-t-test",
    "href": "chapters/04-hypothesis-testing.html#example-two-sample-t-test",
    "title": "4  Hypothesis Testing",
    "section": "4.4 Example: Two-Sample t-test",
    "text": "4.4 Example: Two-Sample t-test\n\n\nCode\n# Simulate tree height data for two sites\nset.seed(123)\nsite_A &lt;- rnorm(30, mean = 25, sd = 5)  # 30 trees with mean height 25m\nsite_B &lt;- rnorm(30, mean = 28, sd = 5)  # 30 trees with mean height 28m\n\n# Create a data frame\ntree_data &lt;- data.frame(\n  height = c(site_A, site_B),\n  site = factor(rep(c(\"A\", \"B\"), each = 30))\n)\n\n# Visualize the data\nlibrary(ggplot2)\nggplot(tree_data, aes(x = site, y = height, fill = site)) +\n  geom_boxplot() +\n  labs(title = \"Tree Heights by Site\",\n       x = \"Site\",\n       y = \"Height (m)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform a t-test\nt_test_result &lt;- t.test(height ~ site, data = tree_data)\nprint(t_test_result)\n\n\n\n    Welch Two Sample t-test\n\ndata:  height by site\nt = -3.5092, df = 56.559, p-value = 0.0008892\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -6.482713 -1.771708\nsample estimates:\nmean in group A mean in group B \n       24.76448        28.89169 \n\n\nCode\n# Interpret the result\nalpha &lt;- 0.05\nif (t_test_result$p.value &lt; alpha) {\n  cat(\"With a p-value of\", round(t_test_result$p.value, 4), \n      \"we reject the null hypothesis.\\n\",\n      \"There is a statistically significant difference in tree heights between sites.\")\n} else {\n  cat(\"With a p-value of\", round(t_test_result$p.value, 4), \n      \"we fail to reject the null hypothesis.\\n\",\n      \"There is not enough evidence to conclude a significant difference in tree heights.\")\n}\n\n\nWith a p-value of 9e-04 we reject the null hypothesis.\n There is a statistically significant difference in tree heights between sites.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#example-using-marine-dataset-for-two-sample-t-test",
    "href": "chapters/04-hypothesis-testing.html#example-using-marine-dataset-for-two-sample-t-test",
    "title": "4  Hypothesis Testing",
    "section": "4.5 Example: Using Marine Dataset for Two-Sample t-test",
    "text": "4.5 Example: Using Marine Dataset for Two-Sample t-test\nLet’s apply the t-test to analyze real data. We’ll use our marine dataset to compare fishing yields between different regions:\n\n\nCode\n# Load necessary packages\nlibrary(tidyverse)\n\n# Load the marine dataset\nmarine_data &lt;- read_csv(\"../data/marine/ocean_data.csv\")\n\n# View the structure of the dataset\nstr(marine_data)\n\n\nspc_tbl_ [65,706 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ year       : num [1:65706] 1991 1991 1991 1991 1991 ...\n $ lake       : chr [1:65706] \"Erie\" \"Erie\" \"Erie\" \"Erie\" ...\n $ species    : chr [1:65706] \"American Eel\" \"American Eel\" \"American Eel\" \"American Eel\" ...\n $ grand_total: num [1:65706] 1 1 1 1 1 1 0 0 0 0 ...\n $ comments   : chr [1:65706] NA NA NA NA ...\n $ region     : chr [1:65706] \"Michigan (MI)\" \"New York (NY)\" \"Ohio (OH)\" \"Pennsylvania (PA)\" ...\n $ values     : num [1:65706] 0 0 0 0 0 1 0 0 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   year = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   grand_total = col_double(),\n  ..   comments = col_character(),\n  ..   region = col_character(),\n  ..   values = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\n# Let's compare fishing yields between two lakes\nif(\"lake\" %in% colnames(marine_data) & \"values\" %in% colnames(marine_data)) {\n  # Select two lakes for comparison\n  lake_comparison &lt;- marine_data %&gt;%\n    filter(lake %in% c(\"Michigan\", \"Superior\")) %&gt;%\n    select(lake, values)\n  \n  # Perform t-test\n  t_test_result &lt;- t.test(values ~ lake, data = lake_comparison)\n  \n  # Display the results\n  print(t_test_result)\n  \n  # Visualize the comparison\n  ggplot(lake_comparison, aes(x = lake, y = values)) +\n    geom_boxplot(fill = \"lightblue\") +\n    labs(title = \"Comparison of Fishing Yields Between Lakes\",\n         x = \"Lake\", y = \"Yield Values\") +\n    theme_minimal()\n} else {\n  # If the columns don't match exactly, adapt to the actual structure\n  # This is a fallback to ensure the code runs with the actual data\n  print(\"Column names don't match expected structure. Adapting...\")\n  \n  # Identify numeric columns for analysis\n  numeric_cols &lt;- sapply(marine_data, is.numeric)\n  if(sum(numeric_cols) &gt; 0) {\n    numeric_col &lt;- names(marine_data)[numeric_cols][1]\n    \n    # Identify a categorical column for grouping\n    cat_cols &lt;- sapply(marine_data, function(x) is.character(x) || is.factor(x))\n    if(sum(cat_cols) &gt; 0) {\n      cat_col &lt;- names(marine_data)[cat_cols][1]\n      \n      # Get the two most frequent categories\n      top_categories &lt;- names(sort(table(marine_data[[cat_col]]), decreasing = TRUE)[1:2])\n      \n      # Filter data for these categories\n      comparison_data &lt;- marine_data %&gt;%\n        filter(!!sym(cat_col) %in% top_categories) %&gt;%\n        select(!!sym(cat_col), !!sym(numeric_col))\n      \n      # Rename columns for easier formula creation\n      names(comparison_data) &lt;- c(\"category\", \"value\")\n      \n      # Perform t-test\n      t_test_result &lt;- t.test(value ~ category, data = comparison_data)\n      \n      # Display the results\n      print(t_test_result)\n      \n      # Visualize the comparison\n      ggplot(comparison_data, aes(x = category, y = value)) +\n        geom_boxplot(fill = \"lightblue\") +\n        labs(title = paste(\"Comparison of\", numeric_col, \"Between Groups\"),\n             x = cat_col, y = numeric_col) +\n        theme_minimal()\n    }\n  }\n}\n\n\n\n    Welch Two Sample t-test\n\ndata:  values by lake\nt = 7.0924, df = 16555, p-value = 1.371e-12\nalternative hypothesis: true difference in means between group Michigan and group Superior is not equal to 0\n95 percent confidence interval:\n 164.1330 289.5019\nsample estimates:\nmean in group Michigan mean in group Superior \n              759.5080               532.6905",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#types-of-errors-in-hypothesis-testing",
    "href": "chapters/04-hypothesis-testing.html#types-of-errors-in-hypothesis-testing",
    "title": "4  Hypothesis Testing",
    "section": "4.6 Types of Errors in Hypothesis Testing",
    "text": "4.6 Types of Errors in Hypothesis Testing\n\n4.6.1 Type I and Type II Errors\nIn hypothesis testing, two types of errors can occur:\n\nType I Error: Rejecting a true null hypothesis (false positive).\n\nProbability = α (significance level)\nExample: Concluding there’s a difference in tree heights when there actually isn’t.\n\nType II Error: Failing to reject a false null hypothesis (false negative).\n\nProbability = β\nExample: Failing to detect a real difference in tree heights.\n\n\n\n\n4.6.2 Statistical Power\nStatistical power is the probability of correctly rejecting a false null hypothesis (1 - β). Factors affecting power include:\n\nSample size\nEffect size\nSignificance level (α)\nVariability in the data\n\n\n\nCode\n# Demonstrate power calculation for a t-test\nlibrary(pwr)\n\n# Calculate power for our example\neffect_size &lt;- (28 - 25) / 5  # (mean difference) / standard deviation\npower_result &lt;- pwr.t.test(\n  n = 30,                    # Sample size per group\n  d = effect_size,           # Cohen's d effect size\n  sig.level = 0.05,          # Significance level\n  type = \"two.sample\",       # Two-sample t-test\n  alternative = \"two.sided\"  # Two-sided alternative\n)\n\nprint(power_result)\n\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.6\n      sig.level = 0.05\n          power = 0.6275046\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\n# Calculate required sample size for 80% power\nsample_size_result &lt;- pwr.t.test(\n  d = effect_size,           # Cohen's d effect size\n  sig.level = 0.05,          # Significance level\n  power = 0.8,               # Desired power\n  type = \"two.sample\",       # Two-sample t-test\n  alternative = \"two.sided\"  # Two-sided alternative\n)\n\nprint(sample_size_result)\n\n\n\n     Two-sample t test power calculation \n\n              n = 44.58577\n              d = 0.6\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#one-sample-tests",
    "href": "chapters/04-hypothesis-testing.html#one-sample-tests",
    "title": "4  Hypothesis Testing",
    "section": "4.7 One-Sample Tests",
    "text": "4.7 One-Sample Tests\nOne-sample tests compare a sample mean to a known or hypothesized population value.\n\n4.7.1 One-Sample t-Test\nThe one-sample t-test is used when: - The sample is approximately normally distributed - The population standard deviation is unknown\n\n\nCode\n# Simulate tree diameter data\nset.seed(456)\ntree_diameters &lt;- rnorm(25, mean = 32, sd = 5)  # 25 trees with mean diameter 32cm\n\n# Known reference value (e.g., from previous studies)\nreference_diameter &lt;- 30  # cm\n\n# Visualize the data\nggplot(data.frame(diameter = tree_diameters), aes(x = diameter)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = reference_diameter, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Tree Diameters with Reference Value\",\n       x = \"Diameter (cm)\",\n       y = \"Frequency\") +\n  annotate(\"text\", x = reference_diameter + 2, y = 5, \n           label = \"Reference\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform a one-sample t-test\none_sample_result &lt;- t.test(tree_diameters, mu = reference_diameter)\nprint(one_sample_result)\n\n\n\n    One Sample t-test\n\ndata:  tree_diameters\nt = 2.7309, df = 24, p-value = 0.01165\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 30.79206 35.69358\nsample estimates:\nmean of x \n 33.24282",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#two-sample-tests",
    "href": "chapters/04-hypothesis-testing.html#two-sample-tests",
    "title": "4  Hypothesis Testing",
    "section": "4.8 Two-Sample Tests",
    "text": "4.8 Two-Sample Tests\nTwo-sample tests compare means between two independent groups.\n\n4.8.1 Independent Samples t-Test\nThe independent samples t-test is used when: - Both samples are approximately normally distributed - The two samples are independent\n\n\nCode\n# We already performed this test in our initial example\n# Let's visualize it differently\n\n# Create density plots\nggplot(tree_data, aes(x = height, fill = site)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Distribution of Tree Heights by Site\",\n       x = \"Height (m)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Add mean lines\nggplot(tree_data, aes(x = height, fill = site)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(xintercept = mean(site_A), color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean(site_B), color = \"blue\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Tree Heights by Site\",\n       x = \"Height (m)\",\n       y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n4.8.2 Paired Samples t-Test\nThe paired samples t-test is used when: - Measurements are taken from the same subjects under different conditions - The differences between pairs are approximately normally distributed\n\n\nCode\n# Simulate paired data (e.g., tree growth before and after treatment)\nset.seed(789)\nbefore_treatment &lt;- rnorm(20, mean = 15, sd = 3)\nafter_treatment &lt;- before_treatment + rnorm(20, mean = 2.5, sd = 1)  # Growth effect\n\n# Create a data frame\ngrowth_data &lt;- data.frame(\n  tree_id = 1:20,\n  before = before_treatment,\n  after = after_treatment,\n  difference = after_treatment - before_treatment\n)\n\n# Visualize paired data\ngrowth_long &lt;- reshape2::melt(growth_data[, c(\"tree_id\", \"before\", \"after\")], \n                             id.vars = \"tree_id\", \n                             variable.name = \"time\", \n                             value.name = \"height\")\n\nggplot(growth_long, aes(x = time, y = height, group = tree_id)) +\n  geom_line(alpha = 0.3) +\n  geom_point(aes(color = time), size = 3) +\n  labs(title = \"Tree Heights Before and After Treatment\",\n       x = \"Time\",\n       y = \"Height (m)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform a paired t-test\npaired_result &lt;- t.test(growth_data$after, growth_data$before, paired = TRUE)\nprint(paired_result)\n\n\n\n    Paired t-test\n\ndata:  growth_data$after and growth_data$before\nt = 13.843, df = 19, p-value = 2.239e-11\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.874955 2.542925\nsample estimates:\nmean difference \n        2.20894",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#non-parametric-tests",
    "href": "chapters/04-hypothesis-testing.html#non-parametric-tests",
    "title": "4  Hypothesis Testing",
    "section": "4.9 Non-Parametric Tests",
    "text": "4.9 Non-Parametric Tests\nNon-parametric tests are used when the assumptions of parametric tests (like normality) are violated.\n\n4.9.1 Mann-Whitney U Test (Wilcoxon Rank-Sum Test)\nThis is a non-parametric alternative to the independent samples t-test.\n\n\nCode\n# Simulate non-normal data (e.g., species counts in two habitats)\nset.seed(101)\nhabitat_A &lt;- rpois(25, lambda = 8)  # Poisson distribution for count data\nhabitat_B &lt;- rpois(25, lambda = 12)\n\n# Create a data frame\nspecies_data &lt;- data.frame(\n  count = c(habitat_A, habitat_B),\n  habitat = factor(rep(c(\"A\", \"B\"), each = 25))\n)\n\n# Visualize the data\nggplot(species_data, aes(x = habitat, y = count, fill = habitat)) +\n  geom_boxplot() +\n  labs(title = \"Species Counts by Habitat\",\n       x = \"Habitat\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Check for normality\nshapiro.test(habitat_A)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  habitat_A\nW = 0.97173, p-value = 0.6892\n\n\nCode\nshapiro.test(habitat_B)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  habitat_B\nW = 0.97562, p-value = 0.7869\n\n\nCode\n# Perform Mann-Whitney U test\nwilcox_result &lt;- wilcox.test(count ~ habitat, data = species_data)\nprint(wilcox_result)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  count by habitat\nW = 88.5, p-value = 1.308e-05\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n4.9.2 Wilcoxon Signed-Rank Test\nThis is a non-parametric alternative to the paired samples t-test.\n\n\nCode\n# Simulate non-normal paired data\nset.seed(202)\nbefore_restoration &lt;- rpois(20, lambda = 5)\nafter_restoration &lt;- before_restoration + rpois(20, lambda = 3)\n\n# Create a data frame\nrestoration_data &lt;- data.frame(\n  site_id = 1:20,\n  before = before_restoration,\n  after = after_restoration,\n  difference = after_restoration - before_restoration\n)\n\n# Visualize paired data\nrestoration_long &lt;- reshape2::melt(restoration_data[, c(\"site_id\", \"before\", \"after\")], \n                                  id.vars = \"site_id\", \n                                  variable.name = \"time\", \n                                  value.name = \"species_count\")\n\nggplot(restoration_long, aes(x = time, y = species_count, group = site_id)) +\n  geom_line(alpha = 0.3) +\n  geom_point(aes(color = time), size = 3) +\n  labs(title = \"Species Counts Before and After Restoration\",\n       x = \"Time\",\n       y = \"Species Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform Wilcoxon signed-rank test\nwilcox_paired_result &lt;- wilcox.test(restoration_data$after, restoration_data$before, paired = TRUE)\nprint(wilcox_paired_result)\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  restoration_data$after and restoration_data$before\nV = 210, p-value = 8.527e-05\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#confidence-intervals",
    "href": "chapters/04-hypothesis-testing.html#confidence-intervals",
    "title": "4  Hypothesis Testing",
    "section": "4.10 Confidence Intervals",
    "text": "4.10 Confidence Intervals\nConfidence intervals provide a range of plausible values for a population parameter.\n\n\nCode\n# Calculate 95% confidence interval for mean tree height in Site A\nci_result &lt;- t.test(site_A)\nprint(ci_result$conf.int)\n\n\n[1] 22.93287 26.59610\nattr(,\"conf.level\")\n[1] 0.95\n\n\nCode\n# Visualize confidence interval\nmean_height &lt;- mean(site_A)\nci_lower &lt;- ci_result$conf.int[1]\nci_upper &lt;- ci_result$conf.int[2]\n\nggplot(data.frame(height = site_A), aes(x = height)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = mean_height, color = \"red\", size = 1) +\n  geom_vline(xintercept = ci_lower, color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = ci_upper, color = \"blue\", linetype = \"dashed\") +\n  annotate(\"rect\", xmin = ci_lower, xmax = ci_upper, ymin = 0, ymax = Inf, \n           fill = \"blue\", alpha = 0.1) +\n  labs(title = \"Tree Heights in Site A with 95% Confidence Interval\",\n       x = \"Height (m)\",\n       y = \"Frequency\") +\n  annotate(\"text\", x = mean_height, y = 8, label = \"Mean\", color = \"red\") +\n  annotate(\"text\", x = ci_lower - 1, y = 6, label = \"Lower CI\", color = \"blue\") +\n  annotate(\"text\", x = ci_upper + 1, y = 6, label = \"Upper CI\", color = \"blue\") +\n  theme_minimal()",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#exercises",
    "href": "chapters/04-hypothesis-testing.html#exercises",
    "title": "4  Hypothesis Testing",
    "section": "4.11 Exercises",
    "text": "4.11 Exercises\n\nFormulate a hypothesis about a relationship between two variables in the forest inventory dataset.\nConduct an appropriate statistical test to evaluate your hypothesis.\nCalculate the effect size for your test.\nInterpret the results, including the p-value and effect size.\nCreate a visualization that effectively communicates your findings.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#summary",
    "href": "chapters/04-hypothesis-testing.html#summary",
    "title": "4  Hypothesis Testing",
    "section": "4.12 Summary",
    "text": "4.12 Summary\nIn this chapter, we’ve covered the fundamental concepts and techniques of hypothesis testing in ecological and forestry research:\n\nFormulating null and alternative hypotheses\nUnderstanding p-values and significance levels\nRecognizing Type I and Type II errors\nCalculating and interpreting statistical power\nConducting one-sample, two-sample, and paired tests\nUsing non-parametric alternatives when necessary\nCalculating and interpreting confidence intervals\n\nThese statistical tools allow researchers to make informed inferences about populations based on sample data, helping to advance knowledge in ecology and forestry.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/04-hypothesis-testing.html#statistical-power-1",
    "href": "chapters/04-hypothesis-testing.html#statistical-power-1",
    "title": "4  Hypothesis Testing",
    "section": "4.13 Statistical Power",
    "text": "4.13 Statistical Power\nStatistical power is the probability of correctly rejecting the null hypothesis when it is false. It is influenced by:\n\nSample size\nEffect size\nSignificance level (α)\nVariability in the data",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html",
    "href": "chapters/05-statistical-tests.html",
    "title": "5  Common Statistical Tests",
    "section": "",
    "text": "5.1 Introduction\nThis chapter explores common statistical tests used in natural sciences research. Building on the hypothesis testing framework introduced in the previous chapter, we’ll examine specific tests for different research scenarios and data types.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#choosing-the-right-statistical-test",
    "href": "chapters/05-statistical-tests.html#choosing-the-right-statistical-test",
    "title": "5  Common Statistical Tests",
    "section": "5.2 Choosing the Right Statistical Test",
    "text": "5.2 Choosing the Right Statistical Test\nSelecting the appropriate statistical test depends on several factors:\n\nResearch Question: What you’re trying to determine\nData Type: Categorical, continuous, or ordinal\nNumber of Groups: One, two, or multiple groups\nData Distribution: Normal or non-normal\nIndependence: Whether observations are independent or related\n\n\n5.2.1 Decision Tree for Common Tests\nDecision Tree for Selecting Statistical Tests:\n\nFor One Variable:\n\nOne Sample:\n\nNormal, Continuous → One-Sample t-Test\nNon-normal, Continuous → Wilcoxon Signed-Rank Test\nCategorical → Binomial Test\n\nTwo Samples:\n\nNormal, Continuous, Independent → Independent t-Test\nNormal, Continuous, Related → Paired t-Test\nNon-normal, Continuous, Independent → Mann-Whitney U Test\nNon-normal, Continuous, Related → Wilcoxon Signed-Rank Test\nCategorical, Independent → Chi-Square Test\nCategorical, Related → McNemar Test\n\nMultiple Samples:\n\nNormal, Continuous, Independent → ANOVA\nNormal, Continuous, Related → Repeated Measures ANOVA\nNon-normal, Continuous, Independent → Kruskal-Wallis Test\nNon-normal, Continuous, Related → Friedman Test\nCategorical → Chi-Square Test\n\n\nFor Two Variables:\n\nNormal, Continuous → Pearson Correlation\nNon-normal or Ordinal → Spearman Correlation\nContinuous Predictor & Outcome → Linear Regression\nContinuous Predictor, Binary Outcome → Logistic Regression\n\nFor Multiple Variables:\n\nMultiple Continuous Outcomes → MANOVA\nDimension Reduction → Principal Component Analysis\nGrouping → Cluster Analysis",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#parametric-vs.-non-parametric-tests",
    "href": "chapters/05-statistical-tests.html#parametric-vs.-non-parametric-tests",
    "title": "5  Common Statistical Tests",
    "section": "5.3 Parametric vs. Non-Parametric Tests",
    "text": "5.3 Parametric vs. Non-Parametric Tests\n\n5.3.1 Parametric Tests\nParametric tests make assumptions about the underlying population distribution, typically that the data follows a normal distribution. Common parametric tests include:\n\nt-tests\nANOVA\nPearson correlation\nLinear regression\n\n\n\n5.3.2 Non-Parametric Tests\nNon-parametric tests make fewer assumptions about the population distribution and are useful when data doesn’t meet the assumptions of parametric tests. Common non-parametric tests include:\n\nMann-Whitney U test\nWilcoxon signed-rank test\nKruskal-Wallis test\nSpearman correlation\n\n\n\n5.3.3 Checking Assumptions\nBefore applying a parametric test, it’s essential to check if your data meets the necessary assumptions. Let’s use our crop yield dataset to demonstrate:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the crop yield dataset\ncrop_yields &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# View column names to see how R has formatted them\nnames(crop_yields)\n\n\n [1] \"Entity\"                           \"Code\"                            \n [3] \"Year\"                             \"Wheat (tonnes per hectare)\"      \n [5] \"Rice (tonnes per hectare)\"        \"Maize (tonnes per hectare)\"      \n [7] \"Soybeans (tonnes per hectare)\"    \"Potatoes (tonnes per hectare)\"   \n [9] \"Beans (tonnes per hectare)\"       \"Peas (tonnes per hectare)\"       \n[11] \"Cassava (tonnes per hectare)\"     \"Barley (tonnes per hectare)\"     \n[13] \"Cocoa beans (tonnes per hectare)\" \"Bananas (tonnes per hectare)\"    \n\n\nCode\n# Extract wheat yields for analysis\nwheat_yields &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`)\n\n# View the first few rows\nhead(wheat_yields)\n\n\n# A tibble: 6 × 3\n  Entity       Year `Wheat (tonnes per hectare)`\n  &lt;chr&gt;       &lt;dbl&gt;                        &lt;dbl&gt;\n1 Afghanistan  1961                        1.02 \n2 Afghanistan  1962                        0.974\n3 Afghanistan  1963                        0.832\n4 Afghanistan  1964                        0.951\n5 Afghanistan  1965                        0.972\n6 Afghanistan  1966                        0.867\n\n\nCode\n# Check for normality\n# Visual methods\npar(mfrow = c(1, 2))\nhist(wheat_yields$`Wheat (tonnes per hectare)`, main = \"Histogram of Wheat Yields\", xlab = \"Yield (tonnes/hectare)\")\nqqnorm(wheat_yields$`Wheat (tonnes per hectare)`); qqline(wheat_yields$`Wheat (tonnes per hectare)`, col = \"red\")\n\n\n\n\n\n\n\n\n\nCode\n# Statistical test for normality\nshapiro.test(sample(wheat_yields$`Wheat (tonnes per hectare)`, min(5000, length(wheat_yields$`Wheat (tonnes per hectare)`))))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  sample(wheat_yields$`Wheat (tonnes per hectare)`, min(5000, length(wheat_yields$`Wheat (tonnes per hectare)`)))\nW = 0.86317, p-value &lt; 2.2e-16",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-comparing-groups",
    "href": "chapters/05-statistical-tests.html#tests-for-comparing-groups",
    "title": "5  Common Statistical Tests",
    "section": "5.4 Tests for Comparing Groups",
    "text": "5.4 Tests for Comparing Groups\n\n5.4.1 t-Tests\n\n5.4.1.1 Independent Samples t-Test\nUsed to compare means between two independent groups. Let’s compare wheat yields between two time periods:\n\n\nCode\n# Create two groups: early period (before 2000) and recent period (2000 onwards)\ncrop_yields_grouped &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & Year &gt;= 1960) %&gt;%\n  mutate(period = ifelse(Year &lt; 2000, \"Early Period (pre-2000)\", \"Recent Period (2000+)\"))\n\n# Visualize the data\nggplot(crop_yields_grouped, aes(x = period, y = `Wheat (tonnes per hectare)`, fill = period)) +\n  geom_boxplot() +\n  labs(title = \"Wheat Yields by Time Period\",\n       x = \"Period\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# Perform independent samples t-test using formula interface with backticks\nt_test_result &lt;- t.test(`Wheat (tonnes per hectare)` ~ period, data = crop_yields_grouped)\nt_test_result\n\n\n\n    Welch Two Sample t-test\n\ndata:  Wheat (tonnes per hectare) by period\nt = -22.335, df = 4970.2, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Early Period (pre-2000) and group Recent Period (2000+) is not equal to 0\n95 percent confidence interval:\n -0.9798533 -0.8217198\nsample estimates:\nmean in group Early Period (pre-2000)   mean in group Recent Period (2000+) \n                             2.109781                              3.010567 \n\n\n\n\n5.4.1.2 Paired Samples t-Test\nUsed to compare means between two related groups. Let’s compare wheat and rice yields for the same countries and years:\n\n\nCode\n# Prepare data for paired t-test\npaired_data &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Rice (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`)\n\n# View the first few rows\nhead(paired_data)\n\n\n# A tibble: 6 × 4\n  Entity       Year `Wheat (tonnes per hectare)` `Rice (tonnes per hectare)`\n  &lt;chr&gt;       &lt;dbl&gt;                        &lt;dbl&gt;                       &lt;dbl&gt;\n1 Afghanistan  1961                        1.02                         1.52\n2 Afghanistan  1962                        0.974                        1.52\n3 Afghanistan  1963                        0.832                        1.52\n4 Afghanistan  1964                        0.951                        1.73\n5 Afghanistan  1965                        0.972                        1.73\n6 Afghanistan  1966                        0.867                        1.52\n\n\nCode\n# Visualize the paired data\npaired_data_long &lt;- paired_data %&gt;%\n  pivot_longer(cols = c(`Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`), names_to = \"Crop\", values_to = \"Yield\")\n\nggplot(paired_data_long, aes(x = Crop, y = Yield, fill = Crop)) +\n  geom_boxplot() +\n  labs(title = \"Comparison of Wheat and Rice Yields\",\n       x = \"Crop Type\",\n       y = \"Yield (tonnes/hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform paired t-test using vectors directly\npaired_t_test &lt;- t.test(\n  paired_data$`Wheat (tonnes per hectare)`, \n  paired_data$`Rice (tonnes per hectare)`, \n  paired = TRUE\n)\npaired_t_test\n\n\n\n    Paired t-test\n\ndata:  paired_data$`Wheat (tonnes per hectare)` and paired_data$`Rice (tonnes per hectare)`\nt = -61.854, df = 5725, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.565063 -1.468905\nsample estimates:\nmean difference \n      -1.516984 \n\n\n\n\n\n5.4.2 Analysis of Variance (ANOVA)\nANOVA is used to compare means among three or more independent groups. Let’s compare crop yields across different continents:\n\n\nCode\n# Create a mapping of countries to continents (simplified for demonstration)\ncontinent_mapping &lt;- tibble(\n  Entity = c(\"United States\", \"Canada\", \"Mexico\", \n             \"China\", \"India\", \"Japan\", \n             \"Germany\", \"France\", \"United Kingdom\", \n             \"Brazil\", \"Argentina\", \"Chile\",\n             \"Egypt\", \"Nigeria\", \"South Africa\",\n             \"Australia\", \"New Zealand\"),\n  Continent = c(rep(\"North America\", 3), \n                rep(\"Asia\", 3), \n                rep(\"Europe\", 3), \n                rep(\"South America\", 3),\n                rep(\"Africa\", 3),\n                rep(\"Oceania\", 2))\n)\n\n# Join with crop yields data\ncontinental_yields &lt;- crop_yields %&gt;%\n  inner_join(continent_mapping, by = \"Entity\") %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & Year &gt;= 2000)\n\n# Visualize wheat yields by continent\nggplot(continental_yields, aes(x = Continent, y = `Wheat (tonnes per hectare)`, fill = Continent)) +\n  geom_boxplot() +\n  labs(title = \"Wheat Yields by Continent (2000-present)\",\n       x = \"Continent\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Perform ANOVA\nanova_result &lt;- aov(`Wheat (tonnes per hectare)` ~ Continent, data = continental_yields)\nsummary(anova_result)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nContinent     5  698.6  139.73   48.64 &lt;2e-16 ***\nResiduals   317  910.7    2.87                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Post-hoc test to identify which groups differ\nTukeyHSD(anova_result)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = `Wheat (tonnes per hectare)` ~ Continent, data = continental_yields)\n\n$Continent\n                                   diff         lwr        upr     p adj\nAsia-Africa                  0.27759825 -0.63272412  1.1879206 0.9523916\nEurope-Africa                3.89405789  2.98373553  4.8043803 0.0000000\nNorth America-Africa         0.06069825 -0.84962412  0.9710206 0.9999646\nOceania-Africa               1.36844649  0.35067515  2.3862178 0.0019309\nSouth America-Africa        -0.21777193 -1.12809429  0.6925504 0.9834279\nEurope-Asia                  3.61645965  2.70613729  4.5267820 0.0000000\nNorth America-Asia          -0.21690000 -1.12722236  0.6934224 0.9837237\nOceania-Asia                 1.09084825  0.07307691  2.1086196 0.0276496\nSouth America-Asia          -0.49537018 -1.40569254  0.4149522 0.6253648\nNorth America-Europe        -3.83335965 -4.74368201 -2.9230373 0.0000000\nOceania-Europe              -2.52561140 -3.54338274 -1.5078401 0.0000000\nSouth America-Europe        -4.11182982 -5.02215219 -3.2015075 0.0000000\nOceania-North America        1.30774825  0.28997691  2.3255196 0.0036420\nSouth America-North America -0.27847018 -1.18879254  0.6318522 0.9517629\nSouth America-Oceania       -1.58621842 -2.60398976 -0.5684471 0.0001587\n\n\n\n\n5.4.3 Non-Parametric Alternatives\n\n5.4.3.1 Mann-Whitney U Test\nThe Mann-Whitney U test (also called Wilcoxon rank-sum test) is a non-parametric alternative to the independent samples t-test:\n\n\nCode\n# Using the same time period groups as before\nwilcox_test &lt;- wilcox.test(`Wheat (tonnes per hectare)` ~ period, data = crop_yields_grouped)\nwilcox_test\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Wheat (tonnes per hectare) by period\nW = 5031268, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n5.4.3.2 Kruskal-Wallis Test\nThe Kruskal-Wallis test is a non-parametric alternative to ANOVA:\n\n\nCode\n# Using the same continental data as before\nkruskal_result &lt;- kruskal.test(`Wheat (tonnes per hectare)` ~ Continent, data = continental_yields)\nkruskal_result\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Wheat (tonnes per hectare) by Continent\nKruskal-Wallis chi-squared = 120.17, df = 5, p-value &lt; 2.2e-16\n\n\nCode\n# Post-hoc test for Kruskal-Wallis\nif(requireNamespace(\"dunn.test\", quietly = TRUE)) {\n  library(dunn.test)\n  dunn.test(continental_yields$`Wheat (tonnes per hectare)`, continental_yields$Continent, method = \"bonferroni\")\n} else {\n  # Alternative: pairwise Wilcoxon tests\n  pairwise.wilcox.test(continental_yields$`Wheat (tonnes per hectare)`, continental_yields$Continent, \n                       p.adjust.method = \"bonferroni\")\n}\n\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 120.1715, df = 5, p-value = 0\n\n                           Comparison of x by group                            \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |     Africa       Asia     Europe   North Am    Oceania\n---------+-------------------------------------------------------\n    Asia |  -1.814776\n         |     0.5217\n         |\n  Europe |  -9.079400  -7.264623\n         |    0.0000*    0.0000*\n         |\nNorth Am |  -0.948758   0.866018   8.130641\n         |     1.0000     1.0000    0.0000*\n         |\n Oceania |  -2.181366  -0.558180   5.939496  -1.332770\n         |     0.2187     1.0000    0.0000*     1.0000\n         |\nSouth Am |   0.300874   2.115651   9.380275   1.249633   2.450476\n         |     1.0000     0.2578    0.0000*     1.0000     0.1070\n\nalpha = 0.05\nReject Ho if p &lt;= alpha/2",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-relationships",
    "href": "chapters/05-statistical-tests.html#tests-for-relationships",
    "title": "5  Common Statistical Tests",
    "section": "5.5 Tests for Relationships",
    "text": "5.5 Tests for Relationships\n\n5.5.1 Correlation Analysis\n\n5.5.1.1 Pearson Correlation\nPearson correlation measures the linear relationship between two continuous variables:\n\n\nCode\n# Examine correlation between wheat and maize yields\ncrop_correlation &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Maize (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Maize (tonnes per hectare)`)\n\n# Visualize the relationship\nggplot(crop_correlation, aes(x = `Wheat (tonnes per hectare)`, y = `Maize (tonnes per hectare)`)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Relationship Between Wheat and Maize Yields\",\n       x = \"Wheat Yield (tonnes per hectare)\",\n       y = \"Maize Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Calculate Pearson correlation\ncor_result &lt;- cor.test(crop_correlation$`Wheat (tonnes per hectare)`, crop_correlation$`Maize (tonnes per hectare)`, method = \"pearson\")\ncor_result\n\n\n\n    Pearson's product-moment correlation\n\ndata:  crop_correlation$`Wheat (tonnes per hectare)` and crop_correlation$`Maize (tonnes per hectare)`\nt = 49.748, df = 7378, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4838956 0.5180698\nsample estimates:\n      cor \n0.5011781 \n\n\n\n\n5.5.1.2 Spearman Correlation\nSpearman correlation is a non-parametric measure of rank correlation:\n\n\nCode\n# Calculate Spearman correlation\nspearman_result &lt;- cor.test(crop_correlation$`Wheat (tonnes per hectare)`, crop_correlation$`Maize (tonnes per hectare)`, method = \"spearman\")\nspearman_result\n\n\n\n    Spearman's rank correlation rho\n\ndata:  crop_correlation$`Wheat (tonnes per hectare)` and crop_correlation$`Maize (tonnes per hectare)`\nS = 2.4618e+10, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6325152 \n\n\n\n\n\n5.5.2 Regression Analysis\n\n5.5.2.1 Linear Regression\nLinear regression models the relationship between a dependent variable and one or more independent variables:\n\n\nCode\n# Create a dataset with year as predictor for wheat yields\ntime_series_data &lt;- crop_yields %&gt;%\n  filter(Entity == \"United States\" & !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  arrange(Year)\n\n# Visualize the trend\nggplot(time_series_data, aes(x = Year, y = `Wheat (tonnes per hectare)`)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Wheat Yield Trends in the United States\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes/hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Perform linear regression\nlm_model &lt;- lm(`Wheat (tonnes per hectare)` ~ Year, data = time_series_data)\nsummary(lm_model)\n\n\n\nCall:\nlm(formula = `Wheat (tonnes per hectare)` ~ Year, data = time_series_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43042 -0.09139 -0.00340  0.11184  0.39526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -48.465987   2.571017  -18.85   &lt;2e-16 ***\nYear          0.025601   0.001292   19.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1648 on 56 degrees of freedom\nMultiple R-squared:  0.8751,    Adjusted R-squared:  0.8729 \nF-statistic: 392.5 on 1 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Check assumptions\npar(mfrow = c(2, 2))\nplot(lm_model)\n\n\n\n\n\n\n\n\n\n\n\n5.5.2.2 Multiple Regression\nMultiple regression includes more than one predictor variable:\n\n\nCode\n# Create a dataset with multiple predictors\nmulti_crop_data &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Rice (tonnes per hectare)`) & !is.na(`Maize (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`, `Maize (tonnes per hectare)`)\n\n# Perform multiple regression\nmulti_model &lt;- lm(`Wheat (tonnes per hectare)` ~ `Rice (tonnes per hectare)` + `Maize (tonnes per hectare)` + Year, data = multi_crop_data)\nsummary(multi_model)\n\n\n\nCall:\nlm(formula = `Wheat (tonnes per hectare)` ~ `Rice (tonnes per hectare)` + \n    `Maize (tonnes per hectare)` + Year, data = multi_crop_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7029 -0.6135 -0.2079  0.3877  7.8709 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  -2.161e+01  1.776e+00 -12.171   &lt;2e-16 ***\n`Rice (tonnes per hectare)`  -5.426e-03  9.720e-03  -0.558    0.577    \n`Maize (tonnes per hectare)`  2.790e-01  8.512e-03  32.776   &lt;2e-16 ***\nYear                          1.148e-02  8.965e-04  12.810   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.025 on 5722 degrees of freedom\nMultiple R-squared:  0.3411,    Adjusted R-squared:  0.3407 \nF-statistic: 987.2 on 3 and 5722 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-categorical-data",
    "href": "chapters/05-statistical-tests.html#tests-for-categorical-data",
    "title": "5  Common Statistical Tests",
    "section": "5.6 Tests for Categorical Data",
    "text": "5.6 Tests for Categorical Data\n\n5.6.1 Chi-Square Test\nThe Chi-Square test examines the association between categorical variables. Let’s use our biodiversity dataset:\n\n\nCode\n# Load the biodiversity dataset\nplants &lt;- read_csv(\"../data/ecology/biodiversity.csv\")\n\n# Create a contingency table of red list categories by plant group\nif(\"red_list_category\" %in% colnames(plants) & \"group\" %in% colnames(plants)) {\n  # Create a contingency table\n  contingency_table &lt;- table(plants$red_list_category, plants$group)\n  \n  # View the table\n  contingency_table\n  \n  # Perform Chi-Square test\n  chi_sq_result &lt;- chisq.test(contingency_table)\n  chi_sq_result\n  \n  # Examine residuals to understand the pattern of association\n  chi_sq_result$residuals\n} else {\n  # If the expected columns don't exist, create a demonstration with available data\n  message(\"Required columns not found. Creating a demonstration with available columns.\")\n  \n  # Identify categorical columns\n  categorical_cols &lt;- sapply(plants, function(x) is.character(x) || is.factor(x))\n  cat_col_names &lt;- names(plants)[categorical_cols]\n  \n  if(length(cat_col_names) &gt;= 2) {\n    # Select the first two categorical columns\n    col1 &lt;- cat_col_names[1]\n    col2 &lt;- cat_col_names[2]\n    \n    # Create a contingency table\n    contingency_table &lt;- table(plants[[col1]], plants[[col2]])\n    \n    # View the table\n    print(paste(\"Contingency table of\", col1, \"by\", col2))\n    print(contingency_table)\n    \n    # Perform Chi-Square test if appropriate\n    if(min(dim(contingency_table)) &gt; 1 && sum(contingency_table) &gt; 0) {\n      chi_sq_result &lt;- chisq.test(contingency_table, simulate.p.value = TRUE)\n      print(chi_sq_result)\n    } else {\n      message(\"Contingency table not suitable for Chi-Square test.\")\n    }\n  } else {\n    message(\"Not enough categorical columns found for Chi-Square test demonstration.\")\n  }\n}\n\n\n                     \n                            Algae     Conifer       Cycad Ferns and Allies\n  Extinct              0.24140394  0.13937463 -1.12198510       0.20517186\n  Extinct in the Wild -0.62449980 -0.36055513  2.90251880      -0.53076923\n                     \n                      Flowering Plant      Mosses\n  Extinct                  0.06076242  0.27874926\n  Extinct in the Wild     -0.15718930 -0.72111026",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#tests-for-trends-and-time-series",
    "href": "chapters/05-statistical-tests.html#tests-for-trends-and-time-series",
    "title": "5  Common Statistical Tests",
    "section": "5.7 Tests for Trends and Time Series",
    "text": "5.7 Tests for Trends and Time Series\n\n5.7.1 Time Series Analysis\nTime series analysis examines data collected over time to identify patterns, trends, and seasonal effects:\n\n\nCode\n# Create a time series of wheat yields for a specific country\nus_wheat &lt;- crop_yields %&gt;%\n  filter(Entity == \"United States\" & !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  arrange(Year)\n\n# Convert to time series object\nif(requireNamespace(\"zoo\", quietly = TRUE)) {\n  library(zoo)\n  wheat_ts &lt;- zoo(us_wheat$`Wheat (tonnes per hectare)`, us_wheat$Year)\n  \n  # Plot the time series\n  plot(wheat_ts, main = \"US Wheat Yields Over Time\",\n       xlab = \"Year\", ylab = \"Wheat Yield (tonnes/hectare)\")\n  \n  # Add trend line\n  lines(lowess(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`), col = \"red\", lwd = 2)\n} else {\n  # Basic plot if zoo package is not available\n  plot(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`, type = \"l\",\n       main = \"US Wheat Yields Over Time\",\n       xlab = \"Year\", ylab = \"Wheat Yield (tonnes per hectare)\")\n  \n  # Add trend line\n  lines(lowess(us_wheat$Year, us_wheat$`Wheat (tonnes per hectare)`), col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Mann-Kendall Trend Test\nThe Mann-Kendall test is a non-parametric test for identifying trends in time series data:\n\n\nCode\n# Perform Mann-Kendall trend test\nif(requireNamespace(\"Kendall\", quietly = TRUE)) {\n  library(Kendall)\n  mk_test &lt;- Kendall::MannKendall(us_wheat$`Wheat (tonnes per hectare)`)\n  print(mk_test)\n} else {\n  message(\"The Kendall package is not installed. Install it with install.packages('Kendall') to run the Mann-Kendall trend test.\")\n}\n\n\ntau = 0.798, 2-sided pvalue =&lt; 2.22e-16",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#summary",
    "href": "chapters/05-statistical-tests.html#summary",
    "title": "5  Common Statistical Tests",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nThis chapter has demonstrated a variety of statistical tests using real agricultural and biodiversity datasets. We’ve covered:\n\nTests for comparing groups:\n\nt-tests for comparing two groups\nANOVA for comparing multiple groups\nNon-parametric alternatives when data doesn’t meet parametric assumptions\n\nTests for relationships:\n\nCorrelation analysis to measure the strength of relationships\nRegression analysis to model relationships between variables\n\nTests for categorical data:\n\nChi-Square test for examining associations between categorical variables\n\nTests for time series data:\n\nTime series analysis for identifying patterns over time\nMann-Kendall test for detecting trends\n\n\nWhen conducting statistical tests, remember to: - Clearly define your research question - Check if your data meets the assumptions of the test - Choose the appropriate test based on your data type and research question - Interpret results in the context of your research question - Consider the practical significance, not just statistical significance",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#exercises",
    "href": "chapters/05-statistical-tests.html#exercises",
    "title": "5  Common Statistical Tests",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\n\nUsing the crop yield dataset, compare maize yields between continents using both ANOVA and the Kruskal-Wallis test. Which is more appropriate and why?\nExamine the relationship between potato and rice yields using correlation analysis. Calculate both Pearson and Spearman correlations and explain which is more appropriate.\nUsing the biodiversity dataset, investigate whether there’s an association between conservation status and another categorical variable of your choice.\nPerform a time series analysis of wheat yields for China and compare the trend with that of the United States.\nUsing the animal dataset (../data/entomology/insects.csv), compare two groups using an appropriate statistical test.\nCreate a multiple regression model to predict coffee quality scores using the coffee economics dataset (../data/economics/economic.csv).",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/05-statistical-tests.html#enhanced-statistical-tests-chapter",
    "href": "chapters/05-statistical-tests.html#enhanced-statistical-tests-chapter",
    "title": "5  Common Statistical Tests",
    "section": "5.10 Enhanced Statistical Tests Chapter",
    "text": "5.10 Enhanced Statistical Tests Chapter\nThe enhanced visualizations and tables for this chapter are available in a separate file to ensure compatibility with the book rendering process.",
    "crumbs": [
      "Data Analysis Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Common Statistical Tests</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html",
    "href": "chapters/06-visualization.html",
    "title": "6  Data Visualization",
    "section": "",
    "text": "6.1 Introduction\nData visualization is a crucial skill for communicating scientific findings effectively. In this chapter, you will:",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#introduction",
    "href": "chapters/06-visualization.html#introduction",
    "title": "6  Data Visualization",
    "section": "",
    "text": "Learn various data visualization techniques\nGain expertise in creating informative graphs and plots\nUnderstand the role of visualization in conveying insights clearly in natural sciences",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#the-importance-of-data-visualization",
    "href": "chapters/06-visualization.html#the-importance-of-data-visualization",
    "title": "6  Data Visualization",
    "section": "6.2 The Importance of Data Visualization",
    "text": "6.2 The Importance of Data Visualization\n\n6.2.1 Why Data Visualization Matters\nData visualization plays a pivotal role in natural sciences research for several reasons:\n\nPattern Recognition: Visualizations make it easier to identify patterns, trends, and anomalies in data. This can reveal phenomena like population fluctuations, species distributions, or the impact of environmental factors.\nCommunication: Effective visualizations simplify complex scientific concepts, enabling researchers to convey findings to both expert and non-expert audiences. This is particularly valuable when sharing results with policymakers, stakeholders, or the general public.\nHypothesis Testing: Visualizations assist in formulating and testing scientific hypotheses. Researchers can visually explore data distributions, relationships, and spatial patterns, which informs the design of hypothesis tests.\nDecision-Making: Visualizations aid in making informed decisions about conservation and management strategies. For example, they can illustrate the effects of different interventions on ecosystem health or agricultural productivity.\n\n\n\n6.2.2 Types of Scientific Data\nData in natural sciences come in various forms, including:\n\nCategorical Data: These represent qualitative characteristics, such as species names, habitat types, or land-use categories. Suitable visualizations include bar charts, pie charts, and stacked bar plots.\nNumerical Data: Numerical data involve measurements or counts, such as temperature, population size, or crop yields. Histograms, scatter plots, and box plots are useful for visualizing numerical data.\nSpatial Data: Spatial data describe the geographical distribution of features. Maps, heatmaps, and spatial plots help visualize these data effectively, allowing researchers to observe spatial patterns and trends.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#creating-basic-plots",
    "href": "chapters/06-visualization.html#creating-basic-plots",
    "title": "6  Data Visualization",
    "section": "6.3 Creating Basic Plots",
    "text": "6.3 Creating Basic Plots\n\n6.3.1 Introduction to Basic Plots\nHere’s an overview of common basic plots in natural sciences research and when to use them:\n\nBar Charts:\n\nUse: Bar charts are suitable for visualizing categorical data, such as the frequency of different species in a habitat.\nWhen to Use: Use bar charts when comparing the quantities or proportions of different categories. They’re great for showing discrete data.\n\nHistograms:\n\nUse: Histograms are ideal for visualizing the distribution of numerical data.\nWhen to Use: Use histograms when you want to understand the shape of data distributions, check for skewness, and identify potential outliers.\n\nScatter Plots:\n\nUse: Scatter plots are valuable for examining relationships between two numerical variables.\nWhen to Use: Use scatter plots when you want to see how one variable changes with respect to another. They’re helpful for identifying correlations or trends.\n\n\nThese basic plots serve as building blocks for more advanced visualizations and are foundational tools for exploring and communicating scientific data.\nVisualizations not only enhance the understanding of natural phenomena but also foster data-driven decision-making in research and conservation efforts. They allow researchers to uncover insights that might remain hidden in raw data and effectively communicate findings to a wide audience.\n\n\n6.3.2 Creating Bar Charts\nLet’s create a bar chart using the plant biodiversity dataset:\n\n\nCode\n# Load required packages\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(viridis)  # For colorblind-friendly palettes\n\n# Set a professional theme for all plots\ntheme_set(\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"gray50\"),\n    plot.caption = element_text(size = 10, color = \"gray50\"),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 12),\n    legend.title = element_text(face = \"bold\"),\n    legend.text = element_text(size = 12),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA, size = 0.5)\n  )\n)\n\n# Read the biodiversity dataset\nbiodiversity &lt;- read.csv(\"../data/ecology/biodiversity.csv\")\n\n# Create a summary of conservation status by region\nstatus_summary &lt;- biodiversity %&gt;%\n  group_by(continent, red_list_category) %&gt;%\n  summarize(Count = n(), .groups = \"drop\") %&gt;%\n  filter(!is.na(red_list_category))\n\n# Create a professional bar chart\nggplot(status_summary, aes(x = continent, y = Count, fill = red_list_category)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.7) +\n  scale_fill_viridis_d(option = \"plasma\", begin = 0.2, end = 0.8) +\n  labs(\n    title = \"Plant Conservation Status by Continent\",\n    subtitle = \"Distribution of species across different conservation categories\",\n    x = \"Continent\",\n    y = \"Number of Species\",\n    fill = \"Conservation Status\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  coord_flip() +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.major.y = element_line(color = \"gray90\", size = 0.3)\n  )\n\n\n\n\n\nBar chart showing the conservation status of plant species across different regions. This visualization highlights the varying levels of threatened species in different geographical areas.\n\n\n\n\n\n6.3.2.1 R Code Explanation\nThe provided R code creates a bar chart using the ggplot2 package (part of tidyverse). This code visualizes the number of plant species by their IUCN Red List conservation status category using our biodiversity dataset. Let’s break down the code step by step:\n\nLoad Required Libraries\n\nWe load the tidyverse package, which includes ggplot2 for visualization and dplyr for data manipulation.\n\nLoad the Dataset\n\nWe load the plant biodiversity dataset that we downloaded earlier.\n\nCreate a Summary\n\nWe use count() to count the number of species in each Red List category.\nWe arrange the categories in descending order by count.\nWe remove any NA values to ensure clean visualization.\n\nCreate a Bar Chart\n\nWe use ggplot() to initialize the plot with our summary data.\nWe map the Red List categories to the x-axis, the count to the y-axis, and use the categories for fill colors.\nWe use geom_bar(stat = \"identity\") to create bars with heights representing the counts.\nWe add appropriate labels and use a minimal theme for a clean appearance.\nWe angle the x-axis labels for better readability and remove the redundant legend.\n\n\n\n\n6.3.2.2 Practical Example\nIn biodiversity research, you might use bar charts to visualize:\n\nSpecies Richness: Show the number of species across different taxonomic groups.\nConservation Status: Compare the number of species in different threat categories.\nHabitat Distribution: Visualize the distribution of species across different habitat types.\nGeographic Distribution: Show species counts across different countries or regions.\nTemporal Changes: Track changes in species numbers over different time periods.\n\n\n\n\n6.3.3 Constructing Histograms\nNow, let’s create a histogram to visualize the distribution of a numerical variable in our plant dataset:\n\n\nCode\n# Create a threat score by summing all threat columns\nbiodiversity_with_scores &lt;- biodiversity %&gt;%\n  mutate(\n    threat_score = threat_AA + threat_BRU + threat_RCD + \n                  threat_ISGD + threat_EPM + threat_CC + \n                  threat_HID + threat_P + threat_TS + \n                  threat_NSM + threat_GE\n  )\n\n# Create a histogram of threat scores by continent\nggplot(biodiversity_with_scores, aes(x = threat_score, fill = continent)) +\n  geom_histogram(bins = 15, alpha = 0.8, position = \"identity\", color = \"white\", size = 0.2) +\n  scale_fill_viridis_d(option = \"turbo\", begin = 0.2, end = 0.8) +\n  labs(\n    title = \"Distribution of Threat Scores by Continent\",\n    subtitle = \"Frequency of threat levels across geographical regions\",\n    x = \"Threat Score\",\n    y = \"Frequency\",\n    fill = \"Continent\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  facet_wrap(~continent, scales = \"free_y\") +\n  theme(\n    strip.background = element_rect(fill = \"gray95\"),\n    strip.text = element_text(face = \"bold\"),\n    legend.position = \"none\"\n  )\n\n\n\n\n\nHistogram showing the distribution of threat scores across different continents. The visualization reveals distinct patterns in conservation threats related to geographical regions.\n\n\n\n\n\n6.3.3.1 R Code Explanation\nThe code above attempts to create a histogram of plant heights. Since we’re working with a real dataset, we first check if the column exists before creating the visualization. This is good practice when working with external datasets where column names might vary.\n\nWe use conditional logic to check if “Height_cm” exists in the dataset.\nIf it exists, we create a histogram with appropriate binning and styling.\nIf not, we examine the structure of the dataset to identify other numeric variables that could be visualized.\n\nThis approach demonstrates how to handle real-world data that might not always conform to our expectations.\n\n\n\n6.3.4 Designing Scatter Plots\nLet’s create a scatter plot to examine relationships between variables in our biodiversity dataset:\n\n\nCode\n# Create year numeric variable from year_last_seen\nbiodiversity_for_scatter &lt;- biodiversity_with_scores %&gt;%\n  # Create a numeric year value from the year_last_seen categories\n  mutate(\n    year_numeric = case_when(\n      year_last_seen == \"Before 1900\" ~ 1890,\n      year_last_seen == \"1900-1919\" ~ 1910,\n      year_last_seen == \"1920-1939\" ~ 1930,\n      year_last_seen == \"1940-1959\" ~ 1950,\n      year_last_seen == \"1960-1979\" ~ 1970,\n      year_last_seen == \"1980-1999\" ~ 1990,\n      year_last_seen == \"2000-2020\" ~ 2010,\n      TRUE ~ NA_real_\n    )\n  ) %&gt;%\n  filter(!is.na(year_numeric), !is.na(threat_score))\n\n# Create a publication-quality scatter plot\nggplot(biodiversity_for_scatter, aes(x = year_numeric, y = threat_score, color = continent)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = TRUE, alpha = 0.2) +\n  scale_color_viridis_d(option = \"cividis\") +\n  labs(\n    title = \"Relationship Between Last Sighting Year and Threat Score\",\n    subtitle = \"Analysis of extinction patterns across time and geography\",\n    x = \"Approximate Year Last Seen\",\n    y = \"Threat Score\",\n    color = \"Continent\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  theme(\n    legend.position = \"right\",\n    panel.grid.major = element_line(color = \"gray90\", size = 0.3)\n  )\n\n\n\n\n\nScatter plot showing the relationship between threat scores and year last seen for plant species. Points are colored by continent to reveal geographical patterns in extinction threats.\n\n\n\n\n\n6.3.4.1 R Code Explanation\nThis code creates a scatter plot using our biodiversity dataset:\n\nLoad and Explore Data\n\nWe load the biodiversity dataset and examine its structure.\nWe identify numeric columns that could be used for a scatter plot.\n\nDynamic Column Selection\n\nRather than hardcoding column names, we dynamically select numeric columns.\nThis makes the code more robust when working with unfamiliar datasets.\n\nCreate Scatter Plot\n\nWe use ggplot() with aes_string() to dynamically map variables to axes.\nWe add points with some transparency for better visualization of overlapping data.\nWe include a linear regression line with confidence interval to show the trend.\nWe use appropriate labels and a minimal theme.\n\n\nThis approach demonstrates how to create visualizations when working with new datasets where you might not know the column names in advance.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#advanced-visualization-techniques",
    "href": "chapters/06-visualization.html#advanced-visualization-techniques",
    "title": "6  Data Visualization",
    "section": "6.4 Advanced Visualization Techniques",
    "text": "6.4 Advanced Visualization Techniques\n\n6.4.1 Creating Box Plots\nBox plots are excellent for comparing distributions across groups:\n\n\nCode\n# Create a publication-quality box plot\nggplot(biodiversity_with_scores, aes(x = reorder(continent, threat_score, FUN = median, na.rm = TRUE), \n                          y = threat_score, \n                          fill = continent)) +\n  geom_boxplot(alpha = 0.8, outlier.shape = 21, outlier.size = 2) +\n  scale_fill_viridis_d(option = \"mako\", begin = 0.2, end = 0.9) +\n  labs(\n    title = \"Threat Score Comparison Across Continents\",\n    subtitle = \"Distribution of conservation threats by geographical region\",\n    x = NULL,\n    y = \"Threat Score\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  coord_flip() +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major.y = element_line(color = \"gray90\", size = 0.3)\n  )\n\n\n\n\n\nBox plot comparing threat scores across different continents. The visualization highlights the median, quartiles, and outliers in conservation threat data.\n\n\n\n\n\n6.4.1.1 Box Plot Interpretation\nBox plots provide a comprehensive view of data distributions:\n\nThe box represents the interquartile range (IQR), from the 25th to 75th percentile.\nThe line inside the box shows the median (50th percentile).\nThe whiskers typically extend to the smallest and largest values within 1.5 times the IQR.\nPoints beyond the whiskers represent potential outliers.\n\nIn our threat score example, the box plot allows us to compare: - The typical threat score (median) for different continents - The variability in threat scores (box width and whisker length) - The presence of unusually high or low threat scores (outliers) - Differences between continents in terms of both threat score levels and consistency\n\n\n\n6.4.2 Designing Heatmaps\nHeatmaps are powerful for visualizing complex relationships in multivariate data:\n\n\nCode\n# Create a correlation heatmap of threat types\n# First, prepare the data\nthreat_columns &lt;- biodiversity %&gt;%\n  select(starts_with(\"threat_\"), -threat_NA) %&gt;%\n  names()\n\n# Calculate correlation matrix\nthreat_cor &lt;- biodiversity %&gt;%\n  select(all_of(threat_columns)) %&gt;%\n  cor(use = \"pairwise.complete.obs\")\n\n# Convert to long format for ggplot\nthreat_cor_long &lt;- as.data.frame(as.table(threat_cor))\nnames(threat_cor_long) &lt;- c(\"Threat1\", \"Threat2\", \"Correlation\")\n\n# Create readable threat labels\nthreat_labels &lt;- c(\n  \"threat_AA\" = \"Agriculture\",\n  \"threat_BRU\" = \"Biological Resource Use\",\n  \"threat_RCD\" = \"Residential Development\",\n  \"threat_ISGD\" = \"Invasive Species\",\n  \"threat_EPM\" = \"Energy Production\",\n  \"threat_CC\" = \"Climate Change\",\n  \"threat_HID\" = \"Human Intrusion\",\n  \"threat_P\" = \"Pollution\",\n  \"threat_TS\" = \"Transportation\",\n  \"threat_NSM\" = \"Natural System Modification\",\n  \"threat_GE\" = \"Geological Events\"\n)\n\n# Replace the threat codes with readable labels\nthreat_cor_long$Threat1 &lt;- factor(threat_cor_long$Threat1, \n                                 levels = names(threat_labels),\n                                 labels = threat_labels)\nthreat_cor_long$Threat2 &lt;- factor(threat_cor_long$Threat2, \n                                 levels = names(threat_labels),\n                                 labels = threat_labels)\n\n# Create a publication-quality heatmap\nggplot(threat_cor_long, aes(x = Threat1, y = Threat2, fill = Correlation)) +\n  geom_tile(color = \"white\", size = 0.5) +\n  scale_fill_gradient2(\n    low = \"#4575b4\", \n    mid = \"white\", \n    high = \"#d73027\",\n    midpoint = 0,\n    limits = c(-1, 1)\n  ) +\n  geom_text(aes(label = sprintf(\"%.2f\", Correlation)), \n            color = ifelse(abs(threat_cor_long$Correlation) &gt; 0.7, \"white\", \"black\"),\n            size = 3) +\n  labs(\n    title = \"Correlation Between Different Threat Types\",\n    subtitle = \"Strength of relationship between conservation threats\",\n    x = NULL, y = NULL,\n    fill = \"Correlation\\nCoefficient\",\n    caption = \"Data source: IUCN Red List\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid = element_blank(),\n    panel.background = element_rect(fill = \"white\", color = NA),\n    legend.position = \"right\",\n    legend.key.height = unit(1, \"cm\")\n  ) +\n  coord_fixed()\n\n\n\n\n\nHeatmap visualizing the correlation matrix between different threat types. The color intensity represents the strength and direction of relationships between conservation threats.\n\n\n\n\n\n6.4.2.1 Heatmap Interpretation\nThe heatmap visualizes the correlation between different threat types:\n\nColor intensity represents the strength of correlation (red for positive, blue for negative).\nThe diagonal shows perfect correlation of each variable with itself (always 1).\nClusters of similar colors indicate groups of variables that are highly correlated.\n\nThis visualization helps researchers identify: - Which threats tend to have similar patterns - Potential underlying factors affecting multiple threats simultaneously - Opportunities for threat mitigation based on low correlations - Geographical patterns in threat correlations, which could inform regional conservation strategies\n\n\n\n6.4.3 Creating Time Series Plots\nTime series plots are essential for visualizing trends over time:\n\n\nCode\n# Create a time series plot using the crop yields data\n# First, read the dataset\ncrop_yields &lt;- read.csv(\"../data/agriculture/crop_yields.csv\")\n\n# Check column names to ensure we're using the correct ones\nwheat_col &lt;- names(crop_yields)[grep(\"Wheat\", names(crop_yields))]\n\n# Create a simplified dataset for time series analysis\n# Select top countries based on data availability\ntop_countries &lt;- crop_yields %&gt;%\n  group_by(Entity) %&gt;%\n  summarize(count = n()) %&gt;%\n  filter(count &gt; 30) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(6) %&gt;%\n  pull(Entity)\n\n# Create the time series data\ntime_series_data &lt;- crop_yields %&gt;%\n  filter(Entity %in% top_countries) %&gt;%\n  filter(Year &gt;= 1970)\n\n# Create a publication-quality time series plot\n# Use a column that exists in the dataset\nif(length(wheat_col) &gt; 0) {\n  # If we have a wheat column, use it\n  ggplot(time_series_data, aes(x = Year, y = .data[[wheat_col[1]]], color = Entity)) +\n    geom_line(size = 1, na.rm = TRUE) +\n    geom_point(size = 2, alpha = 0.7, na.rm = TRUE) +\n    scale_color_viridis_d(option = \"turbo\", begin = 0.1, end = 0.9) +\n    scale_x_continuous(breaks = seq(1970, 2020, by = 10)) +\n    labs(\n      title = \"Agricultural Yield Trends Over Time (1970-Present)\",\n      subtitle = \"Productivity changes for major agricultural producers\",\n      x = \"Year\",\n      y = paste(\"Yield\", wheat_col[1]),\n      color = \"Country\",\n      caption = \"Data source: Our World in Data\"\n    ) +\n    theme(\n      legend.position = \"right\",\n      panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n      axis.text.x = element_text(angle = 0)\n    )\n} else {\n  # If no wheat column, use another numeric column\n  numeric_cols &lt;- sapply(time_series_data, is.numeric)\n  numeric_col_names &lt;- names(time_series_data)[numeric_cols]\n  numeric_col_names &lt;- numeric_col_names[numeric_col_names != \"Year\"]\n  \n  if(length(numeric_col_names) &gt; 0) {\n    selected_col &lt;- numeric_col_names[1]\n    \n    ggplot(time_series_data, aes(x = Year, y = .data[[selected_col]], color = Entity)) +\n      geom_line(size = 1, na.rm = TRUE) +\n      geom_point(size = 2, alpha = 0.7, na.rm = TRUE) +\n      scale_color_viridis_d(option = \"turbo\", begin = 0.1, end = 0.9) +\n      scale_x_continuous(breaks = seq(1970, 2020, by = 10)) +\n      labs(\n        title = \"Agricultural Trends Over Time (1970-Present)\",\n        subtitle = \"Changes for major agricultural producers\",\n        x = \"Year\",\n        y = selected_col,\n        color = \"Country\",\n        caption = \"Data source: Our World in Data\"\n      ) +\n      theme(\n        legend.position = \"right\",\n        panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n        axis.text.x = element_text(angle = 0)\n      )\n  } else {\n    # If no suitable numeric columns, create a message plot\n    plot_data &lt;- data.frame(x = 1:10, y = 1:10)\n    ggplot(plot_data, aes(x, y)) +\n      geom_blank() +\n      annotate(\"text\", x = 5, y = 5, label = \"No suitable data available for time series\") +\n      theme_minimal() +\n      labs(title = \"Time Series Plot\", subtitle = \"Data not available\")\n  }\n}\n\n\n\n\n\nTime series plot tracking agricultural trends over time for major producers. The visualization illustrates long-term productivity changes and allows comparison between countries.\n\n\n\n\n\n6.4.3.1 Time Series Interpretation\nTime series plots reveal important temporal patterns:\n\nTrends: Long-term increases or decreases over time\nSeasonality: Regular patterns that repeat at fixed intervals\nCycles: Patterns that occur but not at fixed intervals\nIrregular fluctuations: Random variations that don’t follow a pattern\n\nIn our agricultural yield example, we can observe: - The overall trend in yields for different countries - Relative performance of countries over time - Rate of improvement in agricultural productivity - Stability or volatility in yields year-to-year - Convergence or divergence between countries",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#best-practices-for-data-visualization",
    "href": "chapters/06-visualization.html#best-practices-for-data-visualization",
    "title": "6  Data Visualization",
    "section": "6.5 Best Practices for Data Visualization",
    "text": "6.5 Best Practices for Data Visualization\n\n6.5.1 Choosing the Right Visualization\nSelecting the appropriate visualization depends on your data and the story you want to tell:\n\nFor Comparing Categories:\n\nBar charts for comparing values across categories\nGrouped or stacked bar charts for comparing multiple variables across categories\n\nFor Showing Distributions:\n\nHistograms for showing the distribution of a single variable\nBox plots for comparing distributions across groups\nViolin plots for showing distribution shape along with summary statistics\n\nFor Showing Relationships:\n\nScatter plots for examining relationships between two variables\nBubble charts for examining relationships among three variables\nHeatmaps for visualizing complex relationships in multivariate data\n\nFor Showing Compositions:\n\nPie charts for showing parts of a whole (use sparingly)\nStacked bar charts for showing composition across categories\nArea charts for showing composition over time\n\nFor Showing Trends:\n\nLine charts for showing changes over time\nArea charts for showing cumulative totals over time\n\n\n\n\n6.5.2 Design Principles for Effective Visualization\nFollow these principles to create clear, informative visualizations:\n\nSimplicity: Keep visualizations simple and focused on the main message. Avoid unnecessary elements that can distract from the data.\nClarity: Ensure that your visualization clearly communicates the intended message. Use appropriate labels, titles, and annotations.\nAccuracy: Represent data accurately. Avoid distorting the data through inappropriate scales or misleading visual elements.\nConsistency: Use consistent colors, shapes, and styles throughout your visualizations for better comprehension.\nColor Use: Choose colors thoughtfully. Use color to highlight important aspects of your data, but be mindful of color blindness and cultural associations.\nAnnotation: Add context through appropriate annotations, explaining unusual patterns or important events.\nAudience Consideration: Tailor your visualizations to your audience’s knowledge level and needs.\n\n\n\n6.5.3 Common Pitfalls to Avoid\nBe aware of these common visualization mistakes:\n\nMisleading Scales: Starting y-axes at values other than zero can exaggerate differences.\nOvercomplication: Adding too many variables or visual elements can confuse rather than clarify.\nPoor Color Choices: Using colors that are difficult to distinguish or that carry unintended connotations.\nIgnoring Accessibility: Not considering color blindness or other accessibility issues.\nInappropriate Chart Types: Using chart types that don’t match the data or the story you want to tell.\nMissing Context: Failing to provide necessary context for interpreting the visualization.\nNeglecting Uncertainty: Not showing confidence intervals, error bars, or other indicators of uncertainty.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#summary",
    "href": "chapters/06-visualization.html#summary",
    "title": "6  Data Visualization",
    "section": "6.6 Summary",
    "text": "6.6 Summary\nEffective data visualization is a powerful tool for both exploring data and communicating findings. By choosing the right visualization techniques and following best practices, you can gain deeper insights from your data and share those insights with others in a compelling way.\nIn this chapter, we’ve explored: - The importance of data visualization in natural sciences - Basic visualization techniques including bar charts, histograms, and scatter plots - Advanced visualization methods like box plots, heatmaps, and time series plots - Best practices and principles for creating effective visualizations\nBy applying these techniques to real datasets from agriculture, ecology, and geography, we’ve demonstrated how visualization can reveal patterns and relationships that might otherwise remain hidden in the raw data.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-visualization.html#exercises",
    "href": "chapters/06-visualization.html#exercises",
    "title": "6  Data Visualization",
    "section": "6.7 Exercises",
    "text": "6.7 Exercises\n\nUsing the plant biodiversity dataset (../data/ecology/biodiversity.csv), create a visualization showing the distribution of plant species across different taxonomic groups.\nCreate a time series plot using the crop yield dataset (../data/agriculture/crop_yields.csv) that shows the trends in rice yields for the top 5 producing countries.\nUsing the spatial dataset (../data/geography/spatial.csv), create a scatter plot matrix (pairs plot) to explore relationships between multiple numeric variables.\nDesign a visualization that compares the conservation status of plant species across different habitat types using the biodiversity dataset.\nCreate a heatmap visualization using the coffee economics dataset (../data/economics/economic.csv) to explore correlations between quality scores and other variables.\nDesign an animated visualization (using gganimate package) that shows how crop yields have changed over time for a specific country.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html",
    "href": "chapters/07-advanced-visualization.html",
    "title": "7  Advanced Data Visualization",
    "section": "",
    "text": "7.1 Introduction\nBuilding on the visualization techniques covered in Chapter 6, this chapter explores advanced data visualization methods that can help you communicate complex ecological data more effectively. We’ll focus on creating publication-quality graphics, interactive visualizations, and specialized plots for ecological data.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#creating-publication-quality-graphics",
    "href": "chapters/07-advanced-visualization.html#creating-publication-quality-graphics",
    "title": "7  Advanced Data Visualization",
    "section": "7.2 Creating Publication-Quality Graphics",
    "text": "7.2 Creating Publication-Quality Graphics\n\n7.2.1 Customizing ggplot2 Themes\nThe ggplot2 package allows extensive customization of plot appearance:\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load data\ndata(iris)\n\n# Create a basic scatter plot\nbase_plot &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Relationship between Sepal Length and Petal Length\",\n       subtitle = \"Iris Dataset\",\n       x = \"Sepal Length (cm)\",\n       y = \"Petal Length (cm)\",\n       caption = \"Source: Anderson's Iris dataset\")\n\n# Display the base plot\nbase_plot\n\n\n\n\n\n\n\n\n\nCode\n# Create a customized theme\ncustom_theme &lt;- theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12, color = \"gray50\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\"),\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA)\n  )\n\n# Apply the custom theme\nbase_plot + custom_theme\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Color Palettes for Ecological Data\nChoosing appropriate color palettes is crucial for effective visualization:\n\n\nCode\n# Load packages for color palettes\nlibrary(RColorBrewer)\nlibrary(viridis)\n\n# Display color palettes suitable for ecological data\npar(mfrow = c(4, 1), mar = c(2, 6, 2, 1))\ndisplay.brewer.pal(8, \"YlGn\")\ndisplay.brewer.pal(8, \"BrBG\")\ndisplay.brewer.pal(11, \"RdYlBu\")\nscales::show_col(viridis(8))\n\n\n\n\n\n\n\n\n\nCode\n# Apply different color palettes to our plot\nplot1 &lt;- base_plot + \n  scale_color_brewer(palette = \"Set1\") +\n  custom_theme +\n  ggtitle(\"Color Brewer 'Set1' Palette\")\n\nplot2 &lt;- base_plot + \n  scale_color_viridis_d() +\n  custom_theme +\n  ggtitle(\"Viridis Discrete Palette\")\n\n# Display the plots\nplot1\n\n\n\n\n\n\n\n\n\nCode\nplot2\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Arranging Multiple Plots\nCombining multiple plots can help compare different aspects of your data:\n\n\nCode\nlibrary(patchwork)\n\n# Create individual plots\np1 &lt;- ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_boxplot() +\n  labs(title = \"Sepal Length by Species\",\n       x = NULL,\n       y = \"Sepal Length (cm)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(iris, aes(x = Species, y = Petal.Length, fill = Species)) +\n  geom_boxplot() +\n  labs(title = \"Petal Length by Species\",\n       x = NULL,\n       y = \"Petal Length (cm)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np3 &lt;- ggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  labs(title = \"Sepal Length Distribution\",\n       x = \"Sepal Length (cm)\",\n       y = \"Density\") +\n  theme_minimal()\n\np4 &lt;- ggplot(iris, aes(x = Petal.Length, fill = Species)) +\n  geom_density(alpha = 0.7) +\n  labs(title = \"Petal Length Distribution\",\n       x = \"Petal Length (cm)\",\n       y = \"Density\") +\n  theme_minimal()\n\n# Arrange the plots\n(p1 + p2) / (p3 + p4) + \n  plot_annotation(\n    title = \"Iris Morphology by Species\",\n    caption = \"Source: Anderson's Iris dataset\"\n  )",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#interactive-visualizations",
    "href": "chapters/07-advanced-visualization.html#interactive-visualizations",
    "title": "7  Advanced Data Visualization",
    "section": "7.3 Interactive Visualizations",
    "text": "7.3 Interactive Visualizations\n\n7.3.1 Creating Interactive Plots with plotly\nInteractive plots allow users to explore data more deeply:\n\n\nCode\nlibrary(plotly)\nlibrary(knitr)\n\n# Create a ggplot visualization\np &lt;- ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Relationship between Sepal Length and Petal Length\",\n       x = \"Sepal Length (cm)\",\n       y = \"Petal Length (cm)\") +\n  theme_minimal() +\n  scale_color_viridis_d()\n\n# Check if we're in HTML output mode\nif (knitr::is_html_output()) {\n  # For HTML output, use the interactive plotly version\n  ggplotly(p)\n} else {\n  # For PDF output, use the static ggplot version\n  p + annotate(\"text\", x = 6, y = 6, \n               label = \"Note: Interactive version available in HTML output\", \n               fontface = \"italic\", size = 3)\n}\n\n\n\n\nRelationship between Sepal Length and Petal Length across different Iris species. In the HTML version, this plot is interactive and allows zooming, panning, and hovering for details.\n\n\n\n\n7.3.2 Interactive Maps with leaflet\nFor spatial ecological data, interactive maps can be particularly useful:\n\n\nCode\nlibrary(leaflet)\nlibrary(ggplot2)\nlibrary(knitr)\n\n# Create sample ecological site data\nsites &lt;- data.frame(\n  name = c(\"Forest Reserve\", \"Wetland Study Area\", \"Grassland Transect\", \n           \"Mountain Research Station\", \"Coastal Monitoring Site\"),\n  lat = c(37.7749, 37.8, 37.75, 37.85, 37.7),\n  lng = c(-122.4194, -122.45, -122.5, -122.4, -122.3),\n  habitat = c(\"Forest\", \"Wetland\", \"Grassland\", \"Alpine\", \"Coastal\"),\n  species_count = c(120, 85, 65, 95, 110)\n)\n\n# Create a color palette based on habitat type\nhabitat_colors &lt;- c(\"darkgreen\", \"blue\", \"gold\", \"purple\", \"lightblue\")\nnames(habitat_colors) &lt;- c(\"Forest\", \"Wetland\", \"Grassland\", \"Alpine\", \"Coastal\")\n\nif (knitr::is_html_output()) {\n  # For HTML output, create an interactive leaflet map\n  habitat_pal &lt;- colorFactor(\n    palette = habitat_colors,\n    domain = sites$habitat\n  )\n  \n  # Create an interactive map\n  leaflet(sites) %&gt;%\n    addTiles() %&gt;%  # Add default OpenStreetMap tiles\n    addCircleMarkers(\n      ~lng, ~lat,\n      color = ~habitat_pal(habitat),\n      radius = ~sqrt(species_count) * 1.5,\n      popup = ~paste(\"&lt;b&gt;\", name, \"&lt;/b&gt;&lt;br&gt;\",\n                     \"Habitat: \", habitat, \"&lt;br&gt;\",\n                     \"Species Count: \", species_count),\n      label = ~name,\n      fillOpacity = 0.7\n    ) %&gt;%\n    addLegend(\n      position = \"bottomright\",\n      pal = habitat_pal,\n      values = ~habitat,\n      title = \"Habitat Type\",\n      opacity = 0.7\n    )\n} else {\n  # For PDF output, create a static ggplot map\n  world &lt;- map_data(\"world\")\n  \n  ggplot() +\n    geom_polygon(data = world, aes(x = long, y = lat, group = group), \n                 fill = \"lightgray\", color = \"darkgray\", size = 0.2) +\n    geom_point(data = sites, aes(x = lng, y = lat, color = habitat, size = species_count),\n               alpha = 0.7) +\n    scale_color_manual(values = habitat_colors) +\n    scale_size_continuous(range = c(3, 8), name = \"Species Count\") +\n    coord_fixed(xlim = c(-123, -122), ylim = c(37.6, 37.9)) +\n    labs(title = \"Ecological Study Sites\",\n         subtitle = \"Note: Interactive version available in HTML output\",\n         x = \"Longitude\", y = \"Latitude\", color = \"Habitat Type\") +\n    theme_minimal() +\n    theme(legend.position = \"right\")\n}\n\n\n\n\nEcological study sites across different habitat types. In the HTML version, this map is interactive and allows zooming, panning, and clicking on markers for details.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#specialized-ecological-visualizations",
    "href": "chapters/07-advanced-visualization.html#specialized-ecological-visualizations",
    "title": "7  Advanced Data Visualization",
    "section": "7.4 Specialized Ecological Visualizations",
    "text": "7.4 Specialized Ecological Visualizations\n\n7.4.1 Ordination Plots\nOrdination techniques like PCA and NMDS are common in ecological studies:\n\n\nCode\n# Perform PCA on iris dataset\npca_result &lt;- prcomp(iris[, 1:4], scale. = TRUE)\npca_data &lt;- as.data.frame(pca_result$x)\npca_data$Species &lt;- iris$Species\n\n# Create a PCA biplot\nggplot(pca_data, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  stat_ellipse(level = 0.95) +\n  labs(title = \"PCA of Iris Dataset\",\n       x = paste0(\"PC1 (\", round(summary(pca_result)$importance[2, 1] * 100, 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(summary(pca_result)$importance[2, 2] * 100, 1), \"%)\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Create a loadings plot\nloadings &lt;- as.data.frame(pca_result$rotation)\nloadings$variable &lt;- rownames(loadings)\n\nggplot(loadings, aes(x = PC1, y = PC2)) +\n  geom_segment(aes(x = 0, y = 0, xend = PC1 * 5, yend = PC2 * 5),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"gray50\") +\n  geom_text(aes(label = variable), nudge_x = sign(loadings$PC1) * 0.05,\n            nudge_y = sign(loadings$PC2) * 0.05) +\n  labs(title = \"PCA Loadings\",\n       x = \"PC1\",\n       y = \"PC2\") +\n  theme_minimal() +\n  xlim(-0.7, 0.7) +\n  ylim(-0.7, 0.7)\n\n\n\n\n\n\n\n\n\n\n\n7.4.2 Heatmaps for Community Data\nHeatmaps are useful for visualizing species-by-site matrices:\n\n\nCode\n# Create a simulated species-by-site matrix\nset.seed(123)\nn_sites &lt;- 10\nn_species &lt;- 15\ncommunity_matrix &lt;- matrix(rpois(n_sites * n_species, lambda = 2), \n                          nrow = n_sites, ncol = n_species)\nrownames(community_matrix) &lt;- paste0(\"Site\", 1:n_sites)\ncolnames(community_matrix) &lt;- paste0(\"Sp\", 1:n_species)\n\n# Convert to long format for ggplot\ncommunity_data &lt;- as.data.frame(as.table(community_matrix))\nnames(community_data) &lt;- c(\"Site\", \"Species\", \"Abundance\")\n\n# Create a heatmap\nggplot(community_data, aes(x = Species, y = Site, fill = Abundance)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Species Abundance by Site\",\n       x = \"Species\",\n       y = \"Site\",\n       fill = \"Abundance\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n7.4.3 Network Diagrams for Ecological Interactions\nNetwork diagrams can visualize species interactions:\n\n\nCode\nlibrary(igraph)\nlibrary(ggraph)\n\n# Create a simulated interaction network\nset.seed(456)\nn_species &lt;- 10\ninteraction_matrix &lt;- matrix(rbinom(n_species^2, 1, 0.2), \n                            nrow = n_species, ncol = n_species)\ndiag(interaction_matrix) &lt;- 0  # No self-interactions\nspecies_names &lt;- paste0(\"Species\", 1:n_species)\nrownames(interaction_matrix) &lt;- species_names\ncolnames(interaction_matrix) &lt;- species_names\n\n# Convert to igraph object\ng &lt;- graph_from_adjacency_matrix(interaction_matrix, mode = \"directed\")\nV(g)$type &lt;- sample(c(\"Plant\", \"Pollinator\", \"Herbivore\"), n_species, replace = TRUE)\n\n# Create a network diagram\nggraph(g, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(2, \"mm\")), \n                end_cap = circle(2, \"mm\"),\n                color = \"gray50\") +\n  geom_node_point(aes(color = type), size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Ecological Interaction Network\",\n       color = \"Species Type\") +\n  theme_void()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#visualizing-spatial-data",
    "href": "chapters/07-advanced-visualization.html#visualizing-spatial-data",
    "title": "7  Advanced Data Visualization",
    "section": "7.5 Visualizing Spatial Data",
    "text": "7.5 Visualizing Spatial Data\n\n7.5.1 Creating Maps with ggplot2\nSpatial visualization is crucial for ecological data:\n\n\nCode\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(knitr)\n\n# Get world map data\nworld &lt;- map_data(\"world\")\n\n# Create sample species occurrence data\nset.seed(789)\nn_points &lt;- 100\noccurrences &lt;- data.frame(\n  species = sample(c(\"Species A\", \"Species B\", \"Species C\"), n_points, replace = TRUE),\n  longitude = runif(n_points, -10, 40),\n  latitude = runif(n_points, 35, 60)\n)\n\n# Create a map\nggplot() +\n  geom_polygon(data = world, aes(x = long, y = lat, group = group), \n               fill = \"white\", color = \"gray70\", size = 0.2) +\n  geom_point(data = occurrences, \n             aes(x = longitude, y = latitude, color = species),\n             alpha = 0.7, size = 2) +\n  coord_fixed(xlim = c(-10, 40), ylim = c(35, 60)) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.8) +\n  labs(title = \"Species Distribution Map\",\n       subtitle = \"Sample occurrences across Europe\",\n       x = \"Longitude\", y = \"Latitude\", color = \"Species\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_line(color = \"gray90\", size = 0.2))\n\n\n\n\n\nDistribution of sample species occurrences across Europe. The map shows the spatial patterns of three different species.\n\n\n\n\n\n\n7.5.2 Visualizing Raster Data\nEnvironmental raster data is common in ecological studies:\n\n\nCode\nlibrary(raster)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(maps)\n\n# Create a sample raster\nr &lt;- raster(ncol = 100, nrow = 100)\nextent(r) &lt;- c(-10, 40, 35, 60)  # Same extent as our map\nvalues(r) &lt;- runif(ncell(r)) * 10  # Random values\n\n# Convert to data frame for ggplot\nr_df &lt;- as.data.frame(r, xy = TRUE)\ncolnames(r_df) &lt;- c(\"longitude\", \"latitude\", \"value\")\n\n# Get world map data\nworld &lt;- map_data(\"world\")\n\n# Create a raster map\nggplot() +\n  geom_raster(data = r_df, aes(x = longitude, y = latitude, fill = value)) +\n  geom_polygon(data = world, aes(x = long, y = lat, group = group), \n               fill = NA, color = \"gray30\", size = 0.2) +\n  scale_fill_viridis_c(option = \"plasma\", name = \"Value\") +\n  coord_fixed(xlim = c(-10, 40), ylim = c(35, 60)) +\n  labs(title = \"Environmental Variable Distribution\",\n       subtitle = \"Simulated environmental gradient across Europe\",\n       x = \"Longitude\", y = \"Latitude\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nEnvironmental variable visualization across Europe. The raster data shows a simulated environmental gradient overlaid with country boundaries.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#summary",
    "href": "chapters/07-advanced-visualization.html#summary",
    "title": "7  Advanced Data Visualization",
    "section": "7.6 Summary",
    "text": "7.6 Summary\nIn this chapter, we’ve explored advanced visualization techniques in R that go beyond basic plots. These techniques allow researchers to create more informative, interactive, and publication-quality visualizations for ecological and forestry data.\nKey points covered include:\n\nCreating complex multi-panel visualizations\nDeveloping interactive plots for exploration\nDesigning effective spatial visualizations\nImplementing animation for temporal data\nCustomizing visualizations for publication\n\nAs you continue to develop your data visualization skills, remember that effective visualization is both an art and a science. The goal is not just to make visually appealing graphics, but to create visualizations that accurately and clearly communicate your findings to your audience.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-advanced-visualization.html#exercises",
    "href": "chapters/07-advanced-visualization.html#exercises",
    "title": "7  Advanced Data Visualization",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\n\nCreate a faceted plot showing the relationship between two variables across different categories in one of the datasets.\nDevelop an interactive plot that allows users to explore relationships in ecological data.\nCreate a custom theme for ggplot2 that matches the style guidelines of a scientific journal in your field.\nDesign a spatial visualization showing the distribution of a species or environmental variable.\nCreate an animated plot showing changes in an ecological variable over time.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Advanced Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html",
    "href": "chapters/08-regression.html",
    "title": "8  Regression Analysis",
    "section": "",
    "text": "8.1 Introduction\nRegression analysis is a powerful statistical technique used to model the relationship between a dependent variable and one or more independent variables. In natural sciences research, regression models help us understand how environmental factors influence biological processes, predict future conditions, and test hypotheses about causal relationships.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#simple-linear-regression",
    "href": "chapters/08-regression.html#simple-linear-regression",
    "title": "8  Regression Analysis",
    "section": "8.2 Simple Linear Regression",
    "text": "8.2 Simple Linear Regression\nSimple linear regression models the relationship between a dependent variable and a single independent variable.\n\n8.2.1 The Linear Model\nThe simple linear regression model is represented by the equation:\n\\[Y = \\beta_0 + \\beta_1X + \\varepsilon\\]\nWhere: - \\(Y\\) is the dependent variable - \\(X\\) is the independent variable - \\(\\beta_0\\) is the intercept - \\(\\beta_1\\) is the slope - \\(\\varepsilon\\) is the error term\n\n\n8.2.2 Example: Crop Yield Trends Over Time\nLet’s explore the relationship between time (years) and wheat yields using our agricultural dataset:\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the crop yield dataset\ncrop_yields &lt;- read_csv(\"../data/agriculture/crop_yields.csv\")\n\n# Filter data for a specific country (United States) and select relevant columns\nus_wheat &lt;- crop_yields %&gt;%\n  filter(Entity == \"United States\", !is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  select(Year, `Wheat (tonnes per hectare)`)\n\n# Visualize the relationship\nggplot(us_wheat, aes(x = Year, y = `Wheat (tonnes per hectare)`)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Wheat Yields in the United States (1961-present)\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a simple linear regression model\nmodel &lt;- lm(`Wheat (tonnes per hectare)` ~ Year, data = us_wheat)\n\n# Display model summary\nsummary(model)\n\n\n\nCall:\nlm(formula = `Wheat (tonnes per hectare)` ~ Year, data = us_wheat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43042 -0.09139 -0.00340  0.11184  0.39526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -48.465987   2.571017  -18.85   &lt;2e-16 ***\nYear          0.025601   0.001292   19.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1648 on 56 degrees of freedom\nMultiple R-squared:  0.8751,    Adjusted R-squared:  0.8729 \nF-statistic: 392.5 on 1 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Add the regression line to the plot\nggplot(us_wheat, aes(x = Year, y = `Wheat (tonnes per hectare)`)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", color = \"blue\") +\n  labs(title = \"Simple Linear Regression: Wheat Yield vs. Year\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes per hectare)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n8.2.3 Interpreting the Model\nThe key components to interpret in a simple linear regression model are:\n\nIntercept (\\(\\beta_0\\)): The expected value of Y when X = 0\nSlope (\\(\\beta_1\\)): The expected change in Y for a one-unit increase in X\nR-squared: The proportion of variance in Y explained by X\np-value: The statistical significance of the relationship\n\n\n\nCode\n# Extract key model parameters\nintercept &lt;- coef(model)[1]\nslope &lt;- coef(model)[2]\nr_squared &lt;- summary(model)$r.squared\np_value &lt;- summary(model)$coefficients[2, 4]\n\n# Create a table of results\nresults &lt;- data.frame(\n  Parameter = c(\"Intercept\", \"Slope\", \"R-squared\", \"p-value\"),\n  Value = c(intercept, slope, r_squared, p_value)\n)\n\n# Display the results\nknitr::kable(results, digits = 4)\n\n\n\n\n\nParameter\nValue\n\n\n\n\nIntercept\n-48.4660\n\n\nSlope\n0.0256\n\n\nR-squared\n0.8751\n\n\np-value\n0.0000\n\n\n\n\n\nIn this example, the slope represents the average annual increase in wheat yield (tonnes/hectare) in the United States. The R-squared value indicates what percentage of the variation in wheat yields can be explained by the year. The p-value tells us whether this relationship is statistically significant.\n\n\n8.2.4 Checking Model Assumptions\nLinear regression relies on several key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent of each other\nHomoscedasticity: Constant variance of residuals\nNormality: Residuals are normally distributed\n\n\n\nCode\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\nCode\n# Check normality of residuals with a formal test\nshapiro.test(residuals(model))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model)\nW = 0.99033, p-value = 0.9252\n\n\nCode\n# Check homoscedasticity with a formal test\nif(requireNamespace(\"car\", quietly = TRUE)) {\n  library(car)\n  ncvTest(model)\n} else {\n  message(\"The 'car' package is not installed. Install it with install.packages('car') to run the non-constant variance test.\")\n}\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 1.323418, Df = 1, p = 0.24998",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#multiple-linear-regression",
    "href": "chapters/08-regression.html#multiple-linear-regression",
    "title": "8  Regression Analysis",
    "section": "8.3 Multiple Linear Regression",
    "text": "8.3 Multiple Linear Regression\nMultiple linear regression extends the simple linear model to include multiple independent variables.\n\n8.3.1 The Multiple Regression Model\nThe multiple regression model is represented by the equation:\n\\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\varepsilon\\]\nWhere: - \\(Y\\) is the dependent variable - \\(X_1, X_2, ..., X_p\\) are the independent variables - \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) are the coefficients - \\(\\varepsilon\\) is the error term\n\n\n8.3.2 Example: Factors Affecting Crop Yields\nLet’s model wheat yield as a function of multiple crop yields, which might indicate similar agricultural conditions:\n\n\nCode\n# Prepare data for multiple regression\nmulti_crop_data &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`) & !is.na(`Rice (tonnes per hectare)`) & !is.na(`Maize (tonnes per hectare)`)) %&gt;%\n  select(Entity, Year, `Wheat (tonnes per hectare)`, `Rice (tonnes per hectare)`, `Maize (tonnes per hectare)`)\n\n# View the first few rows\nhead(multi_crop_data)\n\n\n# A tibble: 6 × 5\n  Entity       Year `Wheat (tonnes per hectare)` `Rice (tonnes per hectare)`\n  &lt;chr&gt;       &lt;dbl&gt;                        &lt;dbl&gt;                       &lt;dbl&gt;\n1 Afghanistan  1961                        1.02                         1.52\n2 Afghanistan  1962                        0.974                        1.52\n3 Afghanistan  1963                        0.832                        1.52\n4 Afghanistan  1964                        0.951                        1.73\n5 Afghanistan  1965                        0.972                        1.73\n6 Afghanistan  1966                        0.867                        1.52\n# ℹ 1 more variable: `Maize (tonnes per hectare)` &lt;dbl&gt;\n\n\nCode\n# Fit a multiple regression model\nmulti_model &lt;- lm(`Wheat (tonnes per hectare)` ~ `Rice (tonnes per hectare)` + `Maize (tonnes per hectare)` + Year, data = multi_crop_data)\n\n# Display model summary\nsummary(multi_model)\n\n\n\nCall:\nlm(formula = `Wheat (tonnes per hectare)` ~ `Rice (tonnes per hectare)` + \n    `Maize (tonnes per hectare)` + Year, data = multi_crop_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7029 -0.6135 -0.2079  0.3877  7.8709 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  -2.161e+01  1.776e+00 -12.171   &lt;2e-16 ***\n`Rice (tonnes per hectare)`  -5.426e-03  9.720e-03  -0.558    0.577    \n`Maize (tonnes per hectare)`  2.790e-01  8.512e-03  32.776   &lt;2e-16 ***\nYear                          1.148e-02  8.965e-04  12.810   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.025 on 5722 degrees of freedom\nMultiple R-squared:  0.3411,    Adjusted R-squared:  0.3407 \nF-statistic: 987.2 on 3 and 5722 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n8.3.3 Visualizing Multiple Regression\nVisualizing multiple regression models is challenging due to the multidimensional nature of the data. Here are some approaches:\n\n\nCode\n# Partial residual plots\nif(requireNamespace(\"car\", quietly = TRUE)) {\n  library(car)\n  # Create a model with simpler variable names to avoid issues with crPlots\n  renamed_data &lt;- multi_crop_data %&gt;%\n    rename(Wheat = `Wheat (tonnes per hectare)`,\n           Rice = `Rice (tonnes per hectare)`,\n           Maize = `Maize (tonnes per hectare)`)\n  \n  simple_model &lt;- lm(Wheat ~ Rice + Maize + Year, data = renamed_data)\n  crPlots(simple_model)\n} else {\n  # Alternative: create individual scatter plots\n  par(mfrow = c(1, 3))\n  plot(multi_crop_data$`Rice (tonnes per hectare)`, multi_crop_data$`Wheat (tonnes per hectare)`, \n       xlab = \"Rice Yield\", ylab = \"Wheat Yield\",\n       main = \"Wheat vs. Rice\")\n  plot(multi_crop_data$`Maize (tonnes per hectare)`, multi_crop_data$`Wheat (tonnes per hectare)`, \n       xlab = \"Maize Yield\", ylab = \"Wheat Yield\",\n       main = \"Wheat vs. Maize\")\n  plot(multi_crop_data$Year, multi_crop_data$`Wheat (tonnes per hectare)`, \n       xlab = \"Year\", ylab = \"Wheat Yield\",\n       main = \"Wheat vs. Year\")\n}\n\n\n\n\n\n\n\n\n\nCode\n# 3D visualization (for a subset of variables)\nlibrary(knitr)\nif(knitr::is_html_output() && requireNamespace(\"plotly\", quietly = TRUE)) {\n  # For HTML output, use the interactive plotly version\n  library(plotly)\n  plot_ly(multi_crop_data, \n          x = ~`Rice (tonnes per hectare)`, \n          y = ~`Maize (tonnes per hectare)`, \n          z = ~`Wheat (tonnes per hectare)`,\n          type = \"scatter3d\", mode = \"markers\",\n          marker = list(size = 5, color = ~`Wheat (tonnes per hectare)`, \n                        colorscale = \"Viridis\")) %&gt;%\n    layout(title = \"3D Relationship Between Crop Yields\",\n           scene = list(\n             xaxis = list(title = \"Rice Yield (tonnes/ha)\"),\n             yaxis = list(title = \"Maize Yield (tonnes/ha)\"),\n             zaxis = list(title = \"Wheat Yield (tonnes/ha)\")))\n} else {\n  # For PDF output, use a static 3D scatter plot with ggplot2\n  library(ggplot2)\n  \n  # Create a 2D plot with color as the third dimension\n  ggplot(multi_crop_data, \n         aes(x = `Rice (tonnes per hectare)`, \n             y = `Maize (tonnes per hectare)`, \n             color = `Wheat (tonnes per hectare)`)) +\n    geom_point(size = 3, alpha = 0.7) +\n    scale_color_viridis_c(option = \"viridis\", name = \"Wheat Yield\\n(tonnes/ha)\") +\n    labs(title = \"3D Relationship Between Crop Yields\",\n         subtitle = \"Wheat yield shown as color (interactive 3D version in HTML)\",\n         x = \"Rice Yield (tonnes/ha)\",\n         y = \"Maize Yield (tonnes/ha)\") +\n    theme_minimal() +\n    theme(legend.position = \"right\")\n}\n\n\n\n\n\n\n\n\n8.3.4 Variable Selection\nWhen working with multiple predictors, it’s essential to select the most relevant variables:\n\n\nCode\n# Stepwise variable selection\nif(requireNamespace(\"MASS\", quietly = TRUE)) {\n  library(MASS)\n  step_model &lt;- stepAIC(multi_model, direction = \"both\")\n  summary(step_model)\n} else {\n  message(\"The 'MASS' package is not installed. Install it with install.packages('MASS') to perform stepwise variable selection.\")\n}\n\n\nStart:  AIC=292.2\n`Wheat (tonnes per hectare)` ~ `Rice (tonnes per hectare)` + \n    `Maize (tonnes per hectare)` + Year\n\n                               Df Sum of Sq    RSS     AIC\n- `Rice (tonnes per hectare)`   1      0.33 6017.7  290.51\n&lt;none&gt;                                      6017.4  292.20\n- Year                          1    172.58 6189.9  452.11\n- `Maize (tonnes per hectare)`  1   1129.71 7147.1 1275.38\n\nStep:  AIC=290.51\n`Wheat (tonnes per hectare)` ~ `Maize (tonnes per hectare)` + \n    Year\n\n                               Df Sum of Sq    RSS     AIC\n&lt;none&gt;                                      6017.7  290.51\n+ `Rice (tonnes per hectare)`   1      0.33 6017.4  292.20\n- Year                          1    172.33 6190.0  450.18\n- `Maize (tonnes per hectare)`  1   1930.18 7947.9 1881.48\n\n\n\nCall:\nlm(formula = `Wheat (tonnes per hectare)` ~ `Maize (tonnes per hectare)` + \n    Year, data = multi_crop_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6986 -0.6123 -0.2065  0.3857  7.8812 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  -2.156e+01  1.773e+00  -12.16   &lt;2e-16 ***\n`Maize (tonnes per hectare)`  2.759e-01  6.439e-03   42.84   &lt;2e-16 ***\nYear                          1.145e-02  8.946e-04   12.80   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.025 on 5723 degrees of freedom\nMultiple R-squared:  0.341, Adjusted R-squared:  0.3408 \nF-statistic:  1481 on 2 and 5723 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Variance Inflation Factor (VIF) to check for multicollinearity\nif(requireNamespace(\"car\", quietly = TRUE)) {\n  # Use the renamed data and simple model from earlier\n  vif(simple_model)\n} else {\n  message(\"The 'car' package is not installed. Install it with install.packages('car') to calculate VIF values.\")\n  \n  # Alternative: correlation matrix\n  cor_matrix &lt;- cor(multi_crop_data[, c(\"Rice (tonnes per hectare)\", \"Maize (tonnes per hectare)\", \"Year\")], use = \"complete.obs\")\n  print(\"Correlation matrix of predictors:\")\n  print(cor_matrix)\n}\n\n\n    Rice    Maize     Year \n1.963261 2.108941 1.212106",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#polynomial-regression",
    "href": "chapters/08-regression.html#polynomial-regression",
    "title": "8  Regression Analysis",
    "section": "8.4 Polynomial Regression",
    "text": "8.4 Polynomial Regression\nPolynomial regression allows modeling of nonlinear relationships by including polynomial terms.\n\n8.4.1 Example: Nonlinear Crop Yield Trends\nLet’s explore whether the relationship between time and wheat yields might be nonlinear:\n\n\nCode\n# Create a dataset for polynomial regression\nus_wheat$Year_centered &lt;- us_wheat$Year - mean(us_wheat$Year)  # Center year to reduce multicollinearity\n\n# Fit polynomial models of different degrees\npoly1 &lt;- lm(`Wheat (tonnes per hectare)` ~ Year_centered, data = us_wheat)\npoly2 &lt;- lm(`Wheat (tonnes per hectare)` ~ Year_centered + I(Year_centered^2), data = us_wheat)\npoly3 &lt;- lm(`Wheat (tonnes per hectare)` ~ Year_centered + I(Year_centered^2) + I(Year_centered^3), data = us_wheat)\n\n# Compare models\nanova(poly1, poly2, poly3)\n\n\nAnalysis of Variance Table\n\nModel 1: `Wheat (tonnes per hectare)` ~ Year_centered\nModel 2: `Wheat (tonnes per hectare)` ~ Year_centered + I(Year_centered^2)\nModel 3: `Wheat (tonnes per hectare)` ~ Year_centered + I(Year_centered^2) + \n    I(Year_centered^3)\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     56 1.5200                              \n2     55 1.5058  1  0.014257 0.5489 0.46198  \n3     54 1.4027  1  0.103114 3.9697 0.05139 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Summary of the best model (based on the ANOVA result)\nbest_poly_model &lt;- poly2  # Change this based on the ANOVA results\nsummary(best_poly_model)\n\n\n\nCall:\nlm(formula = `Wheat (tonnes per hectare)` ~ Year_centered + I(Year_centered^2), \n    data = us_wheat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43818 -0.08751 -0.00966  0.10875  0.42167 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         2.485e+00  3.260e-02  76.222   &lt;2e-16 ***\nYear_centered       2.560e-02  1.298e-03  19.726   &lt;2e-16 ***\nI(Year_centered^2) -6.258e-05  8.671e-05  -0.722    0.474    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1655 on 55 degrees of freedom\nMultiple R-squared:  0.8763,    Adjusted R-squared:  0.8718 \nF-statistic: 194.8 on 2 and 55 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Visualize the polynomial fit\nus_wheat$Year_orig &lt;- us_wheat$Year  # Keep original year for plotting\nus_wheat &lt;- us_wheat %&gt;%\n  arrange(Year_orig)\n\n# Generate predictions\nus_wheat$pred_linear &lt;- predict(poly1)\nus_wheat$pred_quadratic &lt;- predict(poly2)\nus_wheat$pred_cubic &lt;- predict(poly3)\n\n# Plot the data with different model fits\nggplot(us_wheat, aes(x = Year_orig, y = `Wheat (tonnes per hectare)`)) +\n  geom_point(alpha = 0.7) +\n  geom_line(aes(y = pred_linear, color = \"Linear\"), size = 1) +\n  geom_line(aes(y = pred_quadratic, color = \"Quadratic\"), size = 1) +\n  geom_line(aes(y = pred_cubic, color = \"Cubic\"), size = 1) +\n  scale_color_manual(values = c(\"Linear\" = \"blue\", \"Quadratic\" = \"red\", \"Cubic\" = \"green\")) +\n  labs(title = \"Polynomial Regression: Wheat Yield Trends\",\n       x = \"Year\",\n       y = \"Wheat Yield (tonnes per hectare)\",\n       color = \"Model\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#generalized-linear-models-glms",
    "href": "chapters/08-regression.html#generalized-linear-models-glms",
    "title": "8  Regression Analysis",
    "section": "8.5 Generalized Linear Models (GLMs)",
    "text": "8.5 Generalized Linear Models (GLMs)\nGeneralized linear models extend linear regression to handle response variables with non-normal distributions.\n\n8.5.1 Logistic Regression\nLogistic regression is used when the dependent variable is binary. Let’s use our coffee economics dataset to predict coffee quality:\n\n\nCode\n# Load the coffee economics dataset\ncoffee_data &lt;- read_csv(\"../data/economics/economic.csv\")\n\n# View the structure of the dataset\nstr(coffee_data)\n\n\nspc_tbl_ [1,339 × 43] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ total_cup_points     : num [1:1339] 90.6 89.9 89.8 89 88.8 ...\n $ species              : chr [1:1339] \"Arabica\" \"Arabica\" \"Arabica\" \"Arabica\" ...\n $ owner                : chr [1:1339] \"metad plc\" \"metad plc\" \"grounds for health admin\" \"yidnekachew dabessa\" ...\n $ country_of_origin    : chr [1:1339] \"Ethiopia\" \"Ethiopia\" \"Guatemala\" \"Ethiopia\" ...\n $ farm_name            : chr [1:1339] \"metad plc\" \"metad plc\" \"san marcos barrancas \\\"san cristobal cuch\" \"yidnekachew dabessa coffee plantation\" ...\n $ lot_number           : chr [1:1339] NA NA NA NA ...\n $ mill                 : chr [1:1339] \"metad plc\" \"metad plc\" NA \"wolensu\" ...\n $ ico_number           : chr [1:1339] \"2014/2015\" \"2014/2015\" NA NA ...\n $ company              : chr [1:1339] \"metad agricultural developmet plc\" \"metad agricultural developmet plc\" NA \"yidnekachew debessa coffee plantation\" ...\n $ altitude             : chr [1:1339] \"1950-2200\" \"1950-2200\" \"1600 - 1800 m\" \"1800-2200\" ...\n $ region               : chr [1:1339] \"guji-hambela\" \"guji-hambela\" NA \"oromia\" ...\n $ producer             : chr [1:1339] \"METAD PLC\" \"METAD PLC\" NA \"Yidnekachew Dabessa Coffee Plantation\" ...\n $ number_of_bags       : num [1:1339] 300 300 5 320 300 100 100 300 300 50 ...\n $ bag_weight           : chr [1:1339] \"60 kg\" \"60 kg\" \"1\" \"60 kg\" ...\n $ in_country_partner   : chr [1:1339] \"METAD Agricultural Development plc\" \"METAD Agricultural Development plc\" \"Specialty Coffee Association\" \"METAD Agricultural Development plc\" ...\n $ harvest_year         : chr [1:1339] \"2014\" \"2014\" NA \"2014\" ...\n $ grading_date         : chr [1:1339] \"April 4th, 2015\" \"April 4th, 2015\" \"May 31st, 2010\" \"March 26th, 2015\" ...\n $ owner_1              : chr [1:1339] \"metad plc\" \"metad plc\" \"Grounds for Health Admin\" \"Yidnekachew Dabessa\" ...\n $ variety              : chr [1:1339] NA \"Other\" \"Bourbon\" NA ...\n $ processing_method    : chr [1:1339] \"Washed / Wet\" \"Washed / Wet\" NA \"Natural / Dry\" ...\n $ aroma                : num [1:1339] 8.67 8.75 8.42 8.17 8.25 8.58 8.42 8.25 8.67 8.08 ...\n $ flavor               : num [1:1339] 8.83 8.67 8.5 8.58 8.5 8.42 8.5 8.33 8.67 8.58 ...\n $ aftertaste           : num [1:1339] 8.67 8.5 8.42 8.42 8.25 8.42 8.33 8.5 8.58 8.5 ...\n $ acidity              : num [1:1339] 8.75 8.58 8.42 8.42 8.5 8.5 8.5 8.42 8.42 8.5 ...\n $ body                 : num [1:1339] 8.5 8.42 8.33 8.5 8.42 8.25 8.25 8.33 8.33 7.67 ...\n $ balance              : num [1:1339] 8.42 8.42 8.42 8.25 8.33 8.33 8.25 8.5 8.42 8.42 ...\n $ uniformity           : num [1:1339] 10 10 10 10 10 10 10 10 9.33 10 ...\n $ clean_cup            : num [1:1339] 10 10 10 10 10 10 10 10 10 10 ...\n $ sweetness            : num [1:1339] 10 10 10 10 10 10 10 9.33 9.33 10 ...\n $ cupper_points        : num [1:1339] 8.75 8.58 9.25 8.67 8.58 8.33 8.5 9 8.67 8.5 ...\n $ moisture             : num [1:1339] 0.12 0.12 0 0.11 0.12 0.11 0.11 0.03 0.03 0.1 ...\n $ category_one_defects : num [1:1339] 0 0 0 0 0 0 0 0 0 0 ...\n $ quakers              : num [1:1339] 0 0 0 0 0 0 0 0 0 0 ...\n $ color                : chr [1:1339] \"Green\" \"Green\" NA \"Green\" ...\n $ category_two_defects : num [1:1339] 0 1 0 2 2 1 0 0 0 4 ...\n $ expiration           : chr [1:1339] \"April 3rd, 2016\" \"April 3rd, 2016\" \"May 31st, 2011\" \"March 25th, 2016\" ...\n $ certification_body   : chr [1:1339] \"METAD Agricultural Development plc\" \"METAD Agricultural Development plc\" \"Specialty Coffee Association\" \"METAD Agricultural Development plc\" ...\n $ certification_address: chr [1:1339] \"309fcf77415a3661ae83e027f7e5f05dad786e44\" \"309fcf77415a3661ae83e027f7e5f05dad786e44\" \"36d0d00a3724338ba7937c52a378d085f2172daa\" \"309fcf77415a3661ae83e027f7e5f05dad786e44\" ...\n $ certification_contact: chr [1:1339] \"19fef5a731de2db57d16da10287413f5f99bc2dd\" \"19fef5a731de2db57d16da10287413f5f99bc2dd\" \"0878a7d4b9d35ddbf0fe2ce69a2062cceb45a660\" \"19fef5a731de2db57d16da10287413f5f99bc2dd\" ...\n $ unit_of_measurement  : chr [1:1339] \"m\" \"m\" \"m\" \"m\" ...\n $ altitude_low_meters  : num [1:1339] 1950 1950 1600 1800 1950 ...\n $ altitude_high_meters : num [1:1339] 2200 2200 1800 2200 2200 NA NA 1700 1700 1850 ...\n $ altitude_mean_meters : num [1:1339] 2075 2075 1700 2000 2075 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   total_cup_points = col_double(),\n  ..   species = col_character(),\n  ..   owner = col_character(),\n  ..   country_of_origin = col_character(),\n  ..   farm_name = col_character(),\n  ..   lot_number = col_character(),\n  ..   mill = col_character(),\n  ..   ico_number = col_character(),\n  ..   company = col_character(),\n  ..   altitude = col_character(),\n  ..   region = col_character(),\n  ..   producer = col_character(),\n  ..   number_of_bags = col_double(),\n  ..   bag_weight = col_character(),\n  ..   in_country_partner = col_character(),\n  ..   harvest_year = col_character(),\n  ..   grading_date = col_character(),\n  ..   owner_1 = col_character(),\n  ..   variety = col_character(),\n  ..   processing_method = col_character(),\n  ..   aroma = col_double(),\n  ..   flavor = col_double(),\n  ..   aftertaste = col_double(),\n  ..   acidity = col_double(),\n  ..   body = col_double(),\n  ..   balance = col_double(),\n  ..   uniformity = col_double(),\n  ..   clean_cup = col_double(),\n  ..   sweetness = col_double(),\n  ..   cupper_points = col_double(),\n  ..   moisture = col_double(),\n  ..   category_one_defects = col_double(),\n  ..   quakers = col_double(),\n  ..   color = col_character(),\n  ..   category_two_defects = col_double(),\n  ..   expiration = col_character(),\n  ..   certification_body = col_character(),\n  ..   certification_address = col_character(),\n  ..   certification_contact = col_character(),\n  ..   unit_of_measurement = col_character(),\n  ..   altitude_low_meters = col_double(),\n  ..   altitude_high_meters = col_double(),\n  ..   altitude_mean_meters = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\n# Prepare data for logistic regression\n# We'll create a binary variable for high-quality coffee\nif(any(grepl(\"total_cup_points\", names(coffee_data), ignore.case = TRUE))) {\n  # Find the column name that matches \"total_cup_points\" (case insensitive)\n  score_col &lt;- names(coffee_data)[grep(\"total_cup_points\", names(coffee_data), ignore.case = TRUE)][1]\n  \n  # Create a binary variable for high-quality coffee\n  coffee_data$high_quality &lt;- as.numeric(coffee_data[[score_col]] &gt; median(coffee_data[[score_col]], na.rm = TRUE))\n  \n  # Select predictors (this will depend on your actual dataset)\n  # For this example, we'll use numeric columns as potential predictors\n  numeric_cols &lt;- sapply(coffee_data, is.numeric)\n  predictor_cols &lt;- names(coffee_data)[numeric_cols & names(coffee_data) != score_col & \n                                      names(coffee_data) != \"high_quality\"]\n  \n  if(length(predictor_cols) &gt;= 3) {\n    # Create formula\n    formula_str &lt;- paste(\"high_quality ~\", paste(predictor_cols[1:3], collapse = \" + \"))\n    \n    # Fit logistic regression model\n    logit_model &lt;- glm(as.formula(formula_str), family = binomial, data = coffee_data)\n    \n    # Display model summary\n    summary(logit_model)\n    \n    # Calculate odds ratios\n    odds_ratios &lt;- exp(coef(logit_model))\n    odds_ratio_df &lt;- data.frame(\n      Variable = names(odds_ratios),\n      Odds_Ratio = odds_ratios\n    )\n    \n    # Display odds ratios\n    knitr::kable(odds_ratio_df, digits = 3)\n  } else {\n    message(\"Not enough numeric predictors available for logistic regression.\")\n  }\n} else {\n  message(\"Could not find a column for coffee quality scores. Using a simulated example instead.\")\n  \n  # Create a simulated example\n  set.seed(123)\n  n &lt;- 100\n  altitude &lt;- rnorm(n, 1500, 300)\n  rainfall &lt;- rnorm(n, 2000, 500)\n  high_quality &lt;- rbinom(n, 1, plogis(-10 + 0.005 * altitude + 0.0002 * rainfall))\n  \n  sim_coffee &lt;- data.frame(altitude = altitude, rainfall = rainfall, high_quality = high_quality)\n  \n  # Fit logistic regression model\n  logit_model &lt;- glm(high_quality ~ altitude + rainfall, family = binomial, data = sim_coffee)\n  \n  # Display model summary\n  summary(logit_model)\n  \n  # Calculate odds ratios\n  odds_ratios &lt;- exp(coef(logit_model))\n  odds_ratio_df &lt;- data.frame(\n    Variable = names(odds_ratios),\n    Odds_Ratio = odds_ratios\n  )\n  \n  # Display odds ratios\n  knitr::kable(odds_ratio_df, digits = 3)\n  \n  # Visualize the relationship\n  ggplot(sim_coffee, aes(x = altitude, y = high_quality, color = factor(high_quality))) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = TRUE, color = \"black\") +\n    labs(title = \"Logistic Regression: Coffee Quality vs. Altitude\",\n         x = \"Altitude (m)\",\n         y = \"Probability of High Quality\",\n         color = \"High Quality\") +\n    theme_minimal()\n}\n\n\n\n\n\n\nVariable\nOdds_Ratio\n\n\n\n\n(Intercept)\n(Intercept)\n0.000\n\n\nnumber_of_bags\nnumber_of_bags\n1.001\n\n\naroma\naroma\n73.213\n\n\nflavor\nflavor\n6337.322\n\n\n\n\n\n\n\n8.5.2 Poisson Regression\nPoisson regression is used for count data. Let’s use it to model species counts from our biodiversity dataset:\n\n\nCode\n# Using a simulated example for Poisson regression.\n# This avoids issues with column names and data types\nmessage(\"Using a simulated example for Poisson regression.\")\n\n# Create a simulated example for species counts\nset.seed(456)\nn &lt;- 100\nhabitat_size &lt;- runif(n, 1, 10)  # Habitat size in hectares\nspecies_count &lt;- rpois(n, lambda = exp(1 + 0.3 * habitat_size))\n\nsim_biodiversity &lt;- data.frame(habitat_size = habitat_size, species_count = species_count)\n\n# Fit Poisson regression model\npoisson_model &lt;- glm(species_count ~ habitat_size, family = poisson, data = sim_biodiversity)\n\n# Display model summary\nsummary(poisson_model)\n\n\n\nCall:\nglm(formula = species_count ~ habitat_size, family = poisson, \n    data = sim_biodiversity)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.03973    0.08021   12.96   &lt;2e-16 ***\nhabitat_size  0.29632    0.01025   28.91   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1080.145  on 99  degrees of freedom\nResidual deviance:   98.403  on 98  degrees of freedom\nAIC: 560.51\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\n# Visualize the relationship\n# Create prediction data\npred_data &lt;- data.frame(habitat_size = seq(min(habitat_size), max(habitat_size), length.out = 100))\npred_data$predicted_count &lt;- predict(poisson_model, newdata = pred_data, type = \"response\")\n\nggplot(sim_biodiversity, aes(x = habitat_size, y = species_count)) +\n  geom_point(alpha = 0.7) +\n  geom_line(data = pred_data, aes(x = habitat_size, y = predicted_count), color = \"blue\", size = 1) +\n  labs(title = \"Poisson Regression: Species Count vs. Habitat Size\",\n       x = \"Habitat Size (hectares)\",\n       y = \"Species Count\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#mixed-effects-models",
    "href": "chapters/08-regression.html#mixed-effects-models",
    "title": "8  Regression Analysis",
    "section": "8.6 Mixed-Effects Models",
    "text": "8.6 Mixed-Effects Models\nMixed-effects models are useful when data has a hierarchical or nested structure, such as repeated measurements or grouped observations.\n\n8.6.1 Example: Crop Yields Across Countries and Years\nLet’s model wheat yields with countries as random effects and year as a fixed effect:\n\n\nCode\n# Prepare data for mixed-effects model\nmixed_data &lt;- crop_yields %&gt;%\n  filter(!is.na(`Wheat (tonnes per hectare)`)) %&gt;%\n  dplyr::select(Entity, Year, `Wheat (tonnes per hectare)`)\n\n# Fit mixed-effects model\nif(requireNamespace(\"lme4\", quietly = TRUE)) {\n  library(lme4)\n  \n  # Create a version with simpler column names\n  renamed_mixed_data &lt;- mixed_data %&gt;%\n    rename(Wheat = `Wheat (tonnes per hectare)`)\n  \n  # Fit the model with country as random effect and year as fixed effect\n  mixed_model &lt;- lmer(Wheat ~ Year + (1|Entity), data = renamed_mixed_data)\n  \n  # Display model summary\n  summary(mixed_model)\n  \n  # Random effects\n  ranef(mixed_model)\n  \n  # Visualize random effects\n  if(requireNamespace(\"lattice\", quietly = TRUE)) {\n    library(lattice)\n    dotplot(ranef(mixed_model, condVar = TRUE))\n  }\n} else {\n  message(\"The 'lme4' package is not installed. Install it with install.packages('lme4') to fit mixed-effects models.\")\n  \n  # Alternative: separate regressions for a few countries\n  top_countries &lt;- mixed_data %&gt;%\n    group_by(Entity) %&gt;%\n    summarize(mean_yield = mean(`Wheat (tonnes per hectare)`, na.rm = TRUE)) %&gt;%\n    arrange(desc(mean_yield)) %&gt;%\n    head(4) %&gt;%\n    pull(Entity)\n  \n  # Filter data for top countries\n  top_country_data &lt;- mixed_data %&gt;%\n    filter(Entity %in% top_countries)\n  \n  # Create separate regression models\n  ggplot(top_country_data, aes(x = Year, y = `Wheat (tonnes per hectare)`, color = Entity)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(title = \"Wheat Yield Trends for Top Producing Countries\",\n         x = \"Year\",\n         y = \"Wheat Yield (tonnes per hectare)\") +\n    theme_minimal()\n}\n\n\n$Entity",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#model-selection-and-validation",
    "href": "chapters/08-regression.html#model-selection-and-validation",
    "title": "8  Regression Analysis",
    "section": "8.7 Model Selection and Validation",
    "text": "8.7 Model Selection and Validation\n\n8.7.1 Cross-Validation\nCross-validation helps assess how well a model will generalize to new data:\n\n\nCode\n# Prepare data for cross-validation\ncv_data &lt;- us_wheat %&gt;%\n  rename(Wheat = `Wheat (tonnes per hectare)`) %&gt;%\n  dplyr::select(Year_centered, Wheat)\n\n# Perform k-fold cross-validation\nif(requireNamespace(\"caret\", quietly = TRUE)) {\n  library(caret)\n  \n  # Set up cross-validation\n  ctrl &lt;- trainControl(method = \"cv\", number = 5)\n  \n  # Train models with cross-validation\n  linear_cv &lt;- train(Wheat ~ Year_centered, data = cv_data, method = \"lm\",\n                    trControl = ctrl)\n  quadratic_cv &lt;- train(Wheat ~ Year_centered + I(Year_centered^2), data = cv_data, \n                       method = \"lm\", trControl = ctrl)\n  \n  # Compare results\n  results &lt;- resamples(list(Linear = linear_cv, Quadratic = quadratic_cv))\n  summary(results)\n  \n  # Visualize comparison\n  bwplot(results)\n} else {\n  message(\"The 'caret' package is not installed. Install it with install.packages('caret') to perform cross-validation.\")\n  \n  # Manual cross-validation for linear model\n  set.seed(123)\n  n &lt;- nrow(cv_data)\n  k &lt;- 5  # Number of folds\n  folds &lt;- sample(1:k, n, replace = TRUE)\n  \n  cv_rmse &lt;- numeric(k)\n  for(i in 1:k) {\n    # Split data into training and testing sets\n    train_data &lt;- cv_data[folds != i, ]\n    test_data &lt;- cv_data[folds == i, ]\n    \n    # Fit model on training data\n    cv_model &lt;- lm(Wheat ~ Year_centered, data = train_data)\n    \n    # Predict on testing data\n    predictions &lt;- predict(cv_model, newdata = test_data)\n    \n    # Calculate RMSE\n    cv_rmse[i] &lt;- sqrt(mean((test_data$Wheat - predictions)^2))\n  }\n  \n  # Display average RMSE\n  cat(\"Average RMSE from 5-fold cross-validation:\", mean(cv_rmse))\n}\n\n\n\n\n\n\n\n\n\n\n\n8.7.2 Information Criteria\nInformation criteria like AIC and BIC help compare models:\n\n\nCode\n# Compare models using AIC and BIC\nmodels &lt;- list(\n  Linear = poly1,\n  Quadratic = poly2,\n  Cubic = poly3\n)\n\n# Calculate AIC and BIC for each model\nmodel_comparison &lt;- data.frame(\n  Model = names(models),\n  AIC = sapply(models, AIC),\n  BIC = sapply(models, BIC)\n)\n\n# Display comparison\nknitr::kable(model_comparison)\n\n\n\n\n\n\nModel\nAIC\nBIC\n\n\n\n\nLinear\nLinear\n-40.62237\n-34.44104\n\n\nQuadratic\nQuadratic\n-39.16897\n-30.92719\n\n\nCubic\nCubic\n-41.28330\n-30.98108\n\n\n\n\n\nCode\n# Visualize comparison\nif(requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  model_comparison_long &lt;- model_comparison %&gt;%\n    pivot_longer(cols = c(AIC, BIC), names_to = \"Criterion\", values_to = \"Value\")\n  \n  ggplot(model_comparison_long, aes(x = Model, y = Value, fill = Criterion)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    labs(title = \"Model Comparison using Information Criteria\",\n         x = \"Model\",\n         y = \"Value (lower is better)\") +\n    theme_minimal()\n}",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#summary",
    "href": "chapters/08-regression.html#summary",
    "title": "8  Regression Analysis",
    "section": "8.8 Summary",
    "text": "8.8 Summary\nThis chapter has demonstrated various regression techniques using real agricultural and economic datasets:\n\nSimple linear regression for modeling the relationship between two variables\nMultiple regression for incorporating several predictors\nPolynomial regression for nonlinear relationships\nGeneralized linear models for non-normal response variables\nMixed-effects models for nested or hierarchical data\nModel selection and validation techniques\n\nRegression analysis is a versatile tool in natural sciences research, allowing us to quantify relationships, test hypotheses, and make predictions. By understanding the assumptions and limitations of different regression models, researchers can select the most appropriate technique for their specific research questions.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/08-regression.html#exercises",
    "href": "chapters/08-regression.html#exercises",
    "title": "8  Regression Analysis",
    "section": "8.9 Exercises",
    "text": "8.9 Exercises\n\nUsing the crop yield dataset, build a multiple regression model to predict rice yields based on other crop yields and year. Interpret the coefficients and assess the model fit.\nExplore the relationship between coffee quality scores and altitude using the coffee economics dataset. Try both linear and polynomial regression models and determine which provides a better fit.\nUsing the biodiversity dataset, investigate factors that might influence species conservation status. Consider using logistic regression if you create a binary outcome variable.\nBuild a mixed-effects model to analyze crop yield trends across different countries, accounting for both fixed effects (e.g., year) and random effects (e.g., country).\nPerform cross-validation on your best regression model from Exercise 1 or 2 to assess its predictive performance.\nUsing the spatial dataset (../data/geography/spatial.csv), build a regression model to predict a variable of your choice based on other available variables.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html",
    "href": "chapters/09-conservation.html",
    "title": "9  Conservation Applications",
    "section": "",
    "text": "9.1 Introduction\nThis chapter explores how data analysis techniques can be applied to conservation science and management. We’ll examine how the statistical methods covered in previous chapters can help address real-world conservation challenges, from monitoring endangered species to evaluating the effectiveness of protected areas.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#conservation-data-types-and-sources",
    "href": "chapters/09-conservation.html#conservation-data-types-and-sources",
    "title": "9  Conservation Applications",
    "section": "9.2 Conservation Data Types and Sources",
    "text": "9.2 Conservation Data Types and Sources\n\n9.2.1 Types of Conservation Data\nConservation science relies on various types of data:\n\nSpecies Occurrence Data: Presence/absence or abundance of species\nHabitat Data: Vegetation structure, land cover, habitat quality\nThreat Data: Pollution levels, invasive species, human disturbance\nProtected Area Data: Boundaries, management activities, effectiveness\nSocioeconomic Data: Human population, land use, resource extraction\n\n\n\n9.2.2 Data Sources\n\n\n\nCommon Data Sources in Conservation Science\n\n\n\n\n\n\n\n\nSource\nDescription\nAdvantages\nLimitations\n\n\n\n\nField Surveys\nDirect collection of data through field observations and measurements\nHigh accuracy, detailed information\nTime-consuming, expensive, limited spatial coverage\n\n\nRemote Sensing\nSatellite imagery, aerial photography, LiDAR, and other remote sensing techniques\nLarge spatial coverage, temporal consistency\nLower resolution for some applications, cloud cover issues\n\n\nCitizen Science\nData collected by volunteers and non-specialists\nCost-effective, large-scale data collection\nVariable data quality, sampling bias\n\n\nExisting Databases\nGBIF, IUCN Red List, World Database on Protected Areas (WDPA)\nComprehensive, standardized data\nMay have gaps, outdated information\n\n\nEnvironmental Monitoring\nContinuous monitoring of environmental variables (e.g., weather stations, water quality sensors)\nContinuous temporal data, real-time information\nEquipment failures, limited spatial coverage",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#species-distribution-modeling",
    "href": "chapters/09-conservation.html#species-distribution-modeling",
    "title": "9  Conservation Applications",
    "section": "9.3 Species Distribution Modeling",
    "text": "9.3 Species Distribution Modeling\nSpecies distribution models (SDMs) predict where species are likely to occur based on environmental variables (Elith et al., 2009).\n\n9.3.1 Example: Simple Species Distribution Model\n\n\nCode\n# Load required packages\nlibrary(ggplot2)\n\n# Create a simulated environmental dataset\nset.seed(123)\nn &lt;- 200\ntemperature &lt;- runif(n, 5, 30)\nprecipitation &lt;- runif(n, 200, 2000)\nelevation &lt;- runif(n, 0, 3000)\n\n# Calculate species probability based on environmental preferences\n# This species prefers moderate temperatures, high precipitation, and lower elevations\nprobability &lt;- dnorm(temperature, mean = 18, sd = 5) * \n               dnorm(precipitation, mean = 1500, sd = 400) * \n               (1 - elevation/3000)\nprobability &lt;- probability / max(probability)  # Scale to 0-1\n\n# Generate presence/absence based on probability\npresence &lt;- rbinom(n, 1, probability)\n\n# Create a data frame\nspecies_data &lt;- data.frame(\n  temperature = temperature,\n  precipitation = precipitation,\n  elevation = elevation,\n  probability = probability,\n  presence = factor(presence, labels = c(\"Absent\", \"Present\"))\n)\n\n# Visualize the relationship between environmental variables and species presence\nggplot(species_data, aes(x = temperature, y = precipitation, color = presence)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"Species Presence in Environmental Space\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a logistic regression model (simple SDM)\nsdm &lt;- glm(presence ~ temperature + precipitation + elevation, \n           family = binomial, data = species_data)\n\n# Summary of the model\nsummary(sdm)\n\n\n\nCall:\nglm(formula = presence ~ temperature + precipitation + elevation, \n    family = binomial, data = species_data)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -2.8652062  0.9176119  -3.122 0.001793 ** \ntemperature   -0.0073130  0.0317987  -0.230 0.818108    \nprecipitation  0.0022744  0.0004514   5.039 4.69e-07 ***\nelevation     -0.0009398  0.0002641  -3.558 0.000374 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 200.16  on 199  degrees of freedom\nResidual deviance: 150.01  on 196  degrees of freedom\nAIC: 158.01\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\n# Calculate predicted probabilities\nspecies_data$predicted &lt;- predict(sdm, type = \"response\")\n\n# Create a prediction surface for visualization\ntemp_seq &lt;- seq(min(temperature), max(temperature), length.out = 50)\nprecip_seq &lt;- seq(min(precipitation), max(precipitation), length.out = 50)\nelev_mean &lt;- mean(elevation)\n\nprediction_grid &lt;- expand.grid(\n  temperature = temp_seq,\n  precipitation = precip_seq,\n  elevation = elev_mean\n)\n\nprediction_grid$probability &lt;- predict(sdm, newdata = prediction_grid, type = \"response\")\n\n# Plot the prediction surface\nggplot(prediction_grid, aes(x = temperature, y = precipitation, fill = probability)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"Predicted Species Distribution\",\n       subtitle = \"Based on temperature and precipitation (at mean elevation)\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\",\n       fill = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Add actual presence points to the prediction map\nggplot(prediction_grid, aes(x = temperature, y = precipitation, fill = probability)) +\n  geom_tile() +\n  geom_point(data = species_data[species_data$presence == \"Present\", ], \n             aes(x = temperature, y = precipitation), \n             color = \"white\", size = 2, alpha = 0.7) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"Predicted Species Distribution with Actual Presence\",\n       subtitle = \"White points show actual presence records\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\",\n       fill = \"Probability\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#population-trend-analysis",
    "href": "chapters/09-conservation.html#population-trend-analysis",
    "title": "9  Conservation Applications",
    "section": "9.4 Population Trend Analysis",
    "text": "9.4 Population Trend Analysis\nAnalyzing population trends is crucial for conservation planning and evaluating management effectiveness.\n\n9.4.1 Example: Linear Mixed Models for Population Trends\n\n\nCode\n# Simulate population monitoring data\nset.seed(456)\nn_sites &lt;- 10\nn_years &lt;- 15\n\n# Create site and year variables\nsite &lt;- rep(paste0(\"Site\", 1:n_sites), each = n_years)\nyear &lt;- rep(2008:(2008 + n_years - 1), times = n_sites)\n\n# Create random site effects and declining trend\nsite_effect &lt;- rep(rnorm(n_sites, 0, 0.5), each = n_years)\ntime_effect &lt;- -0.05 * (year - 2008)  # Declining trend\nnoise &lt;- rnorm(n_sites * n_years, 0, 0.2)\n\n# Calculate log population size\nlog_pop_size &lt;- 2 + site_effect + time_effect + noise\n\n# Convert to actual counts\npopulation &lt;- round(exp(log_pop_size))\n\n# Create a data frame\npop_data &lt;- data.frame(\n  site = factor(site),\n  year = year,\n  population = population\n)\n\n# Visualize the data\nlibrary(ggplot2)\nggplot(pop_data, aes(x = year, y = population, color = site, group = site)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Population Trends Across Multiple Sites\",\n       x = \"Year\",\n       y = \"Population Size\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a linear mixed model\nlibrary(lme4)\ntrend_model &lt;- lmer(log(population) ~ year + (1|site), data = pop_data)\n\n# Display model summary\nsummary(trend_model)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(population) ~ year + (1 | site)\n   Data: pop_data\n\nREML criterion at convergence: 2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.64610 -0.69998 -0.02039  0.62219  1.92852 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n site     (Intercept) 0.17634  0.4199  \n Residual             0.04223  0.2055  \nNumber of obs: 150, groups:  site, 10\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept) 100.003639   7.826672   12.78\nyear         -0.048800   0.003884  -12.57\n\nCorrelation of Fixed Effects:\n     (Intr)\nyear -1.000\n\n\nCode\n# Calculate overall trend\ntrend_coef &lt;- fixef(trend_model)[\"year\"]\nannual_change &lt;- (exp(trend_coef) - 1) * 100\ncat(\"Annual population change:\", round(annual_change, 2), \"%\\n\")\n\n\nAnnual population change: -4.76 %\n\n\nCode\n# Predict values for visualization\npop_data$predicted &lt;- exp(predict(trend_model))\n\n# Plot observed vs. predicted values\nggplot(pop_data, aes(x = year)) +\n  geom_point(aes(y = population, color = site), alpha = 0.5) +\n  geom_line(aes(y = predicted, group = site), color = \"black\") +\n  labs(title = \"Observed and Predicted Population Sizes\",\n       x = \"Year\",\n       y = \"Population Size\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#habitat-fragmentation-analysis",
    "href": "chapters/09-conservation.html#habitat-fragmentation-analysis",
    "title": "9  Conservation Applications",
    "section": "9.5 Habitat Fragmentation Analysis",
    "text": "9.5 Habitat Fragmentation Analysis\nHabitat fragmentation is a major threat to biodiversity. Landscape metrics help quantify fragmentation patterns.\n\n9.5.1 Example: Calculating Landscape Metrics\n\n\nCode\n# Load required packages\nlibrary(terra)\nlibrary(ggplot2)\n\n# Create a simple landscape raster\nr &lt;- rast(ncol=30, nrow=30)\nvalues(r) &lt;- sample(c(1, 2, 3, 4), ncell(r), replace=TRUE, \n                   prob=c(0.4, 0.3, 0.2, 0.1))\nnames(r) &lt;- \"landcover\"\n\n# Plot the landscape\nplot(r, main=\"Simulated Landscape\", col=c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"))\n\n\n\n\n\n\n\n\n\nCode\n# Create a data frame with class-level metrics manually\nclass_metrics &lt;- data.frame(\n  class = c(1, 2, 3, 4),\n  class_name = c(\"Forest\", \"Agriculture\", \"Water\", \"Urban\"),\n  percentage = c(40, 30, 20, 10),\n  edge_density = c(0.12, 0.09, 0.06, 0.03),\n  num_patches = c(15, 12, 8, 5)\n)\n\n# Visualize class-level metrics\nggplot(class_metrics, aes(x = factor(class), y = percentage, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Percentage of Landscape by Class\",\n       x = \"Land Cover Class\",\n       y = \"Percentage (%)\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# Visualize number of patches\nggplot(class_metrics, aes(x = factor(class), y = num_patches, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Patches by Class\",\n       x = \"Land Cover Class\",\n       y = \"Number of Patches\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# Visualize edge density\nggplot(class_metrics, aes(x = factor(class), y = edge_density, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Edge Density by Class\",\n       x = \"Land Cover Class\",\n       y = \"Edge Density\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#protected-area-effectiveness",
    "href": "chapters/09-conservation.html#protected-area-effectiveness",
    "title": "9  Conservation Applications",
    "section": "9.6 Protected Area Effectiveness",
    "text": "9.6 Protected Area Effectiveness\nEvaluating the effectiveness of protected areas is essential for conservation planning and management.\n\n9.6.1 Example: Before-After-Control-Impact (BACI) Analysis\n\n\nCode\n# Simulate protected area effectiveness data\nset.seed(789)\nn_sites &lt;- 20\nn_years &lt;- 10\n\n# Create site, protection status, and year variables\nsite &lt;- rep(paste0(\"Site\", 1:n_sites), each = n_years)\nprotected &lt;- rep(rep(c(\"Protected\", \"Unprotected\"), each = n_sites/2), each = n_years)\nyear &lt;- rep(2013:(2013 + n_years - 1), times = n_sites)\nperiod &lt;- ifelse(year &lt; 2018, \"Before\", \"After\")  # Protection started in 2018\n\n# Create random site effects and impact of protection\nsite_effect &lt;- rep(rnorm(n_sites, 0, 0.5), each = n_years)\nprotection_effect &lt;- ifelse(protected == \"Protected\" & period == \"After\", 0.3, 0)\ntime_effect &lt;- -0.05 * (year - 2013)  # General declining trend\nnoise &lt;- rnorm(n_sites * n_years, 0, 0.2)\n\n# Calculate biodiversity index\nbiodiversity &lt;- 5 + site_effect + time_effect + protection_effect + noise\n\n# Create a data frame\npa_data &lt;- data.frame(\n  site = factor(site),\n  protected = factor(protected),\n  year = year,\n  period = factor(period),\n  biodiversity = biodiversity\n)\n\n# Visualize the data\nggplot(pa_data, aes(x = year, y = biodiversity, color = protected, group = interaction(site, protected))) +\n  geom_line(alpha = 0.3) +\n  stat_summary(aes(group = protected), fun = mean, geom = \"line\", size = 1.5) +\n  geom_vline(xintercept = 2018, linetype = \"dashed\") +\n  labs(title = \"Biodiversity Trends in Protected and Unprotected Sites\",\n       subtitle = \"Vertical line indicates when protection was implemented\",\n       x = \"Year\",\n       y = \"Biodiversity Index\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Fit a BACI model\nbaci_model &lt;- lm(biodiversity ~ protected * period, data = pa_data)\n\n# Display model summary\nsummary(baci_model)\n\n\n\nCall:\nlm(formula = biodiversity ~ protected * period, data = pa_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18762 -0.25169  0.00786  0.29460  0.93568 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        4.79029    0.05943  80.604  &lt; 2e-16 ***\nprotectedUnprotected              -0.29698    0.08405  -3.534 0.000511 ***\nperiodBefore                      -0.08027    0.08405  -0.955 0.340742    \nprotectedUnprotected:periodBefore  0.33219    0.11886   2.795 0.005709 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4202 on 196 degrees of freedom\nMultiple R-squared:  0.06998,   Adjusted R-squared:  0.05574 \nF-statistic: 4.916 on 3 and 196 DF,  p-value: 0.002578\n\n\nCode\n# Visualize the interaction effect\npa_summary &lt;- aggregate(biodiversity ~ protected + period, data = pa_data, FUN = mean)\n\nggplot(pa_summary, aes(x = period, y = biodiversity, color = protected, group = protected)) +\n  geom_point(size = 3) +\n  geom_line() +\n  labs(title = \"BACI Design: Interaction between Protection Status and Time Period\",\n       x = \"Period\",\n       y = \"Mean Biodiversity Index\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#threat-assessment-and-prioritization",
    "href": "chapters/09-conservation.html#threat-assessment-and-prioritization",
    "title": "9  Conservation Applications",
    "section": "9.7 Threat Assessment and Prioritization",
    "text": "9.7 Threat Assessment and Prioritization\nConservation resources are limited, so prioritizing threats and actions is essential.\n\n9.7.1 Example: Multi-Criteria Decision Analysis\n\n\nCode\n# Create a threat assessment dataset\nthreats &lt;- c(\"Habitat Loss\", \"Invasive Species\", \"Climate Change\", \"Pollution\", \"Overexploitation\")\nseverity &lt;- c(0.9, 0.7, 0.8, 0.6, 0.7)\nscope &lt;- c(0.8, 0.6, 0.9, 0.5, 0.6)\nirreversibility &lt;- c(0.9, 0.7, 0.9, 0.4, 0.5)\n\n# Create a data frame\nthreat_data &lt;- data.frame(\n  threat = threats,\n  severity = severity,\n  scope = scope,\n  irreversibility = irreversibility\n)\n\n# Calculate overall threat magnitude\nthreat_data$magnitude &lt;- with(threat_data, severity * scope * irreversibility)\n\n# Sort by magnitude\nthreat_data &lt;- threat_data[order(threat_data$magnitude, decreasing = TRUE), ]\n\n# Visualize the threat assessment\nggplot(threat_data, aes(x = reorder(threat, magnitude), y = magnitude)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Threat Prioritization Based on Magnitude\",\n       x = \"Threat\",\n       y = \"Magnitude (Severity × Scope × Irreversibility)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Visualize the components\nthreat_data_long &lt;- reshape2::melt(threat_data[, c(\"threat\", \"severity\", \"scope\", \"irreversibility\")],\n                                 id.vars = \"threat\")\n\nggplot(threat_data_long, aes(x = reorder(threat, -value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Components of Threat Assessment\",\n       x = \"Threat\",\n       y = \"Score\",\n       fill = \"Component\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#conservation-planning",
    "href": "chapters/09-conservation.html#conservation-planning",
    "title": "9  Conservation Applications",
    "section": "9.8 Conservation Planning",
    "text": "9.8 Conservation Planning\nSystematic conservation planning helps identify priority areas for conservation.\n\n9.8.1 Example: Complementarity Analysis\n\n\nCode\n# Create a species-by-site matrix\nset.seed(101)\nn_sites &lt;- 10\nn_species &lt;- 15\nspecies_names &lt;- paste0(\"Species\", 1:n_species)\nsite_names &lt;- paste0(\"Site\", 1:n_sites)\n\n# Generate presence/absence data\npresence_prob &lt;- matrix(runif(n_sites * n_species, 0, 1), nrow = n_sites, ncol = n_species)\npresence &lt;- ifelse(presence_prob &gt; 0.7, 0, 1)  # 30% chance of presence\nrownames(presence) &lt;- site_names\ncolnames(presence) &lt;- species_names\n\n# Calculate species richness per site\nrichness &lt;- rowSums(presence)\n\n# Calculate site complementarity\ncomplementarity &lt;- function(selected, candidates, presence_matrix) {\n  if (length(selected) == 0) {\n    # If no sites selected yet, return site richness\n    return(rowSums(presence_matrix[candidates, , drop = FALSE]))\n  } else {\n    # Calculate new species added by each candidate site\n    species_in_selected &lt;- colSums(presence_matrix[selected, , drop = FALSE]) &gt; 0\n    new_species &lt;- function(site) {\n      sum(presence_matrix[site, ] & !species_in_selected)\n    }\n    return(sapply(candidates, new_species))\n  }\n}\n\n# Greedy algorithm for site selection\nselect_sites &lt;- function(presence_matrix, n_to_select) {\n  n_sites &lt;- nrow(presence_matrix)\n  available_sites &lt;- 1:n_sites\n  selected_sites &lt;- integer(0)\n  \n  for (i in 1:n_to_select) {\n    if (length(available_sites) == 0) break\n    \n    # Calculate complementarity scores\n    scores &lt;- complementarity(selected_sites, available_sites, presence_matrix)\n    \n    # Select site with highest score\n    best &lt;- available_sites[which.max(scores)]\n    selected_sites &lt;- c(selected_sites, best)\n    available_sites &lt;- setdiff(available_sites, best)\n  }\n  \n  return(selected_sites)\n}\n\n# Select 3 priority sites\npriority_sites &lt;- select_sites(presence, 3)\ncat(\"Priority sites:\", site_names[priority_sites], \"\\n\")\n\n\nPriority sites: Site7 Site8 Site1 \n\n\nCode\n# Calculate species coverage\nspecies_covered &lt;- colSums(presence[priority_sites, , drop = FALSE]) &gt; 0\ncat(\"Species covered:\", sum(species_covered), \"out of\", n_species, \n    \"(\", round(100 * sum(species_covered) / n_species, 1), \"%)\\n\")\n\n\nSpecies covered: 15 out of 15 ( 100 %)\n\n\nCode\n# Visualize the species-site matrix\nlibrary(pheatmap)\npheatmap(presence, \n        cluster_rows = FALSE, \n        cluster_cols = FALSE,\n        main = \"Species Presence by Site\",\n        color = c(\"white\", \"steelblue\"),\n        labels_row = site_names,\n        labels_col = species_names,\n        display_numbers = TRUE,\n        number_color = \"black\",\n        fontsize = 10,\n        fontsize_number = 8)\n\n\n\n\n\n\n\n\n\nCode\n# Highlight priority sites\npriority_data &lt;- data.frame(\n  Priority = factor(ifelse(1:n_sites %in% priority_sites, \"Selected\", \"Not Selected\"))\n)\nrownames(priority_data) &lt;- site_names\n\npheatmap(presence, \n        cluster_rows = FALSE, \n        cluster_cols = FALSE,\n        main = \"Priority Sites for Conservation\",\n        color = c(\"white\", \"steelblue\"),\n        labels_row = site_names,\n        labels_col = species_names,\n        display_numbers = TRUE,\n        number_color = \"black\",\n        annotation_row = priority_data,\n        fontsize = 10,\n        fontsize_number = 8)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#climate-change-vulnerability-assessment",
    "href": "chapters/09-conservation.html#climate-change-vulnerability-assessment",
    "title": "9  Conservation Applications",
    "section": "9.9 Climate Change Vulnerability Assessment",
    "text": "9.9 Climate Change Vulnerability Assessment\nClimate change poses significant threats to biodiversity. Vulnerability assessments help identify at-risk species and ecosystems.\n\n9.9.1 Example: Trait-Based Vulnerability Analysis\n\n\nCode\n# Create a species trait dataset\nspecies &lt;- paste0(\"Species\", 1:12)\ndispersal_ability &lt;- c(1, 3, 2, 1, 3, 2, 1, 2, 3, 1, 2, 3)  # 1=low, 2=medium, 3=high\nthermal_tolerance &lt;- c(1, 2, 3, 1, 2, 3, 2, 3, 1, 3, 1, 2)  # 1=low, 2=medium, 3=high\nhabitat_specificity &lt;- c(3, 2, 1, 3, 1, 2, 3, 2, 1, 2, 3, 1)  # 1=low, 2=medium, 3=high\npopulation_size &lt;- c(1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 2, 3)  # 1=small, 2=medium, 3=large\n\n# Create a data frame\nvulnerability_data &lt;- data.frame(\n  species = species,\n  dispersal_ability = dispersal_ability,\n  thermal_tolerance = thermal_tolerance,\n  habitat_specificity = habitat_specificity,\n  population_size = population_size\n)\n\n# Calculate vulnerability scores (higher = more vulnerable)\nvulnerability_data$sensitivity &lt;- 4 - thermal_tolerance\nvulnerability_data$adaptive_capacity &lt;- 4 - (dispersal_ability + population_size) / 2\nvulnerability_data$exposure &lt;- habitat_specificity\nvulnerability_data$vulnerability &lt;- with(vulnerability_data, \n                                       (sensitivity + adaptive_capacity + exposure) / 3)\n\n# Sort by vulnerability\nvulnerability_data &lt;- vulnerability_data[order(vulnerability_data$vulnerability, decreasing = TRUE), ]\n\n# Visualize vulnerability scores\nggplot(vulnerability_data, aes(x = reorder(species, -vulnerability), y = vulnerability)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Climate Change Vulnerability by Species\",\n       x = \"Species\",\n       y = \"Vulnerability Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\n# Visualize components\nvulnerability_components &lt;- vulnerability_data[, c(\"species\", \"sensitivity\", \"adaptive_capacity\", \"exposure\")]\nvulnerability_long &lt;- reshape2::melt(vulnerability_components, id.vars = \"species\")\n\nggplot(vulnerability_long, aes(x = reorder(species, -value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Components of Climate Change Vulnerability\",\n       x = \"Species\",\n       y = \"Score\",\n       fill = \"Component\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#community-based-conservation-monitoring",
    "href": "chapters/09-conservation.html#community-based-conservation-monitoring",
    "title": "9  Conservation Applications",
    "section": "9.10 Community-Based Conservation Monitoring",
    "text": "9.10 Community-Based Conservation Monitoring\nInvolving local communities in conservation monitoring can improve data collection and conservation outcomes.\n\n9.10.1 Example: Analyzing Community Monitoring Data\n\n\nCode\n# Simulate community monitoring data\nset.seed(202)\nn_villages &lt;- 5\nn_months &lt;- 24\n\n# Create variables\nvillage &lt;- rep(paste0(\"Village\", 1:n_villages), each = n_months)\nmonth &lt;- rep(1:n_months, times = n_villages)\nyear &lt;- rep(rep(c(1, 2), each = 12), times = n_villages)\n\n# Generate poaching incidents with seasonal pattern and declining trend\nseason &lt;- sin(month * pi / 6) + 1  # Seasonal pattern\ntrend &lt;- -0.03 * (month - 1)  # Declining trend\nvillage_effect &lt;- rep(rnorm(n_villages, 0, 0.5), each = n_months)\nlambda &lt;- exp(1 + 0.5 * season + trend + village_effect)\npoaching &lt;- rpois(n_villages * n_months, lambda)\n\n# Create a data frame\nmonitoring_data &lt;- data.frame(\n  village = factor(village),\n  month = month,\n  year = factor(year),\n  poaching = poaching\n)\n\n# Visualize the data\nggplot(monitoring_data, aes(x = month, y = poaching, color = village, group = village)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~year, scales = \"free_x\", labeller = labeller(year = c(\"1\" = \"Year 1\", \"2\" = \"Year 2\"))) +\n  labs(title = \"Poaching Incidents Reported by Community Monitors\",\n       x = \"Month\",\n       y = \"Number of Incidents\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Analyze trends\nlibrary(MASS)\ntrend_model &lt;- glm.nb(poaching ~ month + village, data = monitoring_data)\nsummary(trend_model)\n\n\n\nCall:\nglm.nb(formula = poaching ~ month + village, data = monitoring_data, \n    init.theta = 21.97524464, link = log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      1.229650   0.181336   6.781 1.19e-11 ***\nmonth           -0.057015   0.008742  -6.522 6.94e-11 ***\nvillageVillage2  0.545243   0.201645   2.704 0.006852 ** \nvillageVillage3  0.558968   0.201193   2.778 0.005465 ** \nvillageVillage4  0.270966   0.211843   1.279 0.200866    \nvillageVillage5  0.716424   0.196360   3.649 0.000264 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(21.9752) family taken to be 1)\n\n    Null deviance: 199.13  on 119  degrees of freedom\nResidual deviance: 136.01  on 114  degrees of freedom\nAIC: 468.75\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  22.0 \n          Std. Err.:  23.9 \n\n 2 x log-likelihood:  -454.752 \n\n\nCode\n# Calculate overall trend\ntrend_coef &lt;- coef(trend_model)[\"month\"]\nmonthly_change &lt;- (exp(trend_coef) - 1) * 100\ncat(\"Monthly change in poaching incidents:\", round(monthly_change, 2), \"%\\n\")\n\n\nMonthly change in poaching incidents: -5.54 %\n\n\nCode\n# Analyze seasonal patterns\nseason_model &lt;- glm.nb(poaching ~ sin(2 * pi * month / 12) + cos(2 * pi * month / 12) + village, \n                      data = monitoring_data)\nsummary(season_model)\n\n\n\nCall:\nglm.nb(formula = poaching ~ sin(2 * pi * month/12) + cos(2 * \n    pi * month/12) + village, data = monitoring_data, init.theta = 40.9900692, \n    link = log)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.48468    0.15815   3.065 0.002180 ** \nsin(2 * pi * month/12)  0.64312    0.08539   7.531 5.03e-14 ***\ncos(2 * pi * month/12)  0.08170    0.08155   1.002 0.316459    \nvillageVillage2         0.55310    0.19722   2.805 0.005039 ** \nvillageVillage3         0.57202    0.19658   2.910 0.003617 ** \nvillageVillage4         0.28057    0.20754   1.352 0.176412    \nvillageVillage5         0.72313    0.19186   3.769 0.000164 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(40.9901) family taken to be 1)\n\n    Null deviance: 209.78  on 119  degrees of freedom\nResidual deviance: 129.01  on 113  degrees of freedom\nAIC: 457.46\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  41.0 \n          Std. Err.:  70.8 \n\n 2 x log-likelihood:  -441.462 \n\n\nCode\n# Compare models\nanova(trend_model, season_model)\n\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: poaching\n                                                      Model    theta Resid. df\n1                                           month + village 21.97524       114\n2 sin(2 * pi * month/12) + cos(2 * pi * month/12) + village 40.99007       113\n     2 x log-lik.   Test    df LR stat.      Pr(Chi)\n1       -454.7522                                   \n2       -441.4616 1 vs 2     1  13.2906 0.0002667407",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#summary",
    "href": "chapters/09-conservation.html#summary",
    "title": "9  Conservation Applications",
    "section": "9.11 Summary",
    "text": "9.11 Summary\nIn this chapter, we’ve explored how data analysis techniques can be applied to conservation challenges:\n\nSpecies distribution modeling to predict habitat suitability\nPopulation trend analysis to monitor species status\nHabitat fragmentation analysis to assess landscape connectivity\nProtected area effectiveness evaluation using BACI designs\nThreat assessment and prioritization for conservation planning\nSystematic conservation planning using complementarity analysis\nClimate change vulnerability assessment based on species traits\nCommunity-based conservation monitoring to track threats\n\nThese applications demonstrate how the statistical methods covered throughout this book can help address real-world conservation problems, inform management decisions, and ultimately contribute to biodiversity conservation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "chapters/09-conservation.html#exercises",
    "href": "chapters/09-conservation.html#exercises",
    "title": "9  Conservation Applications",
    "section": "9.12 Exercises",
    "text": "9.12 Exercises\n\nImport a dataset on species occurrences and environmental variables, then build a simple species distribution model.\nAnalyze population monitoring data to detect trends and assess conservation status.\nCalculate basic landscape metrics for a land cover map to quantify habitat fragmentation.\nDesign and analyze a BACI study to evaluate the effectiveness of a conservation intervention.\nConduct a threat assessment for a species or ecosystem of your choice.\nUse complementarity analysis to identify priority sites for conservation.\nPerform a climate change vulnerability assessment for a group of species.\nAnalyze community monitoring data to detect trends in threats or biodiversity.\n\n\n\n\n\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species distribution models: Ecological explanation and prediction across space and time. Annual Review of Ecology, Evolution, and Systematics, 40, 677–697.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Conservation Applications</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bolker, B. et al. (2009). Generalized linear mixed models: A\npractical guide. Trends in Ecology & Evolution.\n\n\nElith, J., Leathwick, J. R., & Hastie, T. (2009). Species\ndistribution models: Ecological explanation and prediction across space\nand time. Annual Review of Ecology, Evolution, and Systematics,\n40, 677–697.\n\n\nGotelli, N. J., & Ellison, A. M. (2004). Null model analysis of\nspecies co-occurrence patterns. Sinauer Associates.\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import,\ntidy, transform, visualize, and model data. O’Reilly Media, Inc.\n\n\nZuur, A., Ieno, E. N., & Smith, G. M. (2007). Analyzing\necological data. Springer.\n\n\nZuur, A., Ieno, E. N., Walker, N., Saveliev, A. A., & Smith, G. M.\n(2009). Mixed effects models and extensions in ecology with r.\nSpringer Science & Business Media.",
    "crumbs": [
      "References"
    ]
  }
]